{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfc35d6a",
   "metadata": {},
   "source": [
    "Attribute analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fcdeca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e72fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e456864e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73070/1300082723.py:41: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  fault[\"YM\"]=fault[\"TIME_OUTAGE\"].dt.to_period(\"M\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Generalized] Eval on test year=2024  AUC=0.776\n",
      "Top-15 features by permutation importance:\n",
      "log1p_LENGTH_KM          0.062891\n",
      "i_mtbf_inv               0.032813\n",
      "recent_24m               0.026172\n",
      "log1p_hist_faults_cum    0.025391\n",
      "hist_faults_cum          0.018359\n",
      "joint_density            0.016797\n",
      "load_range_idx           0.013672\n",
      "cycle_pm                 0.011328\n",
      "recent_8m                0.007812\n",
      "log1p_age_frac           0.005078\n",
      "Saved: feature_importance_generalized.csv, panel_generalized_year_switch.csv\n",
      "[viz] Eval year=2024  AUROC=0.776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73070/1300082723.py:353: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby(\"bin\")\n",
      "/tmp/ipykernel_73070/1300082723.py:353: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby(\"bin\")\n",
      "/tmp/ipykernel_73070/1300082723.py:353: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby(\"bin\")\n",
      "/tmp/ipykernel_73070/1300082723.py:353: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby(\"bin\")\n",
      "/tmp/ipykernel_73070/1300082723.py:353: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby(\"bin\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All plots saved.\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 7 · Year-generalized panel + feature importance (uses ALL faults up to 2024)\n",
    "#     Version: recent_8m vs recent_24m (replaces recent_12m)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import logging, math, sys, json, numpy as np, pandas as pd\n",
    "# ─── Config ─────────────────────────────────────────────────────────────────\n",
    "OUT_DIR = Path(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/A_fault_model_with_health\")\n",
    "EXPECTED_LIFE_YEARS = 35.0\n",
    "N_THREADS = 3\n",
    "KEEP_VOLTAGES           = {22, 33}\n",
    "FAULT_CSV = Path(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/FAULT DATA/HT_fault_cable_info_processed_with_affected.csv\")\n",
    "CABLE_CSV = Path(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/SWNO_MASTER_COMBINED_FULL_FINAL3.csv\")\n",
    "\n",
    "def norm_sw(s):\n",
    "    return (s.astype(str).str.upper().str.strip()\n",
    "             .str.replace(r\"^(SWNO_|SWNO|SW|S)\\s*\", \"\", regex=True)\n",
    "             .str.replace(r\"\\D+\",\"\",regex=True)\n",
    "             .replace(\"\", np.nan)).astype(\"Int64\")\n",
    "\n",
    "def v_to_num(v):\n",
    "    try: return float(str(v).lower().replace(\"kv\",\"\"))\n",
    "    except: return math.nan\n",
    "fault=pd.read_csv(FAULT_CSV,parse_dates=[\"TIME_OUTAGE\"],low_memory=False)\n",
    "\n",
    "sw_col=\"TO_SWITCH\" if \"TO_SWITCH\" in fault.columns else fault.columns[0]\n",
    "fault[\"SWITCH_ID\"]=norm_sw(fault[sw_col])\n",
    "fault=fault.dropna(subset=[\"SWITCH_ID\",\"TIME_OUTAGE\"])\n",
    "\n",
    "if \"VOLTAGE\" in fault.columns:\n",
    "    fault[\"VNUM\"]=fault[\"VOLTAGE\"].apply(v_to_num)\n",
    "    fault=fault[fault[\"VNUM\"].isin(KEEP_VOLTAGES)]\n",
    "fault=fault.drop_duplicates([\"SWITCH_ID\",\"TIME_OUTAGE\"])\n",
    "fault[\"YM\"]=fault[\"TIME_OUTAGE\"].dt.to_period(\"M\")\n",
    "cables=pd.read_csv(CABLE_CSV, low_memory=False).drop_duplicates(\"DESTINATION_SWITCH_ID\")\n",
    "today=pd.Timestamp.utcnow().tz_localize(None)\n",
    "\n",
    "cab=cables.rename(columns={\"DESTINATION_SWITCH_ID\":\"SWITCH_ID\",\n",
    "                           \"MEASUREDLENGTH\":\"LENGTH_M\",\n",
    "                           \"COMMISSIONEDDATE\":\"DATE_INSTALLED\",\n",
    "                           \"NO_OF_SEGMENT\":\"SEGMENTS\"})\n",
    "cab[\"LENGTH_KM\"]=pd.to_numeric(cab[\"LENGTH_M\"],errors=\"coerce\")/1000\n",
    "cab[\"DATE_INSTALLED\"]=(pd.to_datetime(cab[\"DATE_INSTALLED\"],errors=\"coerce\",utc=True)\n",
    "                       .dt.tz_localize(None))\n",
    "cyc=[c for c in cab.columns if c.startswith(\"CYCLE_Month_\")]\n",
    "var=[c for c in cab.columns if c.startswith(\"VAR_Month_\")]\n",
    "cab[\"cycle_pm\"]=cab[cyc].mean(1)\n",
    "cab[\"load_range_idx\"]=cab[var].mean(1)/cab[var].median(1)\n",
    "\n",
    "\n",
    "for k in [\"OMP_NUM_THREADS\",\"OPENBLAS_NUM_THREADS\",\"MKL_NUM_THREADS\",\n",
    "          \"VECLIB_MAXIMUM_THREADS\",\"NUMEXPR_NUM_THREADS\"]:\n",
    "    os.environ[k] = str(N_THREADS)\n",
    "\n",
    "(OUT_DIR / \"CORR\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def log(msg, *args): print(msg % args if args else msg)\n",
    "\n",
    "# ─── Preconditions ─────────────────────────────────────────────────────────\n",
    "req_fault_cols = {\"SWITCH_ID\",\"TIME_OUTAGE\"}\n",
    "req_cab_cols   = {\"SWITCH_ID\",\"LENGTH_KM\",\"SEGMENTS\",\"cycle_pm\",\"load_range_idx\",\"DATE_INSTALLED\"}\n",
    "missing_f = req_fault_cols - set(globals().get(\"fault\", pd.DataFrame()).columns)\n",
    "missing_c = req_cab_cols   - set(globals().get(\"cab\",   pd.DataFrame()).columns)\n",
    "if missing_f or missing_c:\n",
    "    raise RuntimeError(f\"Missing columns — fault: {missing_f}, cab: {missing_c}\")\n",
    "\n",
    "# ─── Ensure TIME_OUTAGE is tz-aware UTC ────────────────────────────────────\n",
    "fault[\"TIME_OUTAGE\"] = pd.to_datetime(fault[\"TIME_OUTAGE\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "# Precompute inter-fault deltas (hours) once for MTBF\n",
    "fault_sorted = fault.sort_values([\"SWITCH_ID\", \"TIME_OUTAGE\"]).copy()\n",
    "fault_sorted[\"dt_hours\"] = (\n",
    "    fault_sorted.groupby(\"SWITCH_ID\")[\"TIME_OUTAGE\"]\n",
    "    .diff()\n",
    "    .dt.total_seconds()\n",
    "    .div(3600)\n",
    ")\n",
    "\n",
    "# ─── Build per-switch, per-year panel (history up to Jan 1 of year y) ─────\n",
    "min_y = int(fault[\"TIME_OUTAGE\"].dt.year.min())\n",
    "max_y = int(fault[\"TIME_OUTAGE\"].dt.year.max())\n",
    "years = list(range(max(min_y + 1, 2016), max_y + 1))\n",
    "\n",
    "panel_rows = []\n",
    "for y in years:\n",
    "    B    = pd.Timestamp(f\"{y}-01-01\", tz=\"UTC\")\n",
    "    Bm8  = B - pd.DateOffset(months=8)   # ← 8-month window\n",
    "    Bm24 = B - pd.DateOffset(years=2)\n",
    "\n",
    "    # history ≤ B-1day (no leakage)\n",
    "    hist = fault_sorted[fault_sorted[\"TIME_OUTAGE\"] < B]\n",
    "\n",
    "    # next-year target (fault occurs in year y)\n",
    "    tgt  = (fault_sorted[fault_sorted[\"TIME_OUTAGE\"].dt.year == y]\n",
    "            .groupby(\"SWITCH_ID\").size().rename(\"TARGET_FAULT\")).astype(int)\n",
    "\n",
    "    # counts in history\n",
    "    hist_cnt  = hist.groupby(\"SWITCH_ID\").size().rename(\"hist_faults_cum\")\n",
    "    recent8   = hist[hist[\"TIME_OUTAGE\"] >= Bm8 ].groupby(\"SWITCH_ID\").size().rename(\"recent_8m\")\n",
    "    recent24  = hist[hist[\"TIME_OUTAGE\"] >= Bm24].groupby(\"SWITCH_ID\").size().rename(\"recent_24m\")\n",
    "    last_time = hist.groupby(\"SWITCH_ID\")[\"TIME_OUTAGE\"].max().rename(\"last_fault_time\")\n",
    "\n",
    "    # MTBF (hours) in history up to B\n",
    "    mtbf_h = (hist.groupby(\"SWITCH_ID\")[\"dt_hours\"].mean()\n",
    "                  .rename(\"mtbf_hours\"))\n",
    "\n",
    "    feat = cab[[\n",
    "        \"SWITCH_ID\",\"LENGTH_KM\",\"SEGMENTS\",\"cycle_pm\",\"load_range_idx\",\"DATE_INSTALLED\"\n",
    "    ]].copy()\n",
    "\n",
    "    feat = (\n",
    "        feat\n",
    "        .merge(hist_cnt,   on=\"SWITCH_ID\", how=\"left\")\n",
    "        .merge(recent8,    on=\"SWITCH_ID\", how=\"left\")   # ← recent_8m\n",
    "        .merge(recent24,   on=\"SWITCH_ID\", how=\"left\")\n",
    "        .merge(last_time,  on=\"SWITCH_ID\", how=\"left\")\n",
    "        .merge(mtbf_h,     on=\"SWITCH_ID\", how=\"left\")\n",
    "        .merge(tgt,        on=\"SWITCH_ID\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    # fill counts\n",
    "    feat[[\"hist_faults_cum\",\"recent_8m\",\"recent_24m\",\"TARGET_FAULT\"]] = (\n",
    "        feat[[\"hist_faults_cum\",\"recent_8m\",\"recent_24m\",\"TARGET_FAULT\"]]\n",
    "        .fillna(0).astype(int)\n",
    "    )\n",
    "\n",
    "    # age (years)\n",
    "    B_naive = B.tz_convert(None)\n",
    "    feat[\"DATE_INSTALLED\"] = pd.to_datetime(feat[\"DATE_INSTALLED\"], errors=\"coerce\")\n",
    "    age_years = (B_naive  - feat[\"DATE_INSTALLED\"]).dt.days / 365.25\n",
    "    feat[\"age_years\"] = age_years / EXPECTED_LIFE_YEARS\n",
    "    # feat[\"age_years\"] = ((B_naive - feat[\"DATE_INSTALLED\"]).dt.days / 365.25).clip(lower=0)\n",
    "    feat['log1p_age_years'] = np.log1p(feat[\"age_years\"].clip(lower=0))\n",
    "\n",
    "    # time since last fault (days)\n",
    "    feat[\"time_since_last_fault_days\"]  = (B - feat[\"last_fault_time\"]).dt.days\n",
    "    feat.loc[feat[\"last_fault_time\"].isna(), \"time_since_last_fault_days\"] = np.nan\n",
    "\n",
    "    # numeric coercions\n",
    "    feat[\"LENGTH_KM\"] = pd.to_numeric(feat[\"LENGTH_KM\"], errors=\"coerce\")\n",
    "    feat[\"SEGMENTS\"]  = pd.to_numeric(feat[\"SEGMENTS\"],  errors=\"coerce\")\n",
    "\n",
    "    # attributes (aligned with health score; NO per-km faults)\n",
    "    # s: joints density (segments-1 per km)\n",
    "    feat[\"joint_density\"] = (feat[\"SEGMENTS\"].fillna(0).clip(lower=1) - 1) / feat[\"LENGTH_KM\"].replace(0, np.nan)\n",
    "    # i: interruption risk via inverse MTBF (hours); larger => riskier\n",
    "    feat[\"i_mtbf_inv\"] = 1.0 / feat[\"mtbf_hours\"].clip(lower=1)\n",
    "\n",
    "    # age helpers\n",
    "    feat[\"age_frac\"] = feat[\"age_years\"]\n",
    "    feat[\"log1p_age_frac\"] = np.log1p(feat[\"age_frac\"].clip(lower=0))\n",
    "    # log1p transforms for skewed attrs — keep ONLY length\n",
    "    feat[\"log1p_LENGTH_KM\"] = np.log1p(pd.to_numeric(feat[\"LENGTH_KM\"], errors=\"coerce\").clip(lower=0))\n",
    "\n",
    "    feat[\"YEAR\"] = y\n",
    "    panel_rows.append(feat)\n",
    "\n",
    "panel = pd.concat(panel_rows, ignore_index=True)\n",
    "\n",
    "# ─── Clean NaNs/Infs ──────────────────────────────────────────────────────\n",
    "num_cols = panel.select_dtypes(include=[np.number]).columns.tolist()\n",
    "panel[num_cols] = panel[num_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# ─── Drop any years beyond 2024 ───────────────────────────────────────────\n",
    "panel = panel[panel[\"YEAR\"] <= 2024].copy()\n",
    "\n",
    "# ─── Feature set (use counts, NOT per-km; drop all banned log1p) ─────────\n",
    "feat_cols = [\n",
    "    \"cycle_pm\", \"load_range_idx\",\n",
    "    \"hist_faults_cum\",\n",
    "    \"joint_density\", \"i_mtbf_inv\",\n",
    "    \"log1p_LENGTH_KM\",\n",
    "    \"recent_8m\", \"recent_24m\", \"log1p_hist_faults_cum\",\"log1p_age_frac\",\n",
    "]\n",
    "# keep log1p_hist_faults_cum only if it exists; otherwise create it\n",
    "if \"log1p_hist_faults_cum\" not in panel.columns and \"hist_faults_cum\" in panel.columns:\n",
    "    panel[\"log1p_hist_faults_cum\"] = np.log1p(panel[\"hist_faults_cum\"])\n",
    "feat_cols = [c for c in feat_cols if c in panel.columns]\n",
    "\n",
    "# ─── Split: train on 2016–2023, test on 2024 ─────────────────────────────\n",
    "train_df = panel[panel[\"YEAR\"] < 2024].copy()\n",
    "test_df  = panel[panel[\"YEAR\"] == 2024].copy()\n",
    "\n",
    "Xtr = train_df[feat_cols].values\n",
    "ytr = (train_df[\"TARGET_FAULT\"] > 0).astype(int).values\n",
    "Xte = test_df[feat_cols].values\n",
    "yte = (test_df[\"TARGET_FAULT\"]  > 0).astype(int).values\n",
    "\n",
    "# ─── Train Random Forest ─────────────────────────────────────────────────\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=50,\n",
    "    min_samples_leaf=25,\n",
    "    class_weight=\"balanced_subsample\",\n",
    "    n_jobs=N_THREADS,\n",
    "    random_state=42\n",
    ").fit(Xtr, ytr)\n",
    "\n",
    "# ─── Evaluate & choose best hold-out ─────────────────────────────────────\n",
    "if len(np.unique(yte)) > 1:\n",
    "    use_X, use_y, eval_year, note = Xte, yte, 2024, \"test\"\n",
    "else:\n",
    "    val_df = train_df[train_df[\"YEAR\"] == 2023]\n",
    "    yv     = (val_df[\"TARGET_FAULT\"] > 0).astype(int).values\n",
    "    if len(val_df) and len(np.unique(yv)) > 1:\n",
    "        use_X, use_y, eval_year, note = val_df[feat_cols].values, yv, 2023, \"val(2023)\"\n",
    "    else:\n",
    "        use_X, use_y, eval_year, note = Xtr, ytr, int(train_df[\"YEAR\"].min()), \"train(caution)\"\n",
    "\n",
    "pred_proba = rf.predict_proba(use_X)[:,1]\n",
    "auc = roc_auc_score(use_y, pred_proba)\n",
    "log(\"[Generalized] Eval on %s year=%s  AUC=%.3f\", note, eval_year, auc)\n",
    "\n",
    "# ─── Compute & save importances ──────────────────────────────────────────\n",
    "gini_imp = pd.Series(rf.feature_importances_, index=feat_cols, name=\"rf_gini_importance\") \\\n",
    "             .sort_values(ascending=False)\n",
    "perm = permutation_importance(\n",
    "    rf, use_X, use_y, n_repeats=10, random_state=42, n_jobs=N_THREADS\n",
    ")\n",
    "perm_imp = pd.Series(perm.importances_mean, index=feat_cols, name=\"perm_importance_mean\") \\\n",
    "             .sort_values(ascending=False)\n",
    "perm_std = pd.Series(perm.importances_std, index=feat_cols, name=\"perm_importance_std\") \\\n",
    "             .loc[perm_imp.index]\n",
    "\n",
    "imp_table = pd.concat([gini_imp, perm_imp, perm_std], axis=1)\n",
    "(OUT_DIR / \"CORR\" / \"feature_importance_generalized.csv\").write_text(\n",
    "    imp_table.to_csv(index=True)\n",
    ")\n",
    "\n",
    "panel_out_cols = [\"SWITCH_ID\",\"YEAR\",\"TARGET_FAULT\",\n",
    "                  \"cycle_pm\",\"load_range_idx\",\"hist_faults_cum\",\n",
    "                  \"joint_density\",\"i_mtbf_inv\",\"log1p_LENGTH_KM\",\n",
    "                  \"recent_8m\",\"recent_24m\",\"log1p_hist_faults_cum\",\"log1p_age_frac\"]\n",
    "panel_out = panel[[c for c in panel_out_cols if c in panel.columns]]\n",
    "panel_out.to_csv(OUT_DIR / \"CORR\" / \"panel_generalized_year_switch.csv\", index=False)\n",
    "\n",
    "log(\"Top-15 features by permutation importance:\\n%s\", perm_imp.head(15).to_string())\n",
    "log(\"Saved: feature_importance_generalized.csv, panel_generalized_year_switch.csv\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Visuals: load panel, fit RF, and plot with customizable bins/styles\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "OUT_DIR = Path(OUT_DIR)\n",
    "panel_csv = OUT_DIR / \"CORR\" / \"panel_generalized_year_switch.csv\"\n",
    "assert panel_csv.exists(), f\"Missing {panel_csv}. Run the generalized panel block first.\"\n",
    "panel = pd.read_csv(panel_csv)\n",
    "\n",
    "panel = panel[panel[\"YEAR\"] <= 2024].copy()\n",
    "panel = panel.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# Same feature set for viz\n",
    "pref_feat_cols = [\n",
    "    \"cycle_pm\", \"load_range_idx\",\n",
    "    \"hist_faults_cum\",\n",
    "    \"joint_density\", \"i_mtbf_inv\",\n",
    "    \"log1p_LENGTH_KM\",\n",
    "    \"recent_8m\", \"recent_24m\", \"log1p_hist_faults_cum\",\"log1p_age_frac\"\n",
    "]\n",
    "feat_cols = [c for c in pref_feat_cols if c in panel.columns]\n",
    "if not feat_cols:\n",
    "    num = panel.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for drop in [\"TARGET_FAULT\",\"YEAR\"]:\n",
    "        if drop in num: num.remove(drop)\n",
    "    feat_cols = num\n",
    "\n",
    "TRAIN_END = 2023\n",
    "TEST_YEAR = 2024\n",
    "train_df = panel[panel[\"YEAR\"] <= TRAIN_END].copy()\n",
    "test_df  = panel[panel[\"YEAR\"] == TEST_YEAR].copy()\n",
    "\n",
    "Xtr, ytr = train_df[feat_cols].values, (train_df[\"TARGET_FAULT\"] > 0).astype(int).values\n",
    "Xte, yte = test_df[feat_cols].values,  (test_df[\"TARGET_FAULT\"]  > 0).astype(int).values\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=400, max_depth=70, \n",
    "    min_samples_leaf=25,\n",
    "    class_weight=\"balanced_subsample\",\n",
    "    n_jobs=N_THREADS, \n",
    "    random_state=42\n",
    ").fit(Xtr, ytr)\n",
    "\n",
    "p_eval = rf.predict_proba(Xte)[:,1]\n",
    "auc    = roc_auc_score(yte, p_eval)\n",
    "print(f\"[viz] Eval year={TEST_YEAR}  AUROC={auc:.3f}\")\n",
    "\n",
    "perm = permutation_importance(rf, Xte, yte, n_repeats=20, random_state=42, n_jobs=N_THREADS)\n",
    "perm_mean = pd.Series(perm.importances_mean, index=feat_cols).sort_values()\n",
    "\n",
    "# ─── Binning & styling ────────────────────────────────────────────────────\n",
    "bin_settings = {\n",
    "    \"recent_8m\":          {\"method\": \"integer\"},   # ← integer bins for counts\n",
    "    \"recent_24m\":         {\"method\": \"integer\"},\n",
    "    \"SEGMENTS\":           {\"method\": \"integer\"},   # (only if present in panel)\n",
    "    \"log1p_LENGTH_KM\":    {\"method\": \"quantile\", \"q\": 8},\n",
    "    \"time_since_last_fault_days\": {\"method\": \"equal_width\", \"bins\": 7},\n",
    "    \"i_mtbf_inv\":         {\"method\": \"quantile\", \"q\": 6},\n",
    "}\n",
    "style_settings = {\n",
    "    \"Actual event rate\": {\"marker\": \"o\", \"linestyle\": \"-\",  \"linewidth\": 2},\n",
    "    \"Model probability\": {\"marker\": \"s\", \"linestyle\": \"--\", \"linewidth\": 2},\n",
    "}\n",
    "\n",
    "top_feats = perm_mean.index[::-1][:6]\n",
    "for feat in top_feats:\n",
    "    s = test_df[feat].astype(float).replace([np.inf,-np.inf], np.nan)\n",
    "    if feat == \"time_since_last_fault_days\":\n",
    "        s = s.replace(0, np.nan)\n",
    "    mask = s.notna()\n",
    "    s, yv, pv = s[mask], yte[mask], p_eval[mask]\n",
    "\n",
    "    cfg    = bin_settings.get(feat, {\"method\":\"quantile\",\"q\":6})\n",
    "    method = cfg[\"method\"]\n",
    "\n",
    "    if method == \"integer\":\n",
    "        s_int = s.round().astype(int)\n",
    "        grp = (\n",
    "            pd.DataFrame({\"feat\":s_int,\"y\":yv,\"p\":pv})\n",
    "              .groupby(\"feat\")\n",
    "              .agg(event_rate=(\"y\",\"mean\"), pred_mean=(\"p\",\"mean\"), count=(\"y\",\"size\"))\n",
    "              .reset_index()\n",
    "              .sort_values(\"feat\")\n",
    "        )\n",
    "        x_vals = grp[\"feat\"].values\n",
    "\n",
    "    elif method == \"equal_width\":\n",
    "        bins = cfg.get(\"bins\", 5)\n",
    "        b    = pd.cut(s, bins=bins)\n",
    "        grp  = (\n",
    "            pd.DataFrame({\"feat\":s,\"y\":yv,\"p\":pv,\"bin\":b})\n",
    "              .groupby(\"bin\")\n",
    "              .agg(feat_mean=(\"feat\",\"mean\"), event_rate=(\"y\",\"mean\"),\n",
    "                   pred_mean=(\"p\",\"mean\"), count=(\"y\",\"size\"))\n",
    "              .reset_index(drop=True)\n",
    "              .sort_values(\"feat_mean\")\n",
    "        )\n",
    "        x_vals = grp[\"feat_mean\"].values\n",
    "\n",
    "    else:  # quantile\n",
    "        q = cfg.get(\"q\", 5)\n",
    "        try:\n",
    "            b = pd.qcut(s, q=q, duplicates=\"drop\")\n",
    "        except Exception:\n",
    "            b = pd.cut(s, bins=q)\n",
    "        grp = (\n",
    "            pd.DataFrame({\"feat\":s,\"y\":yv,\"p\":pv,\"bin\":b})\n",
    "              .groupby(\"bin\")\n",
    "              .agg(feat_mean=(\"feat\",\"mean\"), event_rate=(\"y\",\"mean\"),\n",
    "                   pred_mean=(\"p\",\"mean\"), count=(\"y\",\"size\"))\n",
    "              .reset_index(drop=True)\n",
    "              .sort_values(\"feat_mean\")\n",
    "        )\n",
    "        x_vals = grp[\"feat_mean\"].values\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 4.5))\n",
    "    ax.plot(x_vals, grp[\"event_rate\"], label=\"Actual event rate\",\n",
    "            marker=style_settings[\"Actual event rate\"][\"marker\"],\n",
    "            linestyle=style_settings[\"Actual event rate\"][\"linestyle\"],\n",
    "            linewidth=style_settings[\"Actual event rate\"][\"linewidth\"])\n",
    "    ax.plot(x_vals, grp[\"pred_mean\"], label=\"Model probability\",\n",
    "            marker=style_settings[\"Model probability\"][\"marker\"],\n",
    "            linestyle=style_settings[\"Model probability\"][\"linestyle\"],\n",
    "            linewidth=style_settings[\"Model probability\"][\"linewidth\"])\n",
    "    ax.set_xlabel(feat)\n",
    "    ax.set_ylabel(\"Probability of any fault in year\")\n",
    "    ax.set_title(f\"{feat} vs risk (binned, {TEST_YEAR})  [n={int(grp['count'].sum())}]\")\n",
    "    ax.legend(); ax.grid(alpha=0.3)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(OUT_DIR / \"CORR\" / f\"rel_{feat}_{TEST_YEAR}.png\", dpi=180)\n",
    "    plt.close(fig)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 0.4 * max(1, len(perm_mean)) + 1))\n",
    "ax.barh(perm_mean.index, perm_mean.values)\n",
    "ax.set_xlabel(\"Permutation importance (Δ AUROC)\")\n",
    "ax.set_title(f\"Feature importance on {TEST_YEAR}\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(OUT_DIR / f\"imp_permutation_{TEST_YEAR}.png\", dpi=180)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"All plots saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d991109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45ac72fd",
   "metadata": {},
   "source": [
    "Change the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58b86f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16463/3881479799.py:67: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  fault[\"YM\"] = fault[\"TIME_OUTAGE\"].dt.to_period(\"M\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Generalized] Eval on test year=2024  AUROC(fault+)=0.771\n",
      "Top-15 features by permutation importance:\n",
      "log1p_LENGTH_KM          0.055469\n",
      "recent_24m               0.028516\n",
      "i_mtbf_inv               0.026953\n",
      "hist_faults_cum          0.022266\n",
      "log1p_hist_faults_cum    0.019922\n",
      "joint_density            0.017188\n",
      "load_range_idx           0.012109\n",
      "log1p_age_frac           0.010937\n",
      "cycle_pm                 0.010547\n",
      "recent_8m                0.004687\n",
      "Saved: feature_importance_generalized.csv, panel_generalized_year_switch.csv\n",
      "[viz] Eval year=2024  AUROC(fault+)= 0.771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16463/3881479799.py:369: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby(\"bin\")\n",
      "/tmp/ipykernel_16463/3881479799.py:369: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby(\"bin\")\n",
      "/tmp/ipykernel_16463/3881479799.py:369: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby(\"bin\")\n",
      "/tmp/ipykernel_16463/3881479799.py:369: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby(\"bin\")\n",
      "/tmp/ipykernel_16463/3881479799.py:369: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby(\"bin\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All plots saved.\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 7 · Year-generalized panel + feature importance (uses ALL faults up to 2024)\n",
    "#     Version: recent_8m vs recent_24m (replaces recent_12m)\n",
    "#     NOTE: Classes flipped → label 1 = NO FAULT, label 0 = FAULT\n",
    "#           Metrics/plots are still reported for FAULT as the “positive” event.\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "import os, math, json, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ─── Config ─────────────────────────────────────────────────────────────────\n",
    "OUT_DIR = Path(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/A_fault_model_with_health\")\n",
    "EXPECTED_LIFE_YEARS = 35.0\n",
    "N_THREADS = 3\n",
    "KEEP_VOLTAGES = {22, 33}\n",
    "FAULT_CSV = Path(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/FAULT DATA/HT_fault_cable_info_processed_with_affected.csv\")\n",
    "CABLE_CSV = Path(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/SWNO_MASTER_COMBINED_FULL_FINAL4.csv\")\n",
    "\n",
    "# ─── Helpers ────────────────────────────────────────────────────────────────\n",
    "def norm_sw(s: pd.Series) -> pd.Series:\n",
    "    return (s.astype(str).str.upper().str.strip()\n",
    "             .str.replace(r\"^(SWNO_|SWNO|SW|S)\\s*\", \"\", regex=True)\n",
    "             .str.replace(r\"\\D+\",\"\",regex=True)\n",
    "             .replace(\"\", np.nan)).astype(\"Int64\")\n",
    "\n",
    "def v_to_num(v):\n",
    "    try:\n",
    "        return float(str(v).lower().replace(\"kv\",\"\"))\n",
    "    except Exception:\n",
    "        return math.nan\n",
    "\n",
    "def log(msg, *args):  # simple printf-style logger\n",
    "    print(msg % args if args else msg)\n",
    "\n",
    "def proba_fault(model, X) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return P(FAULT) given labels are flipped (1=NO FAULT, 0=FAULT).\n",
    "    \"\"\"\n",
    "    i_fault = int(np.where(model.classes_ == 0)[0][0])  # class '0' == FAULT\n",
    "    return model.predict_proba(X)[:, i_fault]\n",
    "\n",
    "# ─── Thread caps for reproducibility/perf ───────────────────────────────────\n",
    "for k in [\"OMP_NUM_THREADS\",\"OPENBLAS_NUM_THREADS\",\"MKL_NUM_THREADS\",\n",
    "          \"VECLIB_MAXIMUM_THREADS\",\"NUMEXPR_NUM_THREADS\"]:\n",
    "    os.environ[k] = str(N_THREADS)\n",
    "\n",
    "(OUT_DIR / \"CORR\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ─── Load & normalize faults ────────────────────────────────────────────────\n",
    "fault = pd.read_csv(FAULT_CSV, parse_dates=[\"TIME_OUTAGE\"], low_memory=False)\n",
    "sw_col = \"TO_SWITCH\" if \"TO_SWITCH\" in fault.columns else fault.columns[0]\n",
    "fault[\"SWITCH_ID\"] = norm_sw(fault[sw_col])\n",
    "fault = fault.dropna(subset=[\"SWITCH_ID\",\"TIME_OUTAGE\"])\n",
    "\n",
    "if \"VOLTAGE\" in fault.columns:\n",
    "    fault[\"VNUM\"] = fault[\"VOLTAGE\"].apply(v_to_num)\n",
    "    fault = fault[fault[\"VNUM\"].isin(KEEP_VOLTAGES)]\n",
    "\n",
    "fault = fault.drop_duplicates([\"SWITCH_ID\",\"TIME_OUTAGE\"])\n",
    "fault[\"TIME_OUTAGE\"] = pd.to_datetime(fault[\"TIME_OUTAGE\"], errors=\"coerce\", utc=True)\n",
    "fault[\"YM\"] = fault[\"TIME_OUTAGE\"].dt.to_period(\"M\")\n",
    "\n",
    "# ─── Load & normalize cable attributes ─────────────────────────────────────\n",
    "cables = pd.read_csv(CABLE_CSV, low_memory=False).drop_duplicates(\"DESTINATION_SWITCH_ID\")\n",
    "cab = cables.rename(columns={\n",
    "    \"DESTINATION_SWITCH_ID\": \"SWITCH_ID\",\n",
    "    \"MEASUREDLENGTH\": \"LENGTH_M\",\n",
    "    \"COMMISSIONEDDATE\": \"DATE_INSTALLED\",\n",
    "    \"NO_OF_SEGMENT\": \"SEGMENTS\"\n",
    "})\n",
    "cab[\"LENGTH_KM\"] = pd.to_numeric(cab[\"LENGTH_M\"], errors=\"coerce\")/1000\n",
    "cab[\"DATE_INSTALLED\"] = (pd.to_datetime(cab[\"DATE_INSTALLED\"], errors=\"coerce\", utc=True)\n",
    "                         .dt.tz_localize(None))\n",
    "\n",
    "cyc = [c for c in cab.columns if c.startswith(\"CYCLE_Month_\")]\n",
    "var = [c for c in cab.columns if c.startswith(\"VAR_Month_\")]\n",
    "cab[\"cycle_pm\"] = cab[cyc].mean(1) if len(cyc) else 0.0\n",
    "cab[\"load_range_idx\"] = (cab[var].mean(1)/cab[var].median(1)) if len(var) else 0.0\n",
    "\n",
    "# ─── Preconditions ─────────────────────────────────────────────────────────\n",
    "req_fault_cols = {\"SWITCH_ID\",\"TIME_OUTAGE\"}\n",
    "req_cab_cols   = {\"SWITCH_ID\",\"LENGTH_KM\",\"SEGMENTS\",\"cycle_pm\",\"load_range_idx\",\"DATE_INSTALLED\"}\n",
    "missing_f = req_fault_cols - set(fault.columns)\n",
    "missing_c = req_cab_cols   - set(cab.columns)\n",
    "if missing_f or missing_c:\n",
    "    raise RuntimeError(f\"Missing columns — fault: {missing_f}, cab: {missing_c}\")\n",
    "\n",
    "# ─── Precompute inter-fault deltas (hours) once for MTBF ───────────────────\n",
    "fault_sorted = fault.sort_values([\"SWITCH_ID\", \"TIME_OUTAGE\"]).copy()\n",
    "fault_sorted[\"dt_hours\"] = (\n",
    "    fault_sorted.groupby(\"SWITCH_ID\")[\"TIME_OUTAGE\"]\n",
    "    .diff()\n",
    "    .dt.total_seconds()\n",
    "    .div(3600)\n",
    ")\n",
    "\n",
    "# ─── Build per-switch, per-year panel (history up to Jan 1 of year y) ──────\n",
    "min_y = int(fault[\"TIME_OUTAGE\"].dt.year.min())\n",
    "max_y = int(fault[\"TIME_OUTAGE\"].dt.year.max())\n",
    "years = list(range(max(min_y + 1, 2016), max_y + 1))\n",
    "\n",
    "panel_rows = []\n",
    "for y in years:\n",
    "    B    = pd.Timestamp(f\"{y}-01-01\", tz=\"UTC\")\n",
    "    Bm8  = B - pd.DateOffset(months=8)    # 8-month recency window\n",
    "    Bm24 = B - pd.DateOffset(years=2)     # 24-month recency window\n",
    "\n",
    "    # history ≤ B-1day (no leakage)\n",
    "    hist = fault_sorted[fault_sorted[\"TIME_OUTAGE\"] < B]\n",
    "\n",
    "    # next-year target (fault occurs in year y)\n",
    "    tgt = (fault_sorted[fault_sorted[\"TIME_OUTAGE\"].dt.year == y]\n",
    "           .groupby(\"SWITCH_ID\").size().rename(\"TARGET_FAULT\")).astype(int)\n",
    "\n",
    "    # counts in history\n",
    "    hist_cnt  = hist.groupby(\"SWITCH_ID\").size().rename(\"hist_faults_cum\")\n",
    "    recent8   = hist[hist[\"TIME_OUTAGE\"] >= Bm8 ].groupby(\"SWITCH_ID\").size().rename(\"recent_8m\")\n",
    "    recent24  = hist[hist[\"TIME_OUTAGE\"] >= Bm24].groupby(\"SWITCH_ID\").size().rename(\"recent_24m\")\n",
    "    last_time = hist.groupby(\"SWITCH_ID\")[\"TIME_OUTAGE\"].max().rename(\"last_fault_time\")\n",
    "    mtbf_h    = (hist.groupby(\"SWITCH_ID\")[\"dt_hours\"].mean().rename(\"mtbf_hours\"))\n",
    "\n",
    "    feat = cab[[\"SWITCH_ID\",\"LENGTH_KM\",\"SEGMENTS\",\"cycle_pm\",\"load_range_idx\",\"DATE_INSTALLED\"]].copy()\n",
    "    feat = (\n",
    "        feat\n",
    "        .merge(hist_cnt,   on=\"SWITCH_ID\", how=\"left\")\n",
    "        .merge(recent8,    on=\"SWITCH_ID\", how=\"left\")\n",
    "        .merge(recent24,   on=\"SWITCH_ID\", how=\"left\")\n",
    "        .merge(last_time,  on=\"SWITCH_ID\", how=\"left\")\n",
    "        .merge(mtbf_h,     on=\"SWITCH_ID\", how=\"left\")\n",
    "        .merge(tgt,        on=\"SWITCH_ID\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    # fill counts\n",
    "    for c in [\"hist_faults_cum\",\"recent_8m\",\"recent_24m\",\"TARGET_FAULT\"]:\n",
    "        if c in feat.columns:\n",
    "            feat[c] = feat[c].fillna(0).astype(int)\n",
    "\n",
    "    # age (years, fractional vs EXPECTED_LIFE_YEARS)\n",
    "    B_naive = B.tz_convert(None)\n",
    "    feat[\"DATE_INSTALLED\"] = pd.to_datetime(feat[\"DATE_INSTALLED\"], errors=\"coerce\")\n",
    "    age_years = (B_naive - feat[\"DATE_INSTALLED\"]).dt.days / 365.25\n",
    "    feat[\"age_years\"] = age_years / EXPECTED_LIFE_YEARS\n",
    "    feat[\"log1p_age_years\"] = np.log1p(feat[\"age_years\"].clip(lower=0))\n",
    "\n",
    "    # time since last fault (days)\n",
    "    feat[\"time_since_last_fault_days\"] = (B - feat[\"last_fault_time\"]).dt.days\n",
    "    feat.loc[feat[\"last_fault_time\"].isna(), \"time_since_last_fault_days\"] = np.nan\n",
    "\n",
    "    # numeric coercions\n",
    "    feat[\"LENGTH_KM\"] = pd.to_numeric(feat[\"LENGTH_KM\"], errors=\"coerce\")\n",
    "    feat[\"SEGMENTS\"]  = pd.to_numeric(feat[\"SEGMENTS\"],  errors=\"coerce\")\n",
    "\n",
    "    # attributes (aligned with health score; NOT per-km faults)\n",
    "    feat[\"joint_density\"] = (feat[\"SEGMENTS\"].fillna(0).clip(lower=1) - 1) / feat[\"LENGTH_KM\"].replace(0, np.nan)\n",
    "    feat[\"i_mtbf_inv\"]    = 1.0 / feat[\"mtbf_hours\"].clip(lower=1)\n",
    "\n",
    "    feat[\"age_frac\"]        = feat[\"age_years\"]\n",
    "    feat[\"log1p_age_frac\"]  = np.log1p(feat[\"age_frac\"].clip(lower=0))\n",
    "    feat[\"log1p_LENGTH_KM\"] = np.log1p(pd.to_numeric(feat[\"LENGTH_KM\"], errors=\"coerce\").clip(lower=0))\n",
    "\n",
    "    feat[\"YEAR\"] = y\n",
    "    panel_rows.append(feat)\n",
    "\n",
    "panel = pd.concat(panel_rows, ignore_index=True)\n",
    "\n",
    "# Clean NaNs/Infs\n",
    "num_cols = panel.select_dtypes(include=[np.number]).columns.tolist()\n",
    "panel[num_cols] = panel[num_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# Drop any years beyond 2024\n",
    "panel = panel[panel[\"YEAR\"] <= 2024].copy()\n",
    "\n",
    "# Feature set (counts + attrs)\n",
    "feat_cols = [\n",
    "    \"cycle_pm\", \"load_range_idx\",\n",
    "    \"hist_faults_cum\",\n",
    "    \"joint_density\", \"i_mtbf_inv\",\n",
    "    \"log1p_LENGTH_KM\",\n",
    "    \"recent_8m\", \"recent_24m\", \"log1p_hist_faults_cum\",\"log1p_age_frac\",\n",
    "]\n",
    "# keep / synthesize log1p_hist_faults_cum\n",
    "if \"log1p_hist_faults_cum\" not in panel.columns and \"hist_faults_cum\" in panel.columns:\n",
    "    panel[\"log1p_hist_faults_cum\"] = np.log1p(panel[\"hist_faults_cum\"])\n",
    "feat_cols = [c for c in feat_cols if c in panel.columns]\n",
    "\n",
    "# ─── Split: train on 2016–2023, eval on 2024 (with fallback) ───────────────\n",
    "train_df = panel[panel[\"YEAR\"] < 2024].copy()\n",
    "test_df  = panel[panel[\"YEAR\"] == 2024].copy()\n",
    "\n",
    "# ↻ FLIPPED CLASSES: positive=NO FAULT (1), negative=FAULT (0)\n",
    "Xtr = train_df[feat_cols].values\n",
    "ytr = (train_df[\"TARGET_FAULT\"] == 0).astype(int).values\n",
    "Xte = test_df[feat_cols].values\n",
    "yte = (test_df[\"TARGET_FAULT\"]  == 0).astype(int).values\n",
    "\n",
    "# Train RF\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=50,\n",
    "    min_samples_leaf=25,\n",
    "    class_weight=\"balanced_subsample\",\n",
    "    n_jobs=N_THREADS,\n",
    "    random_state=42\n",
    ").fit(Xtr, ytr)\n",
    "\n",
    "# Choose best holdout for reporting AUROC with FAULT as positive\n",
    "if len(np.unique(yte)) > 1 and len(yte):\n",
    "    use_X, use_y, eval_year, note = Xte, yte, 2024, \"test\"\n",
    "else:\n",
    "    val_df = train_df[train_df[\"YEAR\"] == 2023]\n",
    "    yv = (val_df[\"TARGET_FAULT\"] == 0).astype(int).values\n",
    "    if len(val_df) and len(np.unique(yv)) > 1:\n",
    "        use_X, use_y, eval_year, note = val_df[feat_cols].values, yv, 2023, \"val(2023)\"\n",
    "    else:\n",
    "        use_X, use_y, eval_year, note = Xtr, ytr, int(train_df[\"YEAR\"].min()), \"train(caution)\"\n",
    "\n",
    "# Evaluate AUROC treating FAULT as the positive event\n",
    "p_fault = proba_fault(rf, use_X)      # P(FAULT)\n",
    "y_fault = 1 - use_y                   # convert labels so 1=FAULT for metrics\n",
    "auc = roc_auc_score(y_fault, p_fault)\n",
    "log(\"[Generalized] Eval on %s year=%s  AUROC(fault+)=%.3f\", note, eval_year, auc)\n",
    "\n",
    "# ─── Feature importances (gini + permutation on chosen holdout) ────────────\n",
    "gini_imp = pd.Series(rf.feature_importances_, index=feat_cols, name=\"rf_gini_importance\") \\\n",
    "             .sort_values(ascending=False)\n",
    "perm = permutation_importance(\n",
    "    rf, use_X, use_y, n_repeats=10, random_state=42, n_jobs=N_THREADS\n",
    ")\n",
    "perm_imp = pd.Series(perm.importances_mean, index=feat_cols, name=\"perm_importance_mean\") \\\n",
    "             .sort_values(ascending=False)\n",
    "perm_std = pd.Series(perm.importances_std, index=feat_cols, name=\"perm_importance_std\") \\\n",
    "             .loc[perm_imp.index]\n",
    "\n",
    "imp_table = pd.concat([gini_imp, perm_imp, perm_std], axis=1)\n",
    "(OUT_DIR / \"CORR\" / \"feature_importance_generalized.csv\").write_text(\n",
    "    imp_table.to_csv(index=True)\n",
    ")\n",
    "\n",
    "# Persist compact panel (for viz step below)\n",
    "panel_out_cols = [\"SWITCH_ID\",\"YEAR\",\"TARGET_FAULT\",\n",
    "                  \"cycle_pm\",\"load_range_idx\",\"hist_faults_cum\",\n",
    "                  \"joint_density\",\"i_mtbf_inv\",\"log1p_LENGTH_KM\",\n",
    "                  \"recent_8m\",\"recent_24m\",\"log1p_hist_faults_cum\",\"log1p_age_frac\"]\n",
    "panel_out = panel[[c for c in panel_out_cols if c in panel.columns]]\n",
    "panel_out.to_csv(OUT_DIR / \"CORR\" / \"panel_generalized_year_switch.csv\", index=False)\n",
    "\n",
    "log(\"Top-15 features by permutation importance:\\n%s\", perm_imp.head(15).to_string())\n",
    "log(\"Saved: feature_importance_generalized.csv, panel_generalized_year_switch.csv\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Visuals: load panel, fit RF, and plot with customizable bins/styles\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "panel_csv = OUT_DIR / \"CORR\" / \"panel_generalized_year_switch.csv\"\n",
    "assert panel_csv.exists(), f\"Missing {panel_csv}. Run the generalized panel block first.\"\n",
    "panel = pd.read_csv(panel_csv)\n",
    "panel = panel[panel[\"YEAR\"] <= 2024].copy()\n",
    "panel = panel.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# Same feature set for viz\n",
    "pref_feat_cols = [\n",
    "    \"cycle_pm\", \"load_range_idx\",\n",
    "    \"hist_faults_cum\",\n",
    "    \"joint_density\", \"i_mtbf_inv\",\n",
    "    \"log1p_LENGTH_KM\",\n",
    "    \"recent_8m\", \"recent_24m\", \"log1p_hist_faults_cum\",\"log1p_age_frac\"\n",
    "]\n",
    "feat_cols = [c for c in pref_feat_cols if c in panel.columns]\n",
    "if not feat_cols:\n",
    "    num = panel.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for drop in [\"TARGET_FAULT\",\"YEAR\"]:\n",
    "        if drop in num: num.remove(drop)\n",
    "    feat_cols = num\n",
    "\n",
    "TRAIN_END = 2023\n",
    "TEST_YEAR = 2024\n",
    "train_df = panel[panel[\"YEAR\"] <= TRAIN_END].copy()\n",
    "test_df  = panel[panel[\"YEAR\"] == TEST_YEAR].copy()\n",
    "\n",
    "# ↻ FLIPPED CLASSES again for viz model\n",
    "Xtr, ytr = train_df[feat_cols].values, (train_df[\"TARGET_FAULT\"] == 0).astype(int).values\n",
    "Xte, yte = test_df[feat_cols].values,  (test_df[\"TARGET_FAULT\"]  == 0).astype(int).values\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300, max_depth=50,\n",
    "    min_samples_leaf=25,\n",
    "    class_weight=\"balanced_subsample\",\n",
    "    n_jobs=N_THREADS, random_state=42\n",
    ").fit(Xtr, ytr)\n",
    "\n",
    "# Evaluate on test with FAULT-as-positive AUROC\n",
    "p_eval_fault = proba_fault(rf, Xte)   # P(FAULT)\n",
    "yte_fault = 1 - yte                   # 1=FAULT for metrics/plots\n",
    "auc = roc_auc_score(yte_fault, p_eval_fault)\n",
    "print(f\"[viz] Eval year={TEST_YEAR}  AUROC(fault+)= {auc:.3f}\")\n",
    "\n",
    "# Permutation importance (default scoring = accuracy)\n",
    "perm = permutation_importance(rf, Xte, yte, n_repeats=20, random_state=42, n_jobs=N_THREADS)\n",
    "perm_mean = pd.Series(perm.importances_mean, index=feat_cols).sort_values()\n",
    "\n",
    "# ─── Binning & styling ──────────────────────────────────────────────────────\n",
    "bin_settings = {\n",
    "    \"recent_8m\":          {\"method\": \"integer\"},   # integer bins for counts\n",
    "    \"recent_24m\":         {\"method\": \"integer\"},\n",
    "    \"SEGMENTS\":           {\"method\": \"integer\"},   # (if present)\n",
    "    \"log1p_LENGTH_KM\":    {\"method\": \"quantile\", \"q\": 8},\n",
    "    \"time_since_last_fault_days\": {\"method\": \"equal_width\", \"bins\": 7},\n",
    "    \"i_mtbf_inv\":         {\"method\": \"quantile\", \"q\": 6},\n",
    "}\n",
    "style_settings = {\n",
    "    \"Actual fault rate\": {\"marker\": \"o\", \"linestyle\": \"-\",  \"linewidth\": 2},\n",
    "    \"Predicted fault prob.\": {\"marker\": \"s\", \"linestyle\": \"--\", \"linewidth\": 2},\n",
    "}\n",
    "\n",
    "top_feats = perm_mean.index[::-1][:6]\n",
    "for feat in top_feats:\n",
    "    if feat not in test_df.columns:\n",
    "        continue\n",
    "    s = test_df[feat].astype(float).replace([np.inf,-np.inf], np.nan)\n",
    "    if feat == \"time_since_last_fault_days\":\n",
    "        s = s.replace(0, np.nan)\n",
    "    mask = s.notna()\n",
    "\n",
    "    # Use FAULT as positive (1) for calibration curves\n",
    "    yv = yte_fault[mask]          # 1 = FAULT\n",
    "    pv = p_eval_fault[mask]       # P(FAULT)\n",
    "    s  = s[mask]\n",
    "\n",
    "    cfg    = bin_settings.get(feat, {\"method\":\"quantile\",\"q\":6})\n",
    "    method = cfg[\"method\"]\n",
    "\n",
    "    if method == \"integer\":\n",
    "        s_int = s.round().astype(int)\n",
    "        grp = (\n",
    "            pd.DataFrame({\"feat\":s_int,\"y\":yv,\"p\":pv})\n",
    "              .groupby(\"feat\")\n",
    "              .agg(event_rate=(\"y\",\"mean\"), pred_mean=(\"p\",\"mean\"), count=(\"y\",\"size\"))\n",
    "              .reset_index()\n",
    "              .sort_values(\"feat\")\n",
    "        )\n",
    "        x_vals = grp[\"feat\"].values\n",
    "\n",
    "    elif method == \"equal_width\":\n",
    "        bins = cfg.get(\"bins\", 5)\n",
    "        b    = pd.cut(s, bins=bins)\n",
    "        grp  = (\n",
    "            pd.DataFrame({\"feat\":s,\"y\":yv,\"p\":pv,\"bin\":b})\n",
    "              .groupby(\"bin\")\n",
    "              .agg(feat_mean=(\"feat\",\"mean\"), event_rate=(\"y\",\"mean\"),\n",
    "                   pred_mean=(\"p\",\"mean\"), count=(\"y\",\"size\"))\n",
    "              .reset_index(drop=True)\n",
    "              .sort_values(\"feat_mean\")\n",
    "        )\n",
    "        x_vals = grp[\"feat_mean\"].values\n",
    "\n",
    "    else:  # quantile\n",
    "        q = cfg.get(\"q\", 5)\n",
    "        try:\n",
    "            b = pd.qcut(s, q=q, duplicates=\"drop\")\n",
    "        except Exception:\n",
    "            b = pd.cut(s, bins=q)\n",
    "        grp = (\n",
    "            pd.DataFrame({\"feat\":s,\"y\":yv,\"p\":pv,\"bin\":b})\n",
    "              .groupby(\"bin\")\n",
    "              .agg(feat_mean=(\"feat\",\"mean\"), event_rate=(\"y\",\"mean\"),\n",
    "                   pred_mean=(\"p\",\"mean\"), count=(\"y\",\"size\"))\n",
    "              .reset_index(drop=True)\n",
    "              .sort_values(\"feat_mean\")\n",
    "        )\n",
    "        x_vals = grp[\"feat_mean\"].values\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 4.5))\n",
    "    ax.plot(x_vals, grp[\"event_rate\"], label=\"Actual fault rate\",\n",
    "            marker=style_settings[\"Actual fault rate\"][\"marker\"],\n",
    "            linestyle=style_settings[\"Actual fault rate\"][\"linestyle\"],\n",
    "            linewidth=style_settings[\"Actual fault rate\"][\"linewidth\"])\n",
    "    ax.plot(x_vals, grp[\"pred_mean\"], label=\"Predicted fault prob.\",\n",
    "            marker=style_settings[\"Predicted fault prob.\"][\"marker\"],\n",
    "            linestyle=style_settings[\"Predicted fault prob.\"][\"linestyle\"],\n",
    "            linewidth=style_settings[\"Predicted fault prob.\"][\"linewidth\"])\n",
    "    ax.set_xlabel(feat)\n",
    "    ax.set_ylabel(\"Probability of any fault in year\")\n",
    "    ax.set_title(f\"{feat} vs risk (binned, {TEST_YEAR})  [n={int(grp['count'].sum())}]\")\n",
    "    ax.legend(); ax.grid(alpha=0.3)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(OUT_DIR / \"CORR\" / f\"rel_{feat}_{TEST_YEAR}.png\", dpi=180)\n",
    "    plt.close(fig)\n",
    "\n",
    "# Permutation-importance bar (holds default scoring semantics)\n",
    "fig, ax = plt.subplots(figsize=(8, 0.4 * max(1, len(perm_mean)) + 1))\n",
    "ax.barh(perm_mean.index, perm_mean.values)\n",
    "ax.set_xlabel(\"Permutation importance (Δ accuracy)\")\n",
    "ax.set_title(f\"Feature importance on {TEST_YEAR}\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(OUT_DIR / f\"imp_permutation_{TEST_YEAR}.png\", dpi=180)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"All plots saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c029d025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaabfc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b33d2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data-driven weights (sum should be 1.0):\n",
      "  a: 0.01777\n",
      "  c: 0.04559\n",
      "  f: 0.17541\n",
      "  i: 0.12673\n",
      "  l: 0.24495\n",
      "  p: 0.15000\n",
      "  r: 0.05332\n",
      "  s: 0.07495\n",
      "  u: 0.11127\n",
      "Sum = 1.0\n"
     ]
    }
   ],
   "source": [
    "# ----- Make W proportional to permutation importance -----\n",
    "\n",
    "# 1) Map model features to health-score factors\n",
    "feat2factor = {\n",
    "    \"cycle_pm\":                \"c\",\n",
    "    \"load_range_idx\":          \"r\",\n",
    "    \"log1p_age_frac\":          \"a\",\n",
    "    \"age_frac\":                \"a\",\n",
    "    \"age_years\":               \"a\",\n",
    "\n",
    "    \"log1p_hist_faults_cum\":   \"f\",\n",
    "    \"hist_faults_cum\":         \"f\",\n",
    "\n",
    "    \"joint_density\":           \"s\",\n",
    "\n",
    "    \"i_mtbf_inv\":              \"i\",\n",
    "\n",
    "    \"log1p_LENGTH_KM\":         \"l\",\n",
    "\n",
    "    # u (recent faults): prefer 24m, fall back to 12m if present\n",
    "    \"recent_24m\":              \"u\",\n",
    "    \"recent_12m\":              \"u\",\n",
    "}\n",
    "\n",
    "# 2) Collect permutation importance into factors\n",
    "#    (perm_mean is a Series: index=feature names, values=ΔAUROC)\n",
    "factor_scores = {k: 0.0 for k in list(\"crasifulu\")}  # c r a f s i l u (order doesn’t matter)\n",
    "for feat, imp in perm_mean.items():\n",
    "    f = feat2factor.get(feat)\n",
    "    if f is not None:\n",
    "        factor_scores[f] += max(0.0, float(imp))  # guard against tiny negative noise\n",
    "\n",
    "# 3) If some factors are 0 (e.g., feature missing), keep them at 0.\n",
    "#    Normalize the eight factors to sum to 0.85; keep p fixed at 0.15.\n",
    "sum_imp = sum(factor_scores.values())\n",
    "if sum_imp <= 0:\n",
    "    # fallback: split 0.85 equally if everything is zero\n",
    "    per = 0.85 / 8.0\n",
    "    for k in factor_scores: factor_scores[k] = per\n",
    "else:\n",
    "    for k in factor_scores:\n",
    "        factor_scores[k] = 0.85 * (factor_scores[k] / sum_imp)\n",
    "\n",
    "W = dict(**factor_scores, p=0.15)  # add p term\n",
    "\n",
    "# 4) (Optional) Pretty print + sanity checks\n",
    "print(\"Data-driven weights (sum should be 1.0):\")\n",
    "for k in sorted(W.keys()):\n",
    "    print(f\"  {k}: {W[k]:.5f}\")\n",
    "print(\"Sum =\", round(sum(W.values()), 6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccee5e82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
