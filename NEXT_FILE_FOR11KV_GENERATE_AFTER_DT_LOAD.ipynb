{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c4be53-3a75-49d2-9364-9088953251fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      DT_LOAD  DT_LOAD_CAL\n",
      "0  127.425882     7.040131\n",
      "1  233.628927    12.907725\n",
      "2  134.062123     7.406775\n",
      "3  245.587090    13.568399\n",
      "4  364.485990    20.137424\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Read your CSV\n",
    "df = pd.read_csv(\"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/final_two_column_with_rank_11_withoutDT.csv\")\n",
    "\n",
    "# 2. Compute the constant denominator: 11 * 0.95 * √3\n",
    "den = 11 * 0.95 * np.sqrt(3)\n",
    "\n",
    "# 3. Create the new column\n",
    "df[\"DT_LOAD_CAL\"] = df[\"DT_LOAD\"] / den\n",
    "\n",
    "# 4. (Optional) inspect first few rows\n",
    "print(df[[\"DT_LOAD\", \"DT_LOAD_CAL\"]].head())\n",
    "\n",
    "# 5. Save to a new CSV (or overwrite)\n",
    "df.to_csv(\"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/fedder_11KV_with_DT_LOAD_CAL.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96da7a8b",
   "metadata": {},
   "source": [
    "MAP THE LOAD OF MEANHLY AVERGAE WITH THE FEEDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2300ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load your files\n",
    "sw = pd.read_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/CSV FILE/Monthly_average_from_scada/monthly_SWNO_matrix_numpy.csv\")       # contains SWNO, Month_01…Month_12\n",
    "feeder = pd.read_csv(\"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/fedder_11KV_with_DT_LOAD_CAL.csv\")      # contains feeder_id, DT_LOAD_CAL\n",
    "\n",
    "# 2) Build a lookup from feeder_id → DT_LOAD_CAL\n",
    "dt_map = feeder.set_index(\"FEEDER_ID\")[\"DT_LOAD_CAL\"].to_dict()\n",
    "\n",
    "# 3) Prepare for the walk:\n",
    "#    We'll keep track of the \"current\" DT_LOAD_CAL and the last calculated values for each month.\n",
    "current_dt = None\n",
    "prev_vals = {f\"Month_{i:02d}\": None for i in range(1,13)}\n",
    "\n",
    "# 4) Iterate rows in order, computing chain‐subtract columns:\n",
    "out_rows = []\n",
    "for _, row in sw.iterrows():\n",
    "    swno = row[\"SWNO\"]\n",
    "    # If this SWNO is a feeder anchor, reset current_dt and prev_vals\n",
    "    if swno in dt_map:\n",
    "        current_dt = dt_map[swno]\n",
    "        # initialize prev_vals for this anchor: first subtraction\n",
    "        for i in range(1,13):\n",
    "            m = f\"Month_{i:02d}\"\n",
    "            prev_vals[m] = row[m] - current_dt\n",
    "    else:\n",
    "        # continue chain: subtract the same current_dt from last prev_vals\n",
    "        for i in range(1,13):\n",
    "            m = f\"Month_{i:02d}\"\n",
    "            prev_vals[m] = prev_vals[m] - current_dt\n",
    "\n",
    "    # build output row\n",
    "    out = row.to_dict()\n",
    "    # add the new _CAL columns\n",
    "    for i in range(1,13):\n",
    "        m = f\"Month_{i:02d}\"\n",
    "        out[f\"{m}_CAL\"] = prev_vals[m]\n",
    "    out_rows.append(out)\n",
    "\n",
    "# 5) Assemble and write out\n",
    "out_df = pd.DataFrame(out_rows)\n",
    "out_df.to_csv(\"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/feeder_11Kv_sw_with_chain_calculations.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6d4ff7",
   "metadata": {},
   "source": [
    "COMMULATIVE DIFFERENCE FOR MONTHLY AVERAGE LOAD OF SCADA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f9e1df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/final_feeder_11kv_monthly_cumulative_diff.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "feeder_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/fedder_11KV_with_DT_LOAD_CAL.csv\"\n",
    "monthly_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/CSV FILE/Monthly_average_from_scada/monthly_SWNO_matrix_numpy.csv\"\n",
    "output_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/final_feeder_11kv_monthly_cumulative_diff.csv\"\n",
    "\n",
    "\n",
    "# Load data\n",
    "feeder_df = pd.read_csv(feeder_path)\n",
    "monthly_df = pd.read_csv(monthly_path)\n",
    "\n",
    "# Merge on FEEDER_ID <-> SWNO\n",
    "merged = feeder_df.merge(monthly_df, left_on='FEEDER_ID', right_on='SWNO', how='left')\n",
    "\n",
    "month_cols = [f'Month_{i:02d}' for i in range(1, 13)]\n",
    "\n",
    "result_rows = []\n",
    "\n",
    "for feeder_id, group in merged.groupby('FEEDER_ID', sort=False):\n",
    "    group = group.copy()\n",
    "    group['__orig_idx'] = group.index\n",
    "    ranks_in_order = group['RANK'].drop_duplicates().tolist()\n",
    "\n",
    "    dt_load_sums = {\n",
    "        rank: group[group['RANK'] == rank]['DT_LOAD_CAL'].fillna(0).sum()\n",
    "        for rank in ranks_in_order\n",
    "    }\n",
    "\n",
    "    prev_cum = {m: None for m in month_cols}\n",
    "\n",
    "    for rank in ranks_in_order:\n",
    "        this_rank_rows = group[group['RANK'] == rank].sort_values('__orig_idx')\n",
    "        first = True\n",
    "        for idx, row in this_rank_rows.iterrows():\n",
    "            base = {col: row[col] for col in feeder_df.columns}\n",
    "            for m in month_cols:\n",
    "                month_val = row.get(m, None)\n",
    "                if first:\n",
    "                    # Only for the first row of this rank\n",
    "                    if rank == ranks_in_order[0]:\n",
    "                        cum_val = month_val - dt_load_sums[rank] if pd.notna(month_val) else None\n",
    "                    else:\n",
    "                        prev = prev_cum[m]\n",
    "                        cum_val = prev - dt_load_sums[rank] if prev is not None else None\n",
    "                    prev_cum[m] = cum_val\n",
    "                else:\n",
    "                    cum_val = None  # For subsequent rows at same rank, leave blank\n",
    "                base[f\"{m}_cumulative_diff\"] = cum_val\n",
    "            result_rows.append(base)\n",
    "            first = False\n",
    "\n",
    "result_df = pd.DataFrame(result_rows)\n",
    "output_cols = list(feeder_df.columns) + [f\"{m}_cumulative_diff\" for m in month_cols]\n",
    "result_df.to_csv(output_path, index=False, columns=output_cols)\n",
    "print(f\"Saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc4db3",
   "metadata": {},
   "source": [
    "AFTER THE RANKING AND USE ONLY MAIN CHAIN DONT USING SUBCHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25cd01dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      DT_LOAD  DT_LOAD_CAL\n",
      "0  616.320000    34.050959\n",
      "1  674.080488    37.242158\n",
      "2  473.442574    26.157148\n",
      "3  148.855367     8.224085\n",
      "4  400.698507    22.138124\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Read your CSV\n",
    "df = pd.read_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/11_WITHOUTDT_CONNECTED_RANKED.csv\")\n",
    "\n",
    "# 2. Compute the constant denominator: 11 * 0.95 * √3\n",
    "den = 11 * 0.95 * np.sqrt(3)\n",
    "\n",
    "# 3. Create the new column\n",
    "df[\"DT_LOAD_CAL\"] = df[\"DT_LOAD\"] / den\n",
    "\n",
    "# 4. (Optional) inspect first few rows\n",
    "print(df[[\"DT_LOAD\", \"DT_LOAD_CAL\"]].head())\n",
    "\n",
    "# 5. Save to a new CSV (or overwrite)\n",
    "df.to_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/fedder_11KV_with_DT_LOAD_CAL.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0c7dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load your files\n",
    "sw = pd.read_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/CSV FILE/Monthly_average_from_scada/monthly_SWNO_matrix_numpy.csv\")       # contains SWNO, Month_01…Month_12\n",
    "feeder = pd.read_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/fedder_11KV_with_DT_LOAD_CAL.csv\")      # contains feeder_id, DT_LOAD_CAL\n",
    "\n",
    "# 2) Build a lookup from feeder_id → DT_LOAD_CAL\n",
    "dt_map = feeder.set_index(\"FEEDER_ID\")[\"DT_LOAD_CAL\"].to_dict()\n",
    "\n",
    "# 3) Prepare for the walk:\n",
    "#    We'll keep track of the \"current\" DT_LOAD_CAL and the last calculated values for each month.\n",
    "current_dt = None\n",
    "prev_vals = {f\"Month_{i:02d}\": None for i in range(1,13)}\n",
    "\n",
    "# 4) Iterate rows in order, computing chain‐subtract columns:\n",
    "out_rows = []\n",
    "for _, row in sw.iterrows():\n",
    "    swno = row[\"SWNO\"]\n",
    "    # If this SWNO is a feeder anchor, reset current_dt and prev_vals\n",
    "    if swno in dt_map:\n",
    "        current_dt = dt_map[swno]\n",
    "        # initialize prev_vals for this anchor: first subtraction\n",
    "        for i in range(1,13):\n",
    "            m = f\"Month_{i:02d}\"\n",
    "            prev_vals[m] = row[m] - current_dt\n",
    "    else:\n",
    "        # continue chain: subtract the same current_dt from last prev_vals\n",
    "        for i in range(1,13):\n",
    "            m = f\"Month_{i:02d}\"\n",
    "            prev_vals[m] = prev_vals[m] - current_dt\n",
    "\n",
    "    # build output row\n",
    "    out = row.to_dict()\n",
    "    # add the new _CAL columns\n",
    "    for i in range(1,13):\n",
    "        m = f\"Month_{i:02d}\"\n",
    "        out[f\"{m}_CAL\"] = prev_vals[m]\n",
    "    out_rows.append(out)\n",
    "\n",
    "# 5) Assemble and write out\n",
    "out_df = pd.DataFrame(out_rows)\n",
    "out_df.to_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/feeder_11Kv_sw_with_chain_calculations.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f50870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths (change as needed)\n",
    "feeder_path = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/fedder_11KV_with_DT_LOAD_CAL.csv\"\n",
    "daily_path_1 = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/daily_SWNO_matrix_11KV_YEAR2025.csv\"\n",
    "daily_path_2 = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/daily_SWNO_matrix_11KV_YEAR2024.csv\"\n",
    "output_path_1 = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/final_feeder_11kv_daily_cumulative_diff_2025.csv\"\n",
    "output_path_2 = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/final_feeder_11kv_daily_cumulative_diff_2024.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "542534c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feeder_df = pd.read_csv(feeder_path)\n",
    "\n",
    "def is_main_chain_only(ranks):\n",
    "    return all('.' not in str(r) for r in ranks)\n",
    "\n",
    "main_chain_flags = (\n",
    "    feeder_df.groupby('FEEDER_ID')['RANK']\n",
    "    .apply(is_main_chain_only)\n",
    "    .rename('IS_MAIN_CHAIN_ONLY')\n",
    "    .reset_index()\n",
    ")\n",
    "main_chain_ids = main_chain_flags[main_chain_flags['IS_MAIN_CHAIN_ONLY']]['FEEDER_ID'].tolist()\n",
    "main_chain_feeder_df = feeder_df[feeder_df['FEEDER_ID'].isin(main_chain_ids)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7788661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/final_feeder_11kv_daily_cumulative_diff_2025.csv\n",
      "Saved to /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/final_feeder_11kv_daily_cumulative_diff_2024.csv\n"
     ]
    }
   ],
   "source": [
    "def process_year(main_chain_feeder_df, daily_path, output_path):\n",
    "    daily_df = pd.read_csv(daily_path)\n",
    "    day_cols = [col for col in daily_df.columns if col.startswith(\"Day_\")]\n",
    "\n",
    "    merged = main_chain_feeder_df.merge(daily_df, left_on='FEEDER_ID', right_on='SWNO', how='left')\n",
    "\n",
    "    result_rows = []\n",
    "\n",
    "    for feeder_id, group in merged.groupby('FEEDER_ID', sort=False):\n",
    "        group = group.copy()\n",
    "        group['__orig_idx'] = group.index\n",
    "        ranks_in_order = group['RANK'].drop_duplicates().tolist()\n",
    "\n",
    "        dt_load_sums = {\n",
    "            rank: group[group['RANK'] == rank]['DT_LOAD_CAL'].fillna(0).sum()\n",
    "            for rank in ranks_in_order\n",
    "        }\n",
    "\n",
    "        prev_cum = {d: None for d in day_cols}\n",
    "\n",
    "        for rank in ranks_in_order:\n",
    "            this_rank_rows = group[group['RANK'] == rank].sort_values('__orig_idx')\n",
    "            first = True\n",
    "            for idx, row in this_rank_rows.iterrows():\n",
    "                base = {col: row[col] for col in feeder_df.columns}\n",
    "                for d in day_cols:\n",
    "                    day_val = row.get(d, None)\n",
    "                    if first:\n",
    "                        if rank == ranks_in_order[0]:\n",
    "                            cum_val = day_val - dt_load_sums[rank] if pd.notna(day_val) else None\n",
    "                        else:\n",
    "                            prev = prev_cum[d]\n",
    "                            cum_val = prev - dt_load_sums[rank] if prev is not None else None\n",
    "                        prev_cum[d] = cum_val\n",
    "                    else:\n",
    "                        cum_val = None\n",
    "                    base[f\"{d}_cumulative_diff\"] = cum_val\n",
    "                result_rows.append(base)\n",
    "                first = False\n",
    "\n",
    "    result_df = pd.DataFrame(result_rows)\n",
    "    output_cols = list(feeder_df.columns) + [f\"{d}_cumulative_diff\" for d in day_cols]\n",
    "    result_df.to_csv(output_path, index=False, columns=output_cols)\n",
    "    print(f\"Saved to {output_path}\")\n",
    "\n",
    "# Process each year\n",
    "process_year(main_chain_feeder_df, daily_path_1, output_path_1)\n",
    "process_year(main_chain_feeder_df, daily_path_2, output_path_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17c84863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/final_feeder_11kv_daily_cumulative_diff_2025.csv\n",
      "Saved to /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/final_feeder_11kv_daily_cumulative_diff_2024.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Identify only main-chain feeders (no sub-branches) ---\n",
    "feeder_df = pd.read_csv(feeder_path)\n",
    "\n",
    "def is_main_chain_only(ranks):\n",
    "    return all('.' not in str(r) for r in ranks)\n",
    "\n",
    "main_chain_flags = (\n",
    "    feeder_df.groupby('FEEDER_ID')['RANK']\n",
    "    .apply(is_main_chain_only)\n",
    "    .rename('IS_MAIN_CHAIN_ONLY')\n",
    "    .reset_index()\n",
    ")\n",
    "main_chain_ids = main_chain_flags[main_chain_flags['IS_MAIN_CHAIN_ONLY']]['FEEDER_ID'].tolist()\n",
    "main_chain_feeder_df = feeder_df[feeder_df['FEEDER_ID'].isin(main_chain_ids)].copy()\n",
    "\n",
    "# --- Processing function ---\n",
    "def process_year(main_chain_feeder_df, daily_path, output_path):\n",
    "    daily_df = pd.read_csv(daily_path)\n",
    "    day_cols = [col for col in daily_df.columns if col.startswith(\"Day_\")]\n",
    "\n",
    "    merged = main_chain_feeder_df.merge(daily_df, left_on='FEEDER_ID', right_on='SWNO', how='left')\n",
    "\n",
    "    result_rows = []\n",
    "\n",
    "    for feeder_id, group in merged.groupby('FEEDER_ID', sort=False):\n",
    "        group = group.copy()\n",
    "        group['__orig_idx'] = group.index\n",
    "        ranks_in_order = group['RANK'].drop_duplicates().tolist()\n",
    "\n",
    "        dt_load_sums = {\n",
    "            rank: group[group['RANK'] == rank]['DT_LOAD_CAL'].fillna(0).sum()\n",
    "            for rank in ranks_in_order\n",
    "        }\n",
    "\n",
    "        prev_cum = {d: None for d in day_cols}\n",
    "\n",
    "        for rank in ranks_in_order:\n",
    "            this_rank_rows = group[group['RANK'] == rank].sort_values('__orig_idx')\n",
    "            first = True\n",
    "            for idx, row in this_rank_rows.iterrows():\n",
    "                base = {col: row[col] for col in main_chain_feeder_df.columns}\n",
    "                for d in day_cols:\n",
    "                    day_val = row.get(d, None)\n",
    "                    if first:\n",
    "                        if rank == ranks_in_order[0]:\n",
    "                            cum_val = day_val - dt_load_sums[rank] if pd.notna(day_val) else None\n",
    "                        else:\n",
    "                            prev = prev_cum[d]\n",
    "                            cum_val = prev - dt_load_sums[rank] if prev is not None else None\n",
    "                        prev_cum[d] = cum_val\n",
    "                    else:\n",
    "                        cum_val = None\n",
    "                    base[f\"{d}_cumulative_diff\"] = cum_val\n",
    "                result_rows.append(base)\n",
    "                first = False\n",
    "\n",
    "    result_df = pd.DataFrame(result_rows)\n",
    "    # --- Add FROM_SWITCH and TO_SWITCH columns ---\n",
    "    if \"FROM_TO\" in result_df.columns:\n",
    "        from_to_split = result_df[\"FROM_TO\"].str.split(\"-\", n=1, expand=True)\n",
    "        result_df[\"FROM_SWITCH\"] = from_to_split[0]\n",
    "        result_df[\"TO_SWITCH\"] = from_to_split[1]\n",
    "        cols = list(result_df.columns)\n",
    "        i = cols.index(\"FROM_TO\") + 1\n",
    "        for col in [\"FROM_SWITCH\", \"TO_SWITCH\"]:\n",
    "            cols.insert(i, cols.pop(cols.index(col)))\n",
    "            i += 1\n",
    "        result_df = result_df[cols]\n",
    "    # --- Save ---\n",
    "    output_cols = list(result_df.columns)  # Save all columns, now includes FROM_SWITCH and TO_SWITCH\n",
    "    result_df.to_csv(output_path, index=False, columns=output_cols)\n",
    "    print(f\"Saved to {output_path}\")\n",
    "\n",
    "# --- Process each year ---\n",
    "process_year(main_chain_feeder_df, daily_path_1, output_path_1)\n",
    "process_year(main_chain_feeder_df, daily_path_2, output_path_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbda874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
