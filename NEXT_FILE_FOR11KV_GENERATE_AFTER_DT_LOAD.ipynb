{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c4be53-3a75-49d2-9364-9088953251fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      DT_LOAD  DT_LOAD_CAL\n",
      "0  127.425882     7.040131\n",
      "1  233.628927    12.907725\n",
      "2  134.062123     7.406775\n",
      "3  245.587090    13.568399\n",
      "4  364.485990    20.137424\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Read your CSV\n",
    "df = pd.read_csv(\"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/final_two_column_with_rank_11_withoutDT.csv\")\n",
    "\n",
    "# 2. Compute the constant denominator: 11 * 0.95 * √3\n",
    "den = 11 * 0.95 * np.sqrt(3)\n",
    "\n",
    "# 3. Create the new column\n",
    "df[\"DT_LOAD_CAL\"] = df[\"DT_LOAD\"] / den\n",
    "\n",
    "# 4. (Optional) inspect first few rows\n",
    "print(df[[\"DT_LOAD\", \"DT_LOAD_CAL\"]].head())\n",
    "\n",
    "# 5. Save to a new CSV (or overwrite)\n",
    "df.to_csv(\"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/fedder_11KV_with_DT_LOAD_CAL.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96da7a8b",
   "metadata": {},
   "source": [
    "MAP THE LOAD OF MEANHLY AVERGAE WITH THE FEEDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2300ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load your files\n",
    "sw = pd.read_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/CSV FILE/Monthly_average_from_scada/monthly_SWNO_matrix_numpy.csv\")       # contains SWNO, Month_01…Month_12\n",
    "feeder = pd.read_csv(\"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/fedder_11KV_with_DT_LOAD_CAL.csv\")      # contains feeder_id, DT_LOAD_CAL\n",
    "\n",
    "# 2) Build a lookup from feeder_id → DT_LOAD_CAL\n",
    "dt_map = feeder.set_index(\"FEEDER_ID\")[\"DT_LOAD_CAL\"].to_dict()\n",
    "\n",
    "# 3) Prepare for the walk:\n",
    "#    We'll keep track of the \"current\" DT_LOAD_CAL and the last calculated values for each month.\n",
    "current_dt = None\n",
    "prev_vals = {f\"Month_{i:02d}\": None for i in range(1,13)}\n",
    "\n",
    "# 4) Iterate rows in order, computing chain‐subtract columns:\n",
    "out_rows = []\n",
    "for _, row in sw.iterrows():\n",
    "    swno = row[\"SWNO\"]\n",
    "    # If this SWNO is a feeder anchor, reset current_dt and prev_vals\n",
    "    if swno in dt_map:\n",
    "        current_dt = dt_map[swno]\n",
    "        # initialize prev_vals for this anchor: first subtraction\n",
    "        for i in range(1,13):\n",
    "            m = f\"Month_{i:02d}\"\n",
    "            prev_vals[m] = row[m] - current_dt\n",
    "    else:\n",
    "        # continue chain: subtract the same current_dt from last prev_vals\n",
    "        for i in range(1,13):\n",
    "            m = f\"Month_{i:02d}\"\n",
    "            prev_vals[m] = prev_vals[m] - current_dt\n",
    "\n",
    "    # build output row\n",
    "    out = row.to_dict()\n",
    "    # add the new _CAL columns\n",
    "    for i in range(1,13):\n",
    "        m = f\"Month_{i:02d}\"\n",
    "        out[f\"{m}_CAL\"] = prev_vals[m]\n",
    "    out_rows.append(out)\n",
    "\n",
    "# 5) Assemble and write out\n",
    "out_df = pd.DataFrame(out_rows)\n",
    "out_df.to_csv(\"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/feeder_11Kv_sw_with_chain_calculations.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d0200",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a6d4ff7",
   "metadata": {},
   "source": [
    "COMMULATIVE DIFFERENCE FOR MONTHLY AVERAGE LOAD OF SCADA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f9e1df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/final_feeder_11kv_monthly_cumulative_diff.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "feeder_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/fedder_11KV_with_DT_LOAD_CAL.csv\"\n",
    "monthly_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/CSV FILE/Monthly_average_from_scada/monthly_SWNO_matrix_numpy.csv\"\n",
    "output_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/final_feeder_11kv_monthly_cumulative_diff.csv\"\n",
    "\n",
    "\n",
    "# Load data\n",
    "feeder_df = pd.read_csv(feeder_path)\n",
    "monthly_df = pd.read_csv(monthly_path)\n",
    "\n",
    "# Merge on FEEDER_ID <-> SWNO\n",
    "merged = feeder_df.merge(monthly_df, left_on='FEEDER_ID', right_on='SWNO', how='left')\n",
    "\n",
    "month_cols = [f'Month_{i:02d}' for i in range(1, 13)]\n",
    "\n",
    "result_rows = []\n",
    "\n",
    "for feeder_id, group in merged.groupby('FEEDER_ID', sort=False):\n",
    "    group = group.copy()\n",
    "    group['__orig_idx'] = group.index\n",
    "    ranks_in_order = group['RANK'].drop_duplicates().tolist()\n",
    "\n",
    "    dt_load_sums = {\n",
    "        rank: group[group['RANK'] == rank]['DT_LOAD_CAL'].fillna(0).sum()\n",
    "        for rank in ranks_in_order\n",
    "    }\n",
    "\n",
    "    prev_cum = {m: None for m in month_cols}\n",
    "\n",
    "    for rank in ranks_in_order:\n",
    "        this_rank_rows = group[group['RANK'] == rank].sort_values('__orig_idx')\n",
    "        first = True\n",
    "        for idx, row in this_rank_rows.iterrows():\n",
    "            base = {col: row[col] for col in feeder_df.columns}\n",
    "            for m in month_cols:\n",
    "                month_val = row.get(m, None)\n",
    "                if first:\n",
    "                    # Only for the first row of this rank\n",
    "                    if rank == ranks_in_order[0]:\n",
    "                        cum_val = month_val - dt_load_sums[rank] if pd.notna(month_val) else None\n",
    "                    else:\n",
    "                        prev = prev_cum[m]\n",
    "                        cum_val = prev - dt_load_sums[rank] if prev is not None else None\n",
    "                    prev_cum[m] = cum_val\n",
    "                else:\n",
    "                    cum_val = None  # For subsequent rows at same rank, leave blank\n",
    "                base[f\"{m}_cumulative_diff\"] = cum_val\n",
    "            result_rows.append(base)\n",
    "            first = False\n",
    "\n",
    "result_df = pd.DataFrame(result_rows)\n",
    "output_cols = list(feeder_df.columns) + [f\"{m}_cumulative_diff\" for m in month_cols]\n",
    "result_df.to_csv(output_path, index=False, columns=output_cols)\n",
    "print(f\"Saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc4db3",
   "metadata": {},
   "source": [
    "AFTER THE RANKING AND USE ONLY MAIN CHAIN DONT USING SUBCHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25cd01dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      DT_LOAD  DT_LOAD_CAL\n",
      "0  616.320000    34.050959\n",
      "1  674.080488    37.242158\n",
      "2  473.442574    26.157148\n",
      "3  148.855367     8.224085\n",
      "4  400.698507    22.138124\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Read your CSV\n",
    "df = pd.read_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/11_WITHOUTDT_CONNECTED_RANKED.csv\")\n",
    "\n",
    "# 2. Compute the constant denominator: 11 * 0.95 * √3\n",
    "den = 11 * 0.95 * np.sqrt(3)\n",
    "\n",
    "# 3. Create the new column\n",
    "df[\"DT_LOAD_CAL\"] = df[\"DT_LOAD\"] / den\n",
    "\n",
    "# 4. (Optional) inspect first few rows\n",
    "print(df[[\"DT_LOAD\", \"DT_LOAD_CAL\"]].head())\n",
    "\n",
    "# 5. Save to a new CSV (or overwrite)\n",
    "df.to_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/fedder_11KV_with_DT_LOAD_CAL.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0c7dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load your files\n",
    "sw = pd.read_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/CSV FILE/Monthly_average_from_scada/monthly_SWNO_matrix_numpy.csv\")       # contains SWNO, Month_01…Month_12\n",
    "feeder = pd.read_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/fedder_11KV_with_DT_LOAD_CAL.csv\")      # contains feeder_id, DT_LOAD_CAL\n",
    "\n",
    "# 2) Build a lookup from feeder_id → DT_LOAD_CAL\n",
    "dt_map = feeder.set_index(\"FEEDER_ID\")[\"DT_LOAD_CAL\"].to_dict()\n",
    "\n",
    "# 3) Prepare for the walk:\n",
    "#    We'll keep track of the \"current\" DT_LOAD_CAL and the last calculated values for each month.\n",
    "current_dt = None\n",
    "prev_vals = {f\"Month_{i:02d}\": None for i in range(1,13)}\n",
    "\n",
    "# 4) Iterate rows in order, computing chain‐subtract columns:\n",
    "out_rows = []\n",
    "for _, row in sw.iterrows():\n",
    "    swno = row[\"SWNO\"]\n",
    "    # If this SWNO is a feeder anchor, reset current_dt and prev_vals\n",
    "    if swno in dt_map:\n",
    "        current_dt = dt_map[swno]\n",
    "        # initialize prev_vals for this anchor: first subtraction\n",
    "        for i in range(1,13):\n",
    "            m = f\"Month_{i:02d}\"\n",
    "            prev_vals[m] = row[m] - current_dt\n",
    "    else:\n",
    "        # continue chain: subtract the same current_dt from last prev_vals\n",
    "        for i in range(1,13):\n",
    "            m = f\"Month_{i:02d}\"\n",
    "            prev_vals[m] = prev_vals[m] - current_dt\n",
    "\n",
    "    # build output row\n",
    "    out = row.to_dict()\n",
    "    # add the new _CAL columns\n",
    "    for i in range(1,13):\n",
    "        m = f\"Month_{i:02d}\"\n",
    "        out[f\"{m}_CAL\"] = prev_vals[m]\n",
    "    out_rows.append(out)\n",
    "\n",
    "# 5) Assemble and write out\n",
    "out_df = pd.DataFrame(out_rows)\n",
    "out_df.to_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/feeder_11Kv_sw_with_chain_calculations.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f50870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths (change as needed)\n",
    "feeder_path = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/fedder_11KV_with_DT_LOAD_CAL.csv\"\n",
    "daily_path_1 = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/daily_SWNO_matrix_11KV_YEAR2025.csv\"\n",
    "daily_path_2 = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/daily_SWNO_matrix_11KV_YEAR2024.csv\"\n",
    "output_path_1 = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/final_feeder_11kv_daily_cumulative_diff_2025.csv\"\n",
    "output_path_2 = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/final_feeder_11kv_daily_cumulative_diff_2024.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "542534c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feeder_df = pd.read_csv(feeder_path)\n",
    "\n",
    "def is_main_chain_only(ranks):\n",
    "    return all('.' not in str(r) for r in ranks)\n",
    "\n",
    "main_chain_flags = (\n",
    "    feeder_df.groupby('FEEDER_ID')['RANK']\n",
    "    .apply(is_main_chain_only)\n",
    "    .rename('IS_MAIN_CHAIN_ONLY')\n",
    "    .reset_index()\n",
    ")\n",
    "main_chain_ids = main_chain_flags[main_chain_flags['IS_MAIN_CHAIN_ONLY']]['FEEDER_ID'].tolist()\n",
    "main_chain_feeder_df = feeder_df[feeder_df['FEEDER_ID'].isin(main_chain_ids)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7788661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/final_feeder_11kv_daily_cumulative_diff_2025.csv\n",
      "Saved to /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/final_feeder_11kv_daily_cumulative_diff_2024.csv\n"
     ]
    }
   ],
   "source": [
    "def process_year(main_chain_feeder_df, daily_path, output_path):\n",
    "    daily_df = pd.read_csv(daily_path)\n",
    "    day_cols = [col for col in daily_df.columns if col.startswith(\"Day_\")]\n",
    "\n",
    "    merged = main_chain_feeder_df.merge(daily_df, left_on='FEEDER_ID', right_on='SWNO', how='left')\n",
    "\n",
    "    result_rows = []\n",
    "\n",
    "    for feeder_id, group in merged.groupby('FEEDER_ID', sort=False):\n",
    "        group = group.copy()\n",
    "        group['__orig_idx'] = group.index\n",
    "        ranks_in_order = group['RANK'].drop_duplicates().tolist()\n",
    "\n",
    "        dt_load_sums = {\n",
    "            rank: group[group['RANK'] == rank]['DT_LOAD_CAL'].fillna(0).sum()\n",
    "            for rank in ranks_in_order\n",
    "        }\n",
    "\n",
    "        prev_cum = {d: None for d in day_cols}\n",
    "\n",
    "        for rank in ranks_in_order:\n",
    "            this_rank_rows = group[group['RANK'] == rank].sort_values('__orig_idx')\n",
    "            first = True\n",
    "            for idx, row in this_rank_rows.iterrows():\n",
    "                base = {col: row[col] for col in feeder_df.columns}\n",
    "                for d in day_cols:\n",
    "                    day_val = row.get(d, None)\n",
    "                    if first:\n",
    "                        if rank == ranks_in_order[0]:\n",
    "                            cum_val = day_val - dt_load_sums[rank] if pd.notna(day_val) else None\n",
    "                        else:\n",
    "                            prev = prev_cum[d]\n",
    "                            cum_val = prev - dt_load_sums[rank] if prev is not None else None\n",
    "                        prev_cum[d] = cum_val\n",
    "                    else:\n",
    "                        cum_val = None\n",
    "                    base[f\"{d}_cumulative_diff\"] = cum_val\n",
    "                result_rows.append(base)\n",
    "                first = False\n",
    "\n",
    "    result_df = pd.DataFrame(result_rows)\n",
    "    output_cols = list(feeder_df.columns) + [f\"{d}_cumulative_diff\" for d in day_cols]\n",
    "    result_df.to_csv(output_path, index=False, columns=output_cols)\n",
    "    print(f\"Saved to {output_path}\")\n",
    "\n",
    "# Process each year\n",
    "process_year(main_chain_feeder_df, daily_path_1, output_path_1)\n",
    "process_year(main_chain_feeder_df, daily_path_2, output_path_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17c84863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/final_feeder_11kv_daily_cumulative_diff_2025.csv\n",
      "Saved to /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/final_feeder_11kv_daily_cumulative_diff_2024.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Identify only main-chain feeders (no sub-branches) ---\n",
    "feeder_df = pd.read_csv(feeder_path)\n",
    "\n",
    "def is_main_chain_only(ranks):\n",
    "    return all('.' not in str(r) for r in ranks)\n",
    "\n",
    "main_chain_flags = (\n",
    "    feeder_df.groupby('FEEDER_ID')['RANK']\n",
    "    .apply(is_main_chain_only)\n",
    "    .rename('IS_MAIN_CHAIN_ONLY')\n",
    "    .reset_index()\n",
    ")\n",
    "main_chain_ids = main_chain_flags[main_chain_flags['IS_MAIN_CHAIN_ONLY']]['FEEDER_ID'].tolist()\n",
    "main_chain_feeder_df = feeder_df[feeder_df['FEEDER_ID'].isin(main_chain_ids)].copy()\n",
    "\n",
    "# --- Processing function ---\n",
    "def process_year(main_chain_feeder_df, daily_path, output_path):\n",
    "    daily_df = pd.read_csv(daily_path)\n",
    "    day_cols = [col for col in daily_df.columns if col.startswith(\"Day_\")]\n",
    "\n",
    "    merged = main_chain_feeder_df.merge(daily_df, left_on='FEEDER_ID', right_on='SWNO', how='left')\n",
    "\n",
    "    result_rows = []\n",
    "\n",
    "    for feeder_id, group in merged.groupby('FEEDER_ID', sort=False):\n",
    "        group = group.copy()\n",
    "        group['__orig_idx'] = group.index\n",
    "        ranks_in_order = group['RANK'].drop_duplicates().tolist()\n",
    "\n",
    "        dt_load_sums = {\n",
    "            rank: group[group['RANK'] == rank]['DT_LOAD_CAL'].fillna(0).sum()\n",
    "            for rank in ranks_in_order\n",
    "        }\n",
    "\n",
    "        prev_cum = {d: None for d in day_cols}\n",
    "\n",
    "        for rank in ranks_in_order:\n",
    "            this_rank_rows = group[group['RANK'] == rank].sort_values('__orig_idx')\n",
    "            first = True\n",
    "            for idx, row in this_rank_rows.iterrows():\n",
    "                base = {col: row[col] for col in main_chain_feeder_df.columns}\n",
    "                for d in day_cols:\n",
    "                    day_val = row.get(d, None)\n",
    "                    if first:\n",
    "                        if rank == ranks_in_order[0]:\n",
    "                            cum_val = day_val - dt_load_sums[rank] if pd.notna(day_val) else None\n",
    "                        else:\n",
    "                            prev = prev_cum[d]\n",
    "                            cum_val = prev - dt_load_sums[rank] if prev is not None else None\n",
    "                        prev_cum[d] = cum_val\n",
    "                    else:\n",
    "                        cum_val = None\n",
    "                    base[f\"{d}_cumulative_diff\"] = cum_val\n",
    "                result_rows.append(base)\n",
    "                first = False\n",
    "\n",
    "    result_df = pd.DataFrame(result_rows)\n",
    "    # --- Add FROM_SWITCH and TO_SWITCH columns ---\n",
    "    if \"FROM_TO\" in result_df.columns:\n",
    "        from_to_split = result_df[\"FROM_TO\"].str.split(\"-\", n=1, expand=True)\n",
    "        result_df[\"FROM_SWITCH\"] = from_to_split[0]\n",
    "        result_df[\"TO_SWITCH\"] = from_to_split[1]\n",
    "        cols = list(result_df.columns)\n",
    "        i = cols.index(\"FROM_TO\") + 1\n",
    "        for col in [\"FROM_SWITCH\", \"TO_SWITCH\"]:\n",
    "            cols.insert(i, cols.pop(cols.index(col)))\n",
    "            i += 1\n",
    "        result_df = result_df[cols]\n",
    "    # --- Save ---\n",
    "    output_cols = list(result_df.columns)  # Save all columns, now includes FROM_SWITCH and TO_SWITCH\n",
    "    result_df.to_csv(output_path, index=False, columns=output_cols)\n",
    "    print(f\"Saved to {output_path}\")\n",
    "\n",
    "# --- Process each year ---\n",
    "process_year(main_chain_feeder_df, daily_path_1, output_path_1)\n",
    "process_year(main_chain_feeder_df, daily_path_2, output_path_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b89993",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52fbb275",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c3bf827",
   "metadata": {},
   "source": [
    "NEW LOGIC TO CALCULATE THE LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbbda874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Wrote /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/fedder_11KV_with_all_monthly_loads.csv\n",
      "FEEDER_ID RANK   DESTINATION_LOCATION  node_load_2024_01  sec_load_2024_01\n",
      "    10205    1 1S-MH-MU-ZSC-CL09-2382           7.187435          7.187435\n",
      "    10205    2 1S-MH-MU-ZSC-CL09-2383           7.861029          7.861029\n",
      "    10205    3 1S-MH-MU-ZSC-CL09-2723           5.521219          8.892494\n",
      "    10205  3.1 1S-MH-MU-ZSC-CL09-2366           1.735930          1.735930\n",
      "    10205  3.2 1S-MH-MU-ZSC-CL09-2348           4.672888          4.672888\n",
      "    10205  3.3 1S-MH-MU-ZSC-CL09-2019           2.483676          2.483676\n",
      "    10205    4 1S-MH-MU-ZSC-CL09-2074           2.414947          8.299723\n",
      "    10205  4.1 1S-MH-MU-ZSC-CL06-3404           0.000000          0.000000\n",
      "    10205  4.2 1S-MH-MU-ZSC-CL06-3489           2.368178          2.368178\n",
      "    10205  4.3 1S-MH-MU-ZSC-CL06-2088           5.931545          5.931545\n",
      "    10205    5 1S-MH-MU-ZSC-CL09-3373           0.586638          0.586638\n",
      "    10205    6 1S-MH-MU-ZSC-CL09-3058           1.472870          1.472870\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Node & Section loads (bottom-up) + cumulative section loads.\n",
    "Data source for feeder monthly averages: monthly_SWNO_matrix_11KV.csv (SWNO == FEEDER_ID)\n",
    "\n",
    "Adds to output:\n",
    "  - FRACTION, SUM_DT_LOAD_CAL\n",
    "  - feeder_2024_01.._12\n",
    "  - node_load_2024_01.._12\n",
    "  - sec_load_2024_01.._12\n",
    "  - sec_cum_2024_01.._12\n",
    "\n",
    "Writes: fedder_11KV_with_all_monthly_loads.csv\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, math\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ── Paths ───────────────────────────────────────────────────────────────────\n",
    "BASE      = \"/media/sagark24/New Volume/MERGE CDIS\"\n",
    "IN_FEDDER = Path(f\"{BASE}/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/11_WITHOUTDT_CONNECTED_RANKED.csv\")\n",
    "IN_MATRIX = Path(f\"{BASE}/IPYNB_FILE/DATA_GENERATION/monthly_SWNO_matrix_11KV.csv\")\n",
    "OUT_FILE  = Path(f\"{BASE}/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/fedder_11KV_with_all_monthly_loads.csv\")\n",
    "\n",
    "# ── Month keys/tags ─────────────────────────────────────────────────────────\n",
    "YEAR = 2024\n",
    "MONTH_KEYS = [f\"Month_{i:02d}\" for i in range(1, 13)]      # matrix column names\n",
    "MONTH_TAGS = [f\"{YEAR}-{i:02d}\" for i in range(1, 13)]      # long format tags\n",
    "NODE_COLS  = [f\"node_load_{YEAR}_{i:02d}\" for i in range(1, 13)]\n",
    "SEC_COLS   = [f\"sec_load_{YEAR}_{i:02d}\"  for i in range(1, 13)]\n",
    "CUM_COLS   = [f\"sec_cum_{YEAR}_{i:02d}\"   for i in range(1, 13)]\n",
    "FEED_COLS  = [f\"feeder_{YEAR}_{i:02d}\"    for i in range(1, 13)]\n",
    "\n",
    "# ── Helpers ─────────────────────────────────────────────────────────────────\n",
    "def norm_id_series(s: pd.Series) -> pd.Series:\n",
    "    return s.astype(str).str.strip()\n",
    "\n",
    "def make_feed_month_from_matrix(mat: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return FEEDER_ID|MONTH|FEEDER_LOAD_AVG (complete Jan..Dec per feeder).\"\"\"\n",
    "    need = {\"SWNO\", *MONTH_KEYS}\n",
    "    miss = [c for c in need if c not in mat.columns]\n",
    "    if miss:\n",
    "        raise RuntimeError(f\"monthly matrix missing columns: {miss}\")\n",
    "\n",
    "    m = mat.copy()\n",
    "    m[\"FEEDER_ID\"] = norm_id_series(m[\"SWNO\"])\n",
    "    for c in MONTH_KEYS:\n",
    "        m[c] = pd.to_numeric(m[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    long = (m.melt(id_vars=[\"FEEDER_ID\"], value_vars=MONTH_KEYS,\n",
    "                   var_name=\"MONTH_KEY\", value_name=\"FEEDER_LOAD_AVG\")\n",
    "              .assign(MONTH=lambda d: d[\"MONTH_KEY\"].str.replace(\"Month_\", f\"{YEAR}-\", regex=False))\n",
    "              .drop(columns=[\"MONTH_KEY\"]))\n",
    "    complete = (pd.MultiIndex.from_product([m[\"FEEDER_ID\"].unique(), MONTH_TAGS],\n",
    "                                           names=[\"FEEDER_ID\",\"MONTH\"]).to_frame(index=False))\n",
    "    long = complete.merge(long, on=[\"FEEDER_ID\",\"MONTH\"], how=\"left\").fillna({\"FEEDER_LOAD_AVG\": 0.0})\n",
    "    return long\n",
    "\n",
    "def parent_rank(r: str) -> Optional[str]:\n",
    "    parts = str(r).split(\".\")\n",
    "    return \".\".join(parts[:-1]) if len(parts) > 1 else None\n",
    "\n",
    "# def compute_section_bottom_up(df_f: pd.DataFrame, node_cols: List[str]) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Bottom-up section aggregation for a single feeder.\n",
    "#     Input df_f must have columns: FEEDER_ID, RANK, DESTINATION_LOCATION, node_cols...\n",
    "#     Returns FEEDER_ID, RANK, DESTINATION_LOCATION, sec_cols..., cum_cols...\n",
    "#     \"\"\"\n",
    "#     cols_keep = [\"FEEDER_ID\",\"RANK\",\"DESTINATION_LOCATION\"] + node_cols\n",
    "#     sec = df_f[cols_keep].drop_duplicates([\"FEEDER_ID\",\"RANK\",\"DESTINATION_LOCATION\"]).copy()\n",
    "#     sec[node_cols] = sec[node_cols].fillna(0.0)\n",
    "#     sec[\"RANK\"] = sec[\"RANK\"].astype(str)\n",
    "\n",
    "#     # index maps\n",
    "#     idx_by_rank: Dict[str, int] = {r: i for i, r in enumerate(sec[\"RANK\"].tolist())}\n",
    "#     depth = sec[\"RANK\"].str.count(r\"\\.\").to_numpy() + 1\n",
    "\n",
    "#     # matrices\n",
    "#     M = sec[node_cols].to_numpy(dtype=float)  # self node loads (n x 12)\n",
    "#     T = M.copy()                               # totals → will accumulate children\n",
    "\n",
    "#     # process deepest → root\n",
    "#     for i in np.argsort(-depth):\n",
    "#         pr = parent_rank(sec.iat[i, sec.columns.get_loc(\"RANK\")])\n",
    "#         if pr and pr in idx_by_rank:\n",
    "#             T[idx_by_rank[pr], :] += T[i, :]\n",
    "\n",
    "#     # build outputs\n",
    "#     out = sec[[\"FEEDER_ID\",\"RANK\",\"DESTINATION_LOCATION\"]].copy()\n",
    "#     # sec monthly\n",
    "#     for j, col in enumerate(SEC_COLS):\n",
    "#         out[col] = T[:, j]\n",
    "#     # cumulative across months (axis=1, Jan→Dec)\n",
    "#     cum = np.cumsum(T, axis=1)\n",
    "#     for j, col in enumerate(CUM_COLS):\n",
    "#         out[col] = cum[:, j]\n",
    "#     return out\n",
    "def compute_section_bottom_up(df_f: pd.DataFrame, node_cols: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    LEAF-ONLY aggregation:\n",
    "      - Leaves contribute their node_load_*.\n",
    "      - Non-leaves contribute 0 as node load.\n",
    "      - For each parent, sec_load_* = sum of children's totals.\n",
    "      - Propagates upward (child → parent → grandparent...) for every month at once.\n",
    "\n",
    "    Input df_f must have: FEEDER_ID, RANK, DESTINATION_LOCATION, and node_cols.\n",
    "    Returns: FEEDER_ID, RANK, DESTINATION_LOCATION, sec_load_YYYY_MM (and can add cum if you like).\n",
    "    \"\"\"\n",
    "    # 1) Unique rows per section; ensure numeric matrix for all months\n",
    "    cols_keep = [\"FEEDER_ID\", \"RANK\", \"DESTINATION_LOCATION\"] + node_cols\n",
    "    sec = (df_f[cols_keep]\n",
    "           .drop_duplicates([\"FEEDER_ID\",\"RANK\",\"DESTINATION_LOCATION\"])\n",
    "           .copy())\n",
    "    sec[node_cols] = sec[node_cols].fillna(0.0)\n",
    "    sec[\"RANK\"] = sec[\"RANK\"].astype(str)\n",
    "\n",
    "    # 2) Build children map to identify leaves (nodes with no children)\n",
    "    ranks = sec[\"RANK\"].tolist()\n",
    "    # parent of \"a.b.c\" is \"a.b\"; root has no parent\n",
    "    def parent_rank(r: str) -> str | None:\n",
    "        parts = r.split(\".\")\n",
    "        return \".\".join(parts[:-1]) if len(parts) > 1 else None\n",
    "\n",
    "    # children lists\n",
    "    children = {r: [] for r in ranks}\n",
    "    for r in ranks:\n",
    "        p = parent_rank(r)\n",
    "        if p in children:\n",
    "            children[p].append(r)\n",
    "\n",
    "    is_leaf = np.array([len(children[r]) == 0 for r in ranks])\n",
    "\n",
    "    # 3) Node matrix M: keep node loads ONLY for leaves, zero for non-leaves\n",
    "    M = sec[node_cols].to_numpy(dtype=float)\n",
    "    M[~is_leaf, :] = 0.0  # <-- key change vs previous version\n",
    "\n",
    "    # This will hold subtree totals; initialize with the leaf-only node matrix\n",
    "    T = M.copy()\n",
    "\n",
    "    # 4) Accumulate bottom-up: add each child’s totals into its parent, deepest → root\n",
    "    # Depth = number of dots + 1\n",
    "    depth = sec[\"RANK\"].str.count(r\"\\.\").to_numpy() + 1\n",
    "    idx_by_rank = {r: i for i, r in enumerate(ranks)}\n",
    "\n",
    "    for i in np.argsort(-depth):  # deepest first\n",
    "        r = ranks[i]\n",
    "        p = parent_rank(r)\n",
    "        if p is not None and p in idx_by_rank:\n",
    "            T[idx_by_rank[p], :] += T[i, :]\n",
    "\n",
    "    # 5) Package output: rename node_load_YYYY_MM → sec_load_YYYY_MM\n",
    "    out = sec[[\"FEEDER_ID\",\"RANK\",\"DESTINATION_LOCATION\"]].copy()\n",
    "    for j, ncol in enumerate(node_cols):\n",
    "        sec_col = f\"sec_load_{ncol.split('_', 2)[-1]}\"  # node_load_2024_01 -> sec_load_2024_01\n",
    "        out[sec_col] = T[:, j]\n",
    "\n",
    "    # (Optional) cumulative across months per section:\n",
    "    # cum = np.cumsum(T, axis=1)\n",
    "    # for j, ncol in enumerate(node_cols):\n",
    "    #     out[f\"sec_cum_{ncol.split('_', 2)[-1]}\"] = cum[:, j]\n",
    "\n",
    "    return out\n",
    "\n",
    "# ── Main ────────────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    if not IN_FEDDER.exists():\n",
    "        raise FileNotFoundError(f\"Feeder file not found: {IN_FEDDER}\")\n",
    "    if not IN_MATRIX.exists():\n",
    "        raise FileNotFoundError(f\"Monthly matrix not found: {IN_MATRIX}\")\n",
    "\n",
    "    # 1) Base trace table\n",
    "    df = pd.read_csv(IN_FEDDER, low_memory=False)\n",
    "    if \"FEEDER_ID\" not in df.columns or \"RANK\" not in df.columns:\n",
    "        raise RuntimeError(\"Input must have FEEDER_ID and RANK\")\n",
    "    df[\"FEEDER_ID\"] = norm_id_series(df[\"FEEDER_ID\"])\n",
    "    if \"LOCATION\" not in df.columns:\n",
    "        if \"DESTINATION_LOCATION\" in df.columns:\n",
    "            df[\"LOCATION\"] = df[\"DESTINATION_LOCATION\"]\n",
    "        else:\n",
    "            raise RuntimeError(\"Need LOCATION or DESTINATION_LOCATION\")\n",
    "\n",
    "    # 2) Ensure DT_LOAD_CAL (once)\n",
    "    if \"DT_LOAD_CAL\" not in df.columns:\n",
    "        den = 11 * 0.95 * math.sqrt(3.0)\n",
    "        df[\"DT_LOAD_CAL\"] = pd.to_numeric(df.get(\"DT_LOAD\", np.nan), errors=\"coerce\") / den\n",
    "\n",
    "    # 3) Fractions per feeder (dedup by LOCATION)\n",
    "    uniq = (df[[\"FEEDER_ID\",\"LOCATION\",\"DT_LOAD_CAL\"]]\n",
    "            .dropna(subset=[\"LOCATION\"])\n",
    "            .groupby([\"FEEDER_ID\",\"LOCATION\"], as_index=False)\n",
    "            .agg(DT_LOAD_CAL=(\"DT_LOAD_CAL\",\"sum\")))  # use 'sum' if parallel DTs must add\n",
    "    sum_by_fid = uniq.groupby(\"FEEDER_ID\")[\"DT_LOAD_CAL\"].sum().rename(\"SUM_DT_LOAD_CAL\")\n",
    "    uniq = uniq.merge(sum_by_fid, on=\"FEEDER_ID\", how=\"left\")\n",
    "    uniq[\"FRACTION\"] = uniq[\"DT_LOAD_CAL\"] / uniq[\"SUM_DT_LOAD_CAL\"].replace(0, np.nan)\n",
    "\n",
    "    # Merge only needed fields (avoid *_x/_y)\n",
    "    loc_key = \"DESTINATION_LOCATION\" if \"DESTINATION_LOCATION\" in df.columns else \"LOCATION\"\n",
    "    df = df.merge(\n",
    "        uniq.rename(columns={\"LOCATION\": loc_key})[[\"FEEDER_ID\", loc_key, \"SUM_DT_LOAD_CAL\", \"FRACTION\"]],\n",
    "        on=[\"FEEDER_ID\", loc_key],\n",
    "        how=\"left\",\n",
    "        validate=\"many_to_one\"\n",
    "    )\n",
    "\n",
    "    # 4) Feeder monthly averages from matrix\n",
    "    matrix = pd.read_csv(IN_MATRIX, low_memory=False)\n",
    "    feed_month = make_feed_month_from_matrix(matrix)\n",
    "    feed_month[\"FEEDER_ID\"] = norm_id_series(feed_month[\"FEEDER_ID\"])\n",
    "\n",
    "    feed_wide = (feed_month.pivot_table(index=\"FEEDER_ID\", columns=\"MONTH\",\n",
    "                                        values=\"FEEDER_LOAD_AVG\", aggfunc=\"mean\", fill_value=0.0)\n",
    "                              .reindex(columns=MONTH_TAGS, fill_value=0.0)\n",
    "                              .reset_index())\n",
    "    feed_wide.columns = [\"FEEDER_ID\"] + FEED_COLS\n",
    "    df = df.merge(feed_wide, on=\"FEEDER_ID\", how=\"left\")\n",
    "\n",
    "    # 5) Node monthly loads in one shot (vectorized)\n",
    "    out_nodes = (uniq[[\"FEEDER_ID\",\"LOCATION\",\"FRACTION\"]]\n",
    "                 .merge(feed_month, on=\"FEEDER_ID\", how=\"left\"))\n",
    "    out_nodes[\"CORRECTED_DT_LOAD\"] = out_nodes[\"FRACTION\"].fillna(0.0) * out_nodes[\"FEEDER_LOAD_AVG\"].fillna(0.0)\n",
    "\n",
    "    node_wide = (out_nodes.pivot_table(index=[\"FEEDER_ID\",\"LOCATION\"], columns=\"MONTH\",\n",
    "                                       values=\"CORRECTED_DT_LOAD\", aggfunc=\"sum\", fill_value=0.0)\n",
    "                           .reindex(columns=MONTH_TAGS, fill_value=0.0)\n",
    "                           .reset_index())\n",
    "    node_wide.columns = [\"FEEDER_ID\",\"LOCATION\"] + NODE_COLS\n",
    "    # attach node loads to each trace row (DESTINATION_LOCATION)\n",
    "    df = df.merge(node_wide.rename(columns={\"LOCATION\": loc_key}),\n",
    "                  on=[\"FEEDER_ID\", loc_key], how=\"left\")\n",
    "\n",
    "    # 6) Section loads (bottom-up) for every month simultaneously + cumulative\n",
    "    sec_parts: List[pd.DataFrame] = []\n",
    "    need = [\"FEEDER_ID\",\"RANK\",\"DESTINATION_LOCATION\"] + NODE_COLS\n",
    "    missing = [c for c in need if c not in df.columns]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"Missing columns for section aggregation: {missing}\")\n",
    "\n",
    "    for fid, grp in df[need].groupby(\"FEEDER_ID\", sort=False):\n",
    "        sec_f = compute_section_bottom_up(grp, NODE_COLS)   # returns sec_load_* and sec_cum_*\n",
    "        sec_parts.append(sec_f)\n",
    "\n",
    "    sec_all = pd.concat(sec_parts, ignore_index=True) if sec_parts else pd.DataFrame(\n",
    "        columns=[\"FEEDER_ID\",\"RANK\",\"DESTINATION_LOCATION\"] + SEC_COLS + CUM_COLS\n",
    "    )\n",
    "\n",
    "    df = df.merge(sec_all, on=[\"FEEDER_ID\",\"RANK\",\"DESTINATION_LOCATION\"], how=\"left\")\n",
    "\n",
    "    # 7) Save\n",
    "    df.to_csv(OUT_FILE, index=False)\n",
    "    print(f\"✓ Wrote {OUT_FILE}\")\n",
    "\n",
    "    # peek: shows node, section, cumulative for Jan\n",
    "    peek = [\"FEEDER_ID\",\"RANK\",loc_key,\"node_load_2024_01\",\"sec_load_2024_01\",\"sec_cum_2024_01\"]\n",
    "    print(df[[c for c in peek if c in df.columns]].head(12).to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e785354d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1099\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/fedder_11KV_with_all_monthly_loads.csv\")\n",
    "col = df['FEEDER_ID'].unique()\n",
    "print(len(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e85d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
