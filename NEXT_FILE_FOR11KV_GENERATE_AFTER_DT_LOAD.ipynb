{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c4be53-3a75-49d2-9364-9088953251fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      DT_LOAD  DT_LOAD_CAL\n",
      "0  127.425882     7.040131\n",
      "1  233.628927    12.907725\n",
      "2  134.062123     7.406775\n",
      "3  245.587090    13.568399\n",
      "4  364.485990    20.137424\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Read your CSV\n",
    "df = pd.read_csv(\"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/final_two_column_with_rank_11_withoutDT.csv\")\n",
    "\n",
    "# 2. Compute the constant denominator: 11 * 0.95 * √3\n",
    "den = 11 * 0.95 * np.sqrt(3)\n",
    "\n",
    "# 3. Create the new column\n",
    "df[\"DT_LOAD_CAL\"] = df[\"DT_LOAD\"] / den\n",
    "\n",
    "# 4. (Optional) inspect first few rows\n",
    "print(df[[\"DT_LOAD\", \"DT_LOAD_CAL\"]].head())\n",
    "\n",
    "# 5. Save to a new CSV (or overwrite)\n",
    "df.to_csv(\"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/fedder_11KV_with_DT_LOAD_CAL.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96da7a8b",
   "metadata": {},
   "source": [
    "MAP THE LOAD OF MEANHLY AVERGAE WITH THE FEEDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2300ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load your files\n",
    "sw = pd.read_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/CSV FILE/Monthly_average_from_scada/monthly_SWNO_matrix_numpy.csv\")       # contains SWNO, Month_01…Month_12\n",
    "feeder = pd.read_csv(\"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/fedder_11KV_with_DT_LOAD_CAL.csv\")      # contains feeder_id, DT_LOAD_CAL\n",
    "\n",
    "# 2) Build a lookup from feeder_id → DT_LOAD_CAL\n",
    "dt_map = feeder.set_index(\"FEEDER_ID\")[\"DT_LOAD_CAL\"].to_dict()\n",
    "\n",
    "# 3) Prepare for the walk:\n",
    "#    We'll keep track of the \"current\" DT_LOAD_CAL and the last calculated values for each month.\n",
    "current_dt = None\n",
    "prev_vals = {f\"Month_{i:02d}\": None for i in range(1,13)}\n",
    "\n",
    "# 4) Iterate rows in order, computing chain‐subtract columns:\n",
    "out_rows = []\n",
    "for _, row in sw.iterrows():\n",
    "    swno = row[\"SWNO\"]\n",
    "    # If this SWNO is a feeder anchor, reset current_dt and prev_vals\n",
    "    if swno in dt_map:\n",
    "        current_dt = dt_map[swno]\n",
    "        # initialize prev_vals for this anchor: first subtraction\n",
    "        for i in range(1,13):\n",
    "            m = f\"Month_{i:02d}\"\n",
    "            prev_vals[m] = row[m] - current_dt\n",
    "    else:\n",
    "        # continue chain: subtract the same current_dt from last prev_vals\n",
    "        for i in range(1,13):\n",
    "            m = f\"Month_{i:02d}\"\n",
    "            prev_vals[m] = prev_vals[m] - current_dt\n",
    "\n",
    "    # build output row\n",
    "    out = row.to_dict()\n",
    "    # add the new _CAL columns\n",
    "    for i in range(1,13):\n",
    "        m = f\"Month_{i:02d}\"\n",
    "        out[f\"{m}_CAL\"] = prev_vals[m]\n",
    "    out_rows.append(out)\n",
    "\n",
    "# 5) Assemble and write out\n",
    "out_df = pd.DataFrame(out_rows)\n",
    "out_df.to_csv(\"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/feeder_11Kv_sw_with_chain_calculations.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d0200",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a6d4ff7",
   "metadata": {},
   "source": [
    "COMMULATIVE DIFFERENCE FOR MONTHLY AVERAGE LOAD OF SCADA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f9e1df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/final_feeder_11kv_monthly_cumulative_diff.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "feeder_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/fedder_11KV_with_DT_LOAD_CAL.csv\"\n",
    "monthly_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/CSV FILE/Monthly_average_from_scada/monthly_SWNO_matrix_numpy.csv\"\n",
    "output_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/final_feeder_11kv_monthly_cumulative_diff.csv\"\n",
    "\n",
    "\n",
    "# Load data\n",
    "feeder_df = pd.read_csv(feeder_path)\n",
    "monthly_df = pd.read_csv(monthly_path)\n",
    "\n",
    "# Merge on FEEDER_ID <-> SWNO\n",
    "merged = feeder_df.merge(monthly_df, left_on='FEEDER_ID', right_on='SWNO', how='left')\n",
    "\n",
    "month_cols = [f'Month_{i:02d}' for i in range(1, 13)]\n",
    "\n",
    "result_rows = []\n",
    "\n",
    "for feeder_id, group in merged.groupby('FEEDER_ID', sort=False):\n",
    "    group = group.copy()\n",
    "    group['__orig_idx'] = group.index\n",
    "    ranks_in_order = group['RANK'].drop_duplicates().tolist()\n",
    "\n",
    "    dt_load_sums = {\n",
    "        rank: group[group['RANK'] == rank]['DT_LOAD_CAL'].fillna(0).sum()\n",
    "        for rank in ranks_in_order\n",
    "    }\n",
    "\n",
    "    prev_cum = {m: None for m in month_cols}\n",
    "\n",
    "    for rank in ranks_in_order:\n",
    "        this_rank_rows = group[group['RANK'] == rank].sort_values('__orig_idx')\n",
    "        first = True\n",
    "        for idx, row in this_rank_rows.iterrows():\n",
    "            base = {col: row[col] for col in feeder_df.columns}\n",
    "            for m in month_cols:\n",
    "                month_val = row.get(m, None)\n",
    "                if first:\n",
    "                    # Only for the first row of this rank\n",
    "                    if rank == ranks_in_order[0]:\n",
    "                        cum_val = month_val - dt_load_sums[rank] if pd.notna(month_val) else None\n",
    "                    else:\n",
    "                        prev = prev_cum[m]\n",
    "                        cum_val = prev - dt_load_sums[rank] if prev is not None else None\n",
    "                    prev_cum[m] = cum_val\n",
    "                else:\n",
    "                    cum_val = None  # For subsequent rows at same rank, leave blank\n",
    "                base[f\"{m}_cumulative_diff\"] = cum_val\n",
    "            result_rows.append(base)\n",
    "            first = False\n",
    "\n",
    "result_df = pd.DataFrame(result_rows)\n",
    "output_cols = list(feeder_df.columns) + [f\"{m}_cumulative_diff\" for m in month_cols]\n",
    "result_df.to_csv(output_path, index=False, columns=output_cols)\n",
    "print(f\"Saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc4db3",
   "metadata": {},
   "source": [
    "AFTER THE RANKING AND USE ONLY MAIN CHAIN DONT USING SUBCHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25cd01dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      DT_LOAD  DT_LOAD_CAL\n",
      "0  616.320000    34.050959\n",
      "1  674.080488    37.242158\n",
      "2  473.442574    26.157148\n",
      "3  148.855367     8.224085\n",
      "4  400.698507    22.138124\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Read your CSV\n",
    "df = pd.read_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/11_WITHOUTDT_CONNECTED_RANKED.csv\")\n",
    "\n",
    "# 2. Compute the constant denominator: 11 * 0.95 * √3\n",
    "den = 11 * 0.95 * np.sqrt(3)\n",
    "\n",
    "# 3. Create the new column\n",
    "df[\"DT_LOAD_CAL\"] = df[\"DT_LOAD\"] / den\n",
    "\n",
    "# 4. (Optional) inspect first few rows\n",
    "print(df[[\"DT_LOAD\", \"DT_LOAD_CAL\"]].head())\n",
    "\n",
    "# 5. Save to a new CSV (or overwrite)\n",
    "df.to_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/fedder_11KV_with_DT_LOAD_CAL.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0c7dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load your files\n",
    "sw = pd.read_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/CSV FILE/Monthly_average_from_scada/monthly_SWNO_matrix_numpy.csv\")       # contains SWNO, Month_01…Month_12\n",
    "feeder = pd.read_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/fedder_11KV_with_DT_LOAD_CAL.csv\")      # contains feeder_id, DT_LOAD_CAL\n",
    "\n",
    "# 2) Build a lookup from feeder_id → DT_LOAD_CAL\n",
    "dt_map = feeder.set_index(\"FEEDER_ID\")[\"DT_LOAD_CAL\"].to_dict()\n",
    "\n",
    "# 3) Prepare for the walk:\n",
    "#    We'll keep track of the \"current\" DT_LOAD_CAL and the last calculated values for each month.\n",
    "current_dt = None\n",
    "prev_vals = {f\"Month_{i:02d}\": None for i in range(1,13)}\n",
    "\n",
    "# 4) Iterate rows in order, computing chain‐subtract columns:\n",
    "out_rows = []\n",
    "for _, row in sw.iterrows():\n",
    "    swno = row[\"SWNO\"]\n",
    "    # If this SWNO is a feeder anchor, reset current_dt and prev_vals\n",
    "    if swno in dt_map:\n",
    "        current_dt = dt_map[swno]\n",
    "        # initialize prev_vals for this anchor: first subtraction\n",
    "        for i in range(1,13):\n",
    "            m = f\"Month_{i:02d}\"\n",
    "            prev_vals[m] = row[m] - current_dt\n",
    "    else:\n",
    "        # continue chain: subtract the same current_dt from last prev_vals\n",
    "        for i in range(1,13):\n",
    "            m = f\"Month_{i:02d}\"\n",
    "            prev_vals[m] = prev_vals[m] - current_dt\n",
    "\n",
    "    # build output row\n",
    "    out = row.to_dict()\n",
    "    # add the new _CAL columns\n",
    "    for i in range(1,13):\n",
    "        m = f\"Month_{i:02d}\"\n",
    "        out[f\"{m}_CAL\"] = prev_vals[m]\n",
    "    out_rows.append(out)\n",
    "\n",
    "# 5) Assemble and write out\n",
    "out_df = pd.DataFrame(out_rows)\n",
    "out_df.to_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/feeder_11Kv_sw_with_chain_calculations.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f50870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths (change as needed)\n",
    "feeder_path = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/fedder_11KV_with_DT_LOAD_CAL.csv\"\n",
    "daily_path_1 = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/daily_SWNO_matrix_11KV_YEAR2025.csv\"\n",
    "daily_path_2 = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/daily_SWNO_matrix_11KV_YEAR2024.csv\"\n",
    "output_path_1 = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/final_feeder_11kv_daily_cumulative_diff_2025.csv\"\n",
    "output_path_2 = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/final_feeder_11kv_daily_cumulative_diff_2024.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "542534c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feeder_df = pd.read_csv(feeder_path)\n",
    "\n",
    "def is_main_chain_only(ranks):\n",
    "    return all('.' not in str(r) for r in ranks)\n",
    "\n",
    "main_chain_flags = (\n",
    "    feeder_df.groupby('FEEDER_ID')['RANK']\n",
    "    .apply(is_main_chain_only)\n",
    "    .rename('IS_MAIN_CHAIN_ONLY')\n",
    "    .reset_index()\n",
    ")\n",
    "main_chain_ids = main_chain_flags[main_chain_flags['IS_MAIN_CHAIN_ONLY']]['FEEDER_ID'].tolist()\n",
    "main_chain_feeder_df = feeder_df[feeder_df['FEEDER_ID'].isin(main_chain_ids)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7788661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/final_feeder_11kv_daily_cumulative_diff_2025.csv\n",
      "Saved to /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/final_feeder_11kv_daily_cumulative_diff_2024.csv\n"
     ]
    }
   ],
   "source": [
    "def process_year(main_chain_feeder_df, daily_path, output_path):\n",
    "    daily_df = pd.read_csv(daily_path)\n",
    "    day_cols = [col for col in daily_df.columns if col.startswith(\"Day_\")]\n",
    "\n",
    "    merged = main_chain_feeder_df.merge(daily_df, left_on='FEEDER_ID', right_on='SWNO', how='left')\n",
    "\n",
    "    result_rows = []\n",
    "\n",
    "    for feeder_id, group in merged.groupby('FEEDER_ID', sort=False):\n",
    "        group = group.copy()\n",
    "        group['__orig_idx'] = group.index\n",
    "        ranks_in_order = group['RANK'].drop_duplicates().tolist()\n",
    "\n",
    "        dt_load_sums = {\n",
    "            rank: group[group['RANK'] == rank]['DT_LOAD_CAL'].fillna(0).sum()\n",
    "            for rank in ranks_in_order\n",
    "        }\n",
    "\n",
    "        prev_cum = {d: None for d in day_cols}\n",
    "\n",
    "        for rank in ranks_in_order:\n",
    "            this_rank_rows = group[group['RANK'] == rank].sort_values('__orig_idx')\n",
    "            first = True\n",
    "            for idx, row in this_rank_rows.iterrows():\n",
    "                base = {col: row[col] for col in feeder_df.columns}\n",
    "                for d in day_cols:\n",
    "                    day_val = row.get(d, None)\n",
    "                    if first:\n",
    "                        if rank == ranks_in_order[0]:\n",
    "                            cum_val = day_val - dt_load_sums[rank] if pd.notna(day_val) else None\n",
    "                        else:\n",
    "                            prev = prev_cum[d]\n",
    "                            cum_val = prev - dt_load_sums[rank] if prev is not None else None\n",
    "                        prev_cum[d] = cum_val\n",
    "                    else:\n",
    "                        cum_val = None\n",
    "                    base[f\"{d}_cumulative_diff\"] = cum_val\n",
    "                result_rows.append(base)\n",
    "                first = False\n",
    "\n",
    "    result_df = pd.DataFrame(result_rows)\n",
    "    output_cols = list(feeder_df.columns) + [f\"{d}_cumulative_diff\" for d in day_cols]\n",
    "    result_df.to_csv(output_path, index=False, columns=output_cols)\n",
    "    print(f\"Saved to {output_path}\")\n",
    "\n",
    "# Process each year\n",
    "process_year(main_chain_feeder_df, daily_path_1, output_path_1)\n",
    "process_year(main_chain_feeder_df, daily_path_2, output_path_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17c84863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/final_feeder_11kv_daily_cumulative_diff_2025.csv\n",
      "Saved to /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/final_feeder_11kv_daily_cumulative_diff_2024.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Identify only main-chain feeders (no sub-branches) ---\n",
    "feeder_df = pd.read_csv(feeder_path)\n",
    "\n",
    "def is_main_chain_only(ranks):\n",
    "    return all('.' not in str(r) for r in ranks)\n",
    "\n",
    "main_chain_flags = (\n",
    "    feeder_df.groupby('FEEDER_ID')['RANK']\n",
    "    .apply(is_main_chain_only)\n",
    "    .rename('IS_MAIN_CHAIN_ONLY')\n",
    "    .reset_index()\n",
    ")\n",
    "main_chain_ids = main_chain_flags[main_chain_flags['IS_MAIN_CHAIN_ONLY']]['FEEDER_ID'].tolist()\n",
    "main_chain_feeder_df = feeder_df[feeder_df['FEEDER_ID'].isin(main_chain_ids)].copy()\n",
    "\n",
    "# --- Processing function ---\n",
    "def process_year(main_chain_feeder_df, daily_path, output_path):\n",
    "    daily_df = pd.read_csv(daily_path)\n",
    "    day_cols = [col for col in daily_df.columns if col.startswith(\"Day_\")]\n",
    "\n",
    "    merged = main_chain_feeder_df.merge(daily_df, left_on='FEEDER_ID', right_on='SWNO', how='left')\n",
    "\n",
    "    result_rows = []\n",
    "\n",
    "    for feeder_id, group in merged.groupby('FEEDER_ID', sort=False):\n",
    "        group = group.copy()\n",
    "        group['__orig_idx'] = group.index\n",
    "        ranks_in_order = group['RANK'].drop_duplicates().tolist()\n",
    "\n",
    "        dt_load_sums = {\n",
    "            rank: group[group['RANK'] == rank]['DT_LOAD_CAL'].fillna(0).sum()\n",
    "            for rank in ranks_in_order\n",
    "        }\n",
    "\n",
    "        prev_cum = {d: None for d in day_cols}\n",
    "\n",
    "        for rank in ranks_in_order:\n",
    "            this_rank_rows = group[group['RANK'] == rank].sort_values('__orig_idx')\n",
    "            first = True\n",
    "            for idx, row in this_rank_rows.iterrows():\n",
    "                base = {col: row[col] for col in main_chain_feeder_df.columns}\n",
    "                for d in day_cols:\n",
    "                    day_val = row.get(d, None)\n",
    "                    if first:\n",
    "                        if rank == ranks_in_order[0]:\n",
    "                            cum_val = day_val - dt_load_sums[rank] if pd.notna(day_val) else None\n",
    "                        else:\n",
    "                            prev = prev_cum[d]\n",
    "                            cum_val = prev - dt_load_sums[rank] if prev is not None else None\n",
    "                        prev_cum[d] = cum_val\n",
    "                    else:\n",
    "                        cum_val = None\n",
    "                    base[f\"{d}_cumulative_diff\"] = cum_val\n",
    "                result_rows.append(base)\n",
    "                first = False\n",
    "\n",
    "    result_df = pd.DataFrame(result_rows)\n",
    "    # --- Add FROM_SWITCH and TO_SWITCH columns ---\n",
    "    if \"FROM_TO\" in result_df.columns:\n",
    "        from_to_split = result_df[\"FROM_TO\"].str.split(\"-\", n=1, expand=True)\n",
    "        result_df[\"FROM_SWITCH\"] = from_to_split[0]\n",
    "        result_df[\"TO_SWITCH\"] = from_to_split[1]\n",
    "        cols = list(result_df.columns)\n",
    "        i = cols.index(\"FROM_TO\") + 1\n",
    "        for col in [\"FROM_SWITCH\", \"TO_SWITCH\"]:\n",
    "            cols.insert(i, cols.pop(cols.index(col)))\n",
    "            i += 1\n",
    "        result_df = result_df[cols]\n",
    "    # --- Save ---\n",
    "    output_cols = list(result_df.columns)  # Save all columns, now includes FROM_SWITCH and TO_SWITCH\n",
    "    result_df.to_csv(output_path, index=False, columns=output_cols)\n",
    "    print(f\"Saved to {output_path}\")\n",
    "\n",
    "# --- Process each year ---\n",
    "process_year(main_chain_feeder_df, daily_path_1, output_path_1)\n",
    "process_year(main_chain_feeder_df, daily_path_2, output_path_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b89993",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52fbb275",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c3bf827",
   "metadata": {},
   "source": [
    "NEW LOGIC TO CALCULATE THE LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbbda874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote -> /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/fedder_11KV_with_all_monthly_loads.csv\n"
     ]
    }
   ],
   "source": [
    "# ── Imports ────────────────────────────────────────────────────────────────\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ── Paths ───────────────────────────────────────────────────────────────────\n",
    "BASE      = \"/media/sagark24/New Volume/MERGE CDIS\"\n",
    "IN_FEDDER = Path(f\"{BASE}/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/11_WITHOUTDT_CONNECTED_RANKED.csv\")\n",
    "IN_MATRIX = Path(f\"{BASE}/IPYNB_FILE/DATA_GENERATION/monthly_SWNO_matrix_11KV.csv\")\n",
    "OUT_FILE  = Path(f\"{BASE}/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/fedder_11KV_with_all_monthly_loads.csv\")\n",
    "\n",
    "# ── Dynamically read month columns from the matrix header ───────────────────\n",
    "#     (any column exactly matching \"Month_XX\")\n",
    "hdr = pd.read_csv(IN_MATRIX, nrows=0).columns\n",
    "MONTH_KEYS = sorted(\n",
    "    [c for c in hdr if re.fullmatch(r\"Month_\\d{2}\", c)],\n",
    "    key=lambda x: int(x.split(\"_\")[1])\n",
    ")\n",
    "if not MONTH_KEYS:\n",
    "    raise RuntimeError(\"No Month_XX columns found in the matrix!\")\n",
    "\n",
    "# Build downstream column names that keep the original tag\n",
    "FEED_COLS = [f\"feeder_{m}\"    for m in MONTH_KEYS]\n",
    "NODE_COLS = [f\"node_load_{m}\" for m in MONTH_KEYS]\n",
    "CUM_COLS  = [f\"sec_cum_{m}\"   for m in MONTH_KEYS]\n",
    "\n",
    "# ── Helpers ─────────────────────────────────────────────────────────────────\n",
    "def make_feed_long(mat: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return FEEDER_ID | MONTH | FEEDER_LOAD_AVG (long form),\n",
    "    where MONTH is the *original* column label (e.g. 'Month_01').\n",
    "    Fills missing (feeder,month) pairs with 0.\n",
    "    \"\"\"\n",
    "    need = {\"SWNO\", *MONTH_KEYS}\n",
    "    miss = [c for c in need if c not in mat.columns]\n",
    "    if miss:\n",
    "        raise RuntimeError(f\"monthly matrix missing columns: {miss}\")\n",
    "\n",
    "    m = mat.copy()\n",
    "    m[\"FEEDER_ID\"] = m[\"SWNO\"].astype(str).str.strip()\n",
    "    for c in MONTH_KEYS:\n",
    "        m[c] = pd.to_numeric(m[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    long = m.melt(id_vars=[\"FEEDER_ID\"], value_vars=MONTH_KEYS,\n",
    "                  var_name=\"MONTH\", value_name=\"FEEDER_LOAD_AVG\")\n",
    "\n",
    "    complete = (pd.MultiIndex.from_product(\n",
    "                    [m[\"FEEDER_ID\"].unique(), MONTH_KEYS],\n",
    "                    names=[\"FEEDER_ID\", \"MONTH\"]\n",
    "                ).to_frame(index=False))\n",
    "    return (complete.merge(long, on=[\"FEEDER_ID\", \"MONTH\"], how=\"left\")\n",
    "                    .fillna({\"FEEDER_LOAD_AVG\": 0.0}))\n",
    "\n",
    "def build_graph(df_f: pd.DataFrame) -> dict[str, list[str]]:\n",
    "    \"\"\"Adjacency: SOURCE_LOCATION -> [DESTINATION_LOCATION…].\"\"\"\n",
    "    adj: dict[str, list[str]] = {}\n",
    "    for s, d in zip(df_f[\"SOURCE_LOCATION\"].astype(str),\n",
    "                    df_f[\"DESTINATION_LOCATION\"].astype(str)):\n",
    "        adj.setdefault(s, [])\n",
    "        if d not in adj[s]:\n",
    "            adj[s].append(d)\n",
    "        adj.setdefault(d, [])          # ensure leaf key\n",
    "    return adj\n",
    "\n",
    "def dfs_subtree_cum(adj, node_vec):\n",
    "    \"\"\"Bottom-up DFS returning {node: np[int64, 12]}.\"\"\"\n",
    "    agg, state = {}, {}\n",
    "    all_nodes = set(adj.keys()) | {c for kids in adj.values() for c in kids}\n",
    "\n",
    "    for root in list(all_nodes):\n",
    "        if state.get(root, 0) != 0:\n",
    "            continue\n",
    "        stack = [(root, 0)]\n",
    "        while stack:\n",
    "            node, phase = stack.pop()\n",
    "            if phase == 0:\n",
    "                if state.get(node, 0) in (1, 2):\n",
    "                    continue\n",
    "                state[node] = 1\n",
    "                stack.append((node, 1))\n",
    "                stack.extend((ch, 0) for ch in adj.get(node, []))\n",
    "            else:\n",
    "                subtotal = node_vec.get(node, np.zeros(len(MONTH_KEYS), dtype=np.int64)).copy()\n",
    "                for ch in adj.get(node, []):\n",
    "                    subtotal += agg.get(ch, np.zeros(len(MONTH_KEYS), dtype=np.int64))\n",
    "                agg[node] = subtotal\n",
    "                state[node] = 2\n",
    "    return agg\n",
    "\n",
    "# ── Main ────────────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    # 1) Feeder trace --------------------------------------------------------\n",
    "    df = pd.read_csv(IN_FEDDER, low_memory=False)\n",
    "    df[\"FEEDER_ID\"] = df[\"FEEDER_ID\"].astype(str).str.strip()\n",
    "\n",
    "\n",
    "    if \"DT_LOAD_CAL\" not in df.columns:\n",
    "        den = 11 * 0.95 * np.sqrt(3.0)\n",
    "        df[\"DT_LOAD_CAL\"] = pd.to_numeric(df.get(\"DT_LOAD\", np.nan), errors=\"coerce\") / den\n",
    "\n",
    "    # 2) FRACTION per feeder -------------------------------------------------\n",
    "    uniq = (df[[\"FEEDER_ID\", \"LOCATION\", \"DT_LOAD_CAL\"]]\n",
    "            .dropna(subset=[\"DT_LOAD_CAL\"])\n",
    "            .groupby([\"FEEDER_ID\", \"LOCATION\"], as_index=False)\n",
    "            .agg(DT_LOAD_CAL=(\"DT_LOAD_CAL\", \"sum\")))\n",
    "    sum_by_fid = uniq.groupby(\"FEEDER_ID\")[\"DT_LOAD_CAL\"].sum().rename(\"SUM_DT_LOAD_CAL\")\n",
    "    uniq = uniq.merge(sum_by_fid, on=\"FEEDER_ID\")\n",
    "    uniq[\"FRACTION\"] = uniq[\"DT_LOAD_CAL\"] / uniq[\"SUM_DT_LOAD_CAL\"].replace(0, np.nan)\n",
    "\n",
    "    df = df.merge(\n",
    "        uniq.rename(columns={\"LOCATION\": \"DESTINATION_LOCATION\"})\n",
    "            [[\"FEEDER_ID\", \"DESTINATION_LOCATION\", \"SUM_DT_LOAD_CAL\", \"FRACTION\"]],\n",
    "        on=[\"FEEDER_ID\", \"DESTINATION_LOCATION\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # 3) Feeder monthly averages -------------------------------------------\n",
    "    matrix = pd.read_csv(IN_MATRIX, low_memory=False)\n",
    "    feed_long = make_feed_long(matrix)             # long form\n",
    "    feed_wide = (feed_long.pivot(index=\"FEEDER_ID\", columns=\"MONTH\",\n",
    "                                 values=\"FEEDER_LOAD_AVG\")\n",
    "                           .reindex(columns=MONTH_KEYS, fill_value=0.0)\n",
    "                           .reset_index())\n",
    "    feed_wide.columns = [\"FEEDER_ID\"] + FEED_COLS\n",
    "    df = df.merge(feed_wide, on=\"FEEDER_ID\", how=\"left\")\n",
    "\n",
    "    # 4) Per-node integer monthly loads ------------------------------------\n",
    "    out_nodes = (uniq[[\"FEEDER_ID\", \"LOCATION\", \"FRACTION\"]]\n",
    "                 .merge(feed_long, on=\"FEEDER_ID\"))\n",
    "    out_nodes[\"NODE_LOAD\"] = out_nodes[\"FRACTION\"].fillna(0) * out_nodes[\"FEEDER_LOAD_AVG\"]\n",
    "\n",
    "    node_wide = (out_nodes.pivot(index=[\"FEEDER_ID\", \"LOCATION\"], columns=\"MONTH\",\n",
    "                                 values=\"NODE_LOAD\")\n",
    "                           .reindex(columns=MONTH_KEYS, fill_value=0)\n",
    "                           .reset_index())\n",
    "    node_wide[MONTH_KEYS] = np.ceil(node_wide[MONTH_KEYS].astype(float)).astype(\"Int64\")\n",
    "\n",
    "    node_wide_out = node_wide.rename(columns={\"LOCATION\": \"DESTINATION_LOCATION\"})\n",
    "    node_wide_out.columns = [\"FEEDER_ID\", \"DESTINATION_LOCATION\"] + NODE_COLS\n",
    "    df = df.merge(node_wide_out, on=[\"FEEDER_ID\", \"DESTINATION_LOCATION\"], how=\"left\")\n",
    "\n",
    "    # 5) Bottom-up cumulative section loads --------------------------------\n",
    "    sec_parts = []\n",
    "    for fid, gdf in df.groupby(\"FEEDER_ID\", sort=False):\n",
    "        adj = build_graph(gdf[[\"SOURCE_LOCATION\", \"DESTINATION_LOCATION\"]])\n",
    "        sub_nodes = node_wide[node_wide[\"FEEDER_ID\"] == fid]\n",
    "        node_vec = {str(loc): sub_nodes.loc[sub_nodes[\"LOCATION\"] == loc, MONTH_KEYS]\n",
    "                                     .to_numpy(dtype=\"int64\").reshape(-1, len(MONTH_KEYS))[0]\n",
    "                    for loc in sub_nodes[\"LOCATION\"].astype(str).unique()}\n",
    "        for n in set(adj.keys()) | {c for kids in adj.values() for c in kids}:\n",
    "            node_vec.setdefault(n, np.zeros(len(MONTH_KEYS), dtype=np.int64))\n",
    "\n",
    "        agg = dfs_subtree_cum(adj, node_vec)\n",
    "        rows = []\n",
    "        for loc, vec in agg.items():\n",
    "            cum = np.cumsum(vec, dtype=np.int64)\n",
    "            rows.append({\"FEEDER_ID\": fid, \"DESTINATION_LOCATION\": loc,\n",
    "                         **{f\"sec_cum_{m}\": int(cum[j]) for j, m in enumerate(MONTH_KEYS)}})\n",
    "        sec_parts.append(pd.DataFrame(rows))\n",
    "\n",
    "    sec_all = pd.concat(sec_parts, ignore_index=True)\n",
    "    df = df.merge(sec_all, on=[\"FEEDER_ID\", \"DESTINATION_LOCATION\"], how=\"left\")\n",
    "\n",
    "    # 6) Write --------------------------------------------------------------\n",
    "    df.to_csv(OUT_FILE, index=False)\n",
    "    print(f\"Wrote -> {OUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e785354d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1099\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/fedder_11KV_with_all_monthly_loads.csv\")\n",
    "col = df['FEEDER_ID'].unique()\n",
    "print(len(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708d3139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3aacdd1",
   "metadata": {},
   "source": [
    "RECURSIVE AND ITTERATIVE BOTH \n",
    "FUNCTION DFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6502a41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Wrote using AGG_METHOD='re' → /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/fedder_11KV_with_all_monthly_loads.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "11 kV feeder ETL: node-level monthly loads + cumulative section flows.\n",
    "\n",
    "Choose aggregation style with AGG_METHOD:\n",
    "    \"iter\"  – explicit-stack DFS (depth-safe, quiet on cycles)\n",
    "    \"rec\"   – recursive DFS   (simpler; self-loops skipped, other cycles → ValueError)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import re, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ── USER CHOICE ────────────────────────────────────────────────────────────\n",
    "AGG_METHOD  = \"re\"      # \"iter\"  or  \"rec\"\n",
    "DEPTH_LIMIT = 20_000      # only used if AGG_METHOD == \"rec\"\n",
    "\n",
    "# ── Paths ───────────────────────────────────────────────────────────────────\n",
    "BASE      = \"/media/sagark24/New Volume/MERGE CDIS\"\n",
    "IN_FEDDER = Path(f\"{BASE}/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/11_WITHOUTDT_CONNECTED_RANKED.csv\")\n",
    "IN_MATRIX = Path(f\"{BASE}/IPYNB_FILE/DATA_GENERATION/monthly_SWNO_matrix_11KV.csv\")\n",
    "OUT_FILE  = Path(f\"{BASE}/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/fedder_11KV_with_all_monthly_loads.csv\")\n",
    "\n",
    "# ── Discover columns Month_01 … Month_12 ───────────────────────────────────\n",
    "hdr = pd.read_csv(IN_MATRIX, nrows=0).columns\n",
    "MONTH_KEYS = sorted([c for c in hdr if re.fullmatch(r\"Month_\\d{2}\", c)],\n",
    "                    key=lambda x: int(x.split(\"_\")[1]))\n",
    "if not MONTH_KEYS:\n",
    "    raise RuntimeError(\"No Month_XX columns in matrix!\")\n",
    "\n",
    "FEED_COLS = [f\"feeder_{m}\"    for m in MONTH_KEYS]\n",
    "NODE_COLS = [f\"node_load_{m}\" for m in MONTH_KEYS]\n",
    "CUM_COLS  = [f\"sec_cum_{m}\"   for m in MONTH_KEYS]\n",
    "\n",
    "# ── Helpers ────────────────────────────────────────────────────────────────\n",
    "def make_feed_long(mat: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"SWNO matrix → long FEEDER_ID | MONTH | FEEDER_LOAD_AVG (fills 0).\"\"\"\n",
    "    need = {\"SWNO\", *MONTH_KEYS}\n",
    "    miss = [c for c in need if c not in mat.columns]\n",
    "    if miss:\n",
    "        raise RuntimeError(f\"matrix missing {miss}\")\n",
    "\n",
    "    m = mat.copy()\n",
    "    m[\"FEEDER_ID\"] = m[\"SWNO\"].astype(str).str.strip()\n",
    "    m[MONTH_KEYS]  = m[MONTH_KEYS].apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    long = m.melt(id_vars=[\"FEEDER_ID\"], value_vars=MONTH_KEYS,\n",
    "                  var_name=\"MONTH\", value_name=\"FEEDER_LOAD_AVG\")\n",
    "\n",
    "    complete = pd.MultiIndex.from_product(\n",
    "        [m[\"FEEDER_ID\"].unique(), MONTH_KEYS],\n",
    "        names=[\"FEEDER_ID\",\"MONTH\"]\n",
    "    ).to_frame(index=False)\n",
    "\n",
    "    return (complete.merge(long, on=[\"FEEDER_ID\",\"MONTH\"], how=\"left\")\n",
    "                  .fillna({\"FEEDER_LOAD_AVG\": 0.0}))\n",
    "\n",
    "def build_adj(df_f: pd.DataFrame) -> dict[str, list[str]]:\n",
    "    \"\"\"parent -> [children] (dedup, leaf keys included).\"\"\"\n",
    "    adj: dict[str,list[str]] = {}\n",
    "    for s,d in zip(df_f[\"SOURCE_LOCATION\"].astype(str),\n",
    "                   df_f[\"DESTINATION_LOCATION\"].astype(str)):\n",
    "        adj.setdefault(s, []).append(d)\n",
    "        adj.setdefault(d, [])\n",
    "    for k in adj:\n",
    "        adj[k] = list(dict.fromkeys(adj[k]))\n",
    "    return adj\n",
    "\n",
    "# ── Aggregation  explicit-stack DFS ───────────────────────────────────────\n",
    "def agg_iter(adj: dict[str,list[str]], node_vec) -> dict[str,np.ndarray]:\n",
    "    agg, state = {}, {}          # 0 unseen • 1 visiting • 2 done\n",
    "    all_nodes  = set(adj.keys()) | {c for kids in adj.values() for c in kids}\n",
    "\n",
    "    for start in all_nodes:\n",
    "        if state.get(start,0):\n",
    "            continue\n",
    "        stack = [(start,0)]\n",
    "        while stack:\n",
    "            n, ph = stack.pop()\n",
    "            if ph==0:\n",
    "                if state.get(n,0):                       # visiting/done\n",
    "                    continue\n",
    "                state[n]=1\n",
    "                stack.append((n,1))\n",
    "                stack.extend((c,0) for c in adj[n] if c!=n)  # skip self-loop\n",
    "            else:\n",
    "                v = node_vec.get(n, np.zeros_like(next(iter(node_vec.values())))).copy()\n",
    "                for c in adj[n]:\n",
    "                    v += agg.get(c, np.zeros_like(v))\n",
    "                agg[n]=v; state[n]=2\n",
    "    return agg\n",
    "\n",
    "# ── Aggregation  recursive DFS (self-loop safe) ───────────────────────────\n",
    "def agg_rec(adj: dict[str,list[str]], node_vec,\n",
    "            depth_limit: int = 20_000) -> dict[str,np.ndarray]:\n",
    "    sys.setrecursionlimit(depth_limit)\n",
    "\n",
    "    parents  = set(adj.keys())\n",
    "    children = {c for kids in adj.values() for c in kids}\n",
    "    roots    = parents - children or parents      # fallback\n",
    "\n",
    "    done, visiting = {}, set()\n",
    "    zeros_proto = np.zeros_like(next(iter(node_vec.values())))\n",
    "\n",
    "    def dfs(v:str)->np.ndarray:\n",
    "        if v in done:        return done[v]\n",
    "        if v in visiting:    raise ValueError(f\"cycle at {v}\")\n",
    "        visiting.add(v)\n",
    "        s = node_vec.get(v, zeros_proto).copy()\n",
    "        for c in adj[v]:\n",
    "            if c!=v:\n",
    "                s += dfs(c)\n",
    "        visiting.remove(v)\n",
    "        done[v] = s\n",
    "        return s\n",
    "\n",
    "    for r in roots: dfs(r)\n",
    "    return done\n",
    "\n",
    "# ── Pipeline ───────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    # 1) feeder trace -------------------------------------------------------\n",
    "    df = pd.read_csv(IN_FEDDER, low_memory=False)\n",
    "    df[\"FEEDER_ID\"] = df[\"FEEDER_ID\"].astype(str).str.strip()\n",
    "\n",
    "    if \"DT_LOAD_CAL\" not in df.columns:\n",
    "        den = 11 * 0.95 * np.sqrt(3.0)\n",
    "        df[\"DT_LOAD_CAL\"] = pd.to_numeric(df.get(\"DT_LOAD\"), errors=\"coerce\") / den\n",
    "\n",
    "    # 2) FRACTION per feeder -------------------------------------------------\n",
    "    uniq = (df[[\"FEEDER_ID\",\"LOCATION\",\"DT_LOAD_CAL\"]]\n",
    "              .dropna(subset=[\"DT_LOAD_CAL\"])\n",
    "              .groupby([\"FEEDER_ID\",\"LOCATION\"], as_index=False)\n",
    "              .agg(DT_LOAD_CAL=(\"DT_LOAD_CAL\",\"sum\")))\n",
    "    total = uniq.groupby(\"FEEDER_ID\")[\"DT_LOAD_CAL\"].sum().rename(\"SUM_DT_LOAD_CAL\")\n",
    "    uniq = uniq.merge(total, on=\"FEEDER_ID\")\n",
    "    uniq[\"FRACTION\"] = uniq[\"DT_LOAD_CAL\"] / uniq[\"SUM_DT_LOAD_CAL\"].replace(0,np.nan)\n",
    "\n",
    "    df = df.merge(\n",
    "        uniq.rename(columns={\"LOCATION\":\"DESTINATION_LOCATION\"})\n",
    "            [[\"FEEDER_ID\",\"DESTINATION_LOCATION\",\"SUM_DT_LOAD_CAL\",\"FRACTION\"]],\n",
    "        on=[\"FEEDER_ID\",\"DESTINATION_LOCATION\"], how=\"left\"\n",
    "    )\n",
    "\n",
    "    # 3) feeder monthly averages -------------------------------------------\n",
    "    feed_long = make_feed_long(pd.read_csv(IN_MATRIX, low_memory=False))\n",
    "    feed_wide = (feed_long.pivot(index=\"FEEDER_ID\", columns=\"MONTH\",\n",
    "                                 values=\"FEEDER_LOAD_AVG\")\n",
    "                         .reindex(columns=MONTH_KEYS, fill_value=0.0)\n",
    "                         .reset_index())\n",
    "    feed_wide.columns = [\"FEEDER_ID\"] + FEED_COLS\n",
    "    df = df.merge(feed_wide, on=\"FEEDER_ID\", how=\"left\")\n",
    "\n",
    "    # 4) per-node int monthly loads ----------------------------------------\n",
    "    out_nodes = (uniq[[\"FEEDER_ID\",\"LOCATION\",\"FRACTION\"]]\n",
    "                 .merge(feed_long, on=\"FEEDER_ID\"))\n",
    "    out_nodes[\"NODE_LOAD\"] = out_nodes[\"FRACTION\"].fillna(0)*out_nodes[\"FEEDER_LOAD_AVG\"]\n",
    "\n",
    "    node_wide = (out_nodes.pivot(index=[\"FEEDER_ID\",\"LOCATION\"], columns=\"MONTH\",\n",
    "                                 values=\"NODE_LOAD\")\n",
    "                           .reindex(columns=MONTH_KEYS, fill_value=0)\n",
    "                           .reset_index())\n",
    "    node_wide[MONTH_KEYS] = np.ceil(node_wide[MONTH_KEYS].astype(float)).astype(\"Int64\")\n",
    "\n",
    "    node_wide_out = node_wide.rename(columns={\"LOCATION\":\"DESTINATION_LOCATION\"})\n",
    "    node_wide_out.columns = [\"FEEDER_ID\",\"DESTINATION_LOCATION\"] + NODE_COLS\n",
    "    df = df.merge(node_wide_out, on=[\"FEEDER_ID\",\"DESTINATION_LOCATION\"], how=\"left\")\n",
    "\n",
    "    # 5) bottom-up cumulative ----------------------------------------------\n",
    "    sec_parts=[]\n",
    "    for fid,gdf in df.groupby(\"FEEDER_ID\", sort=False):\n",
    "        adj = build_adj(gdf[[\"SOURCE_LOCATION\",\"DESTINATION_LOCATION\"]])\n",
    "        sub = node_wide[node_wide[\"FEEDER_ID\"]==fid]\n",
    "        node_vec = {str(l): sub.loc[sub[\"LOCATION\"]==l, MONTH_KEYS]\n",
    "                                   .to_numpy(dtype=\"int64\").reshape(-1,len(MONTH_KEYS))[0]\n",
    "                    for l in sub[\"LOCATION\"].astype(str).unique()}\n",
    "        for n in set(adj.keys())|{c for kids in adj.values() for c in kids}:\n",
    "            node_vec.setdefault(n, np.zeros(len(MONTH_KEYS),dtype=np.int64))\n",
    "\n",
    "        # ── pick aggregation strategy correctly ───────────────────────────\n",
    "        if AGG_METHOD == \"iter\":\n",
    "            agg = agg_iter(adj, node_vec)                         # ← two args\n",
    "        else:\n",
    "            agg = agg_rec(adj, node_vec, DEPTH_LIMIT)             # ← three args\n",
    "\n",
    "\n",
    "        rows=[]\n",
    "        for loc,vec in agg.items():\n",
    "            cum=np.cumsum(vec, dtype=np.int64)\n",
    "            rows.append({\"FEEDER_ID\":fid,\"DESTINATION_LOCATION\":loc,\n",
    "                         **{f\"sec_cum_{m}\":int(cum[i]) for i,m in enumerate(MONTH_KEYS)}})\n",
    "        sec_parts.append(pd.DataFrame(rows))\n",
    "\n",
    "    df = df.merge(pd.concat(sec_parts, ignore_index=True),\n",
    "                  on=[\"FEEDER_ID\",\"DESTINATION_LOCATION\"], how=\"left\")\n",
    "\n",
    "    # 6) write --------------------------------------------------------------\n",
    "    df.to_csv(OUT_FILE, index=False)\n",
    "    print(f\" Wrote using AGG_METHOD='{AGG_METHOD}' -> {OUT_FILE}\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fd99eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
