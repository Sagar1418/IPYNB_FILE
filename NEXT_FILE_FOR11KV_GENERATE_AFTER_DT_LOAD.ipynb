{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c4be53-3a75-49d2-9364-9088953251fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      DT_LOAD  DT_LOAD_CAL\n",
      "0  127.425882     7.040131\n",
      "1  233.628927    12.907725\n",
      "2  134.062123     7.406775\n",
      "3  245.587090    13.568399\n",
      "4  364.485990    20.137424\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Read your CSV\n",
    "df = pd.read_csv(\"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/final_two_column_with_rank_11_withoutDT.csv\")\n",
    "\n",
    "# 2. Compute the constant denominator: 11 * 0.95 * √3\n",
    "den = 11 * 0.95 * np.sqrt(3)\n",
    "\n",
    "# 3. Create the new column\n",
    "df[\"DT_LOAD_CAL\"] = df[\"DT_LOAD\"] / den\n",
    "\n",
    "# 4. (Optional) inspect first few rows\n",
    "print(df[[\"DT_LOAD\", \"DT_LOAD_CAL\"]].head())\n",
    "\n",
    "# 5. Save to a new CSV (or overwrite)\n",
    "df.to_csv(\"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/fedder_11KV_with_DT_LOAD_CAL.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96da7a8b",
   "metadata": {},
   "source": [
    "MAP THE LOAD OF MEANHLY AVERGAE WITH THE FEEDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2300ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load your files\n",
    "sw = pd.read_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/CSV FILE/Monthly_average_from_scada/monthly_SWNO_matrix_numpy.csv\")       # contains SWNO, Month_01…Month_12\n",
    "feeder = pd.read_csv(\"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/fedder_11KV_with_DT_LOAD_CAL.csv\")      # contains feeder_id, DT_LOAD_CAL\n",
    "\n",
    "# 2) Build a lookup from feeder_id → DT_LOAD_CAL\n",
    "dt_map = feeder.set_index(\"FEEDER_ID\")[\"DT_LOAD_CAL\"].to_dict()\n",
    "\n",
    "# 3) Prepare for the walk:\n",
    "#    We'll keep track of the \"current\" DT_LOAD_CAL and the last calculated values for each month.\n",
    "current_dt = None\n",
    "prev_vals = {f\"Month_{i:02d}\": None for i in range(1,13)}\n",
    "\n",
    "# 4) Iterate rows in order, computing chain‐subtract columns:\n",
    "out_rows = []\n",
    "for _, row in sw.iterrows():\n",
    "    swno = row[\"SWNO\"]\n",
    "    # If this SWNO is a feeder anchor, reset current_dt and prev_vals\n",
    "    if swno in dt_map:\n",
    "        current_dt = dt_map[swno]\n",
    "        # initialize prev_vals for this anchor: first subtraction\n",
    "        for i in range(1,13):\n",
    "            m = f\"Month_{i:02d}\"\n",
    "            prev_vals[m] = row[m] - current_dt\n",
    "    else:\n",
    "        # continue chain: subtract the same current_dt from last prev_vals\n",
    "        for i in range(1,13):\n",
    "            m = f\"Month_{i:02d}\"\n",
    "            prev_vals[m] = prev_vals[m] - current_dt\n",
    "\n",
    "    # build output row\n",
    "    out = row.to_dict()\n",
    "    # add the new _CAL columns\n",
    "    for i in range(1,13):\n",
    "        m = f\"Month_{i:02d}\"\n",
    "        out[f\"{m}_CAL\"] = prev_vals[m]\n",
    "    out_rows.append(out)\n",
    "\n",
    "# 5) Assemble and write out\n",
    "out_df = pd.DataFrame(out_rows)\n",
    "out_df.to_csv(\"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/feeder_11Kv_sw_with_chain_calculations.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d0200",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a6d4ff7",
   "metadata": {},
   "source": [
    "COMMULATIVE DIFFERENCE FOR MONTHLY AVERAGE LOAD OF SCADA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f9e1df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/final_feeder_11kv_monthly_cumulative_diff.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "feeder_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/fedder_11KV_with_DT_LOAD_CAL.csv\"\n",
    "monthly_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/CSV FILE/Monthly_average_from_scada/monthly_SWNO_matrix_numpy.csv\"\n",
    "output_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/final_feeder_11kv_monthly_cumulative_diff.csv\"\n",
    "\n",
    "\n",
    "# Load data\n",
    "feeder_df = pd.read_csv(feeder_path)\n",
    "monthly_df = pd.read_csv(monthly_path)\n",
    "\n",
    "# Merge on FEEDER_ID <-> SWNO\n",
    "merged = feeder_df.merge(monthly_df, left_on='FEEDER_ID', right_on='SWNO', how='left')\n",
    "\n",
    "month_cols = [f'Month_{i:02d}' for i in range(1, 13)]\n",
    "\n",
    "result_rows = []\n",
    "\n",
    "for feeder_id, group in merged.groupby('FEEDER_ID', sort=False):\n",
    "    group = group.copy()\n",
    "    group['__orig_idx'] = group.index\n",
    "    ranks_in_order = group['RANK'].drop_duplicates().tolist()\n",
    "\n",
    "    dt_load_sums = {\n",
    "        rank: group[group['RANK'] == rank]['DT_LOAD_CAL'].fillna(0).sum()\n",
    "        for rank in ranks_in_order\n",
    "    }\n",
    "\n",
    "    prev_cum = {m: None for m in month_cols}\n",
    "\n",
    "    for rank in ranks_in_order:\n",
    "        this_rank_rows = group[group['RANK'] == rank].sort_values('__orig_idx')\n",
    "        first = True\n",
    "        for idx, row in this_rank_rows.iterrows():\n",
    "            base = {col: row[col] for col in feeder_df.columns}\n",
    "            for m in month_cols:\n",
    "                month_val = row.get(m, None)\n",
    "                if first:\n",
    "                    # Only for the first row of this rank\n",
    "                    if rank == ranks_in_order[0]:\n",
    "                        cum_val = month_val - dt_load_sums[rank] if pd.notna(month_val) else None\n",
    "                    else:\n",
    "                        prev = prev_cum[m]\n",
    "                        cum_val = prev - dt_load_sums[rank] if prev is not None else None\n",
    "                    prev_cum[m] = cum_val\n",
    "                else:\n",
    "                    cum_val = None  # For subsequent rows at same rank, leave blank\n",
    "                base[f\"{m}_cumulative_diff\"] = cum_val\n",
    "            result_rows.append(base)\n",
    "            first = False\n",
    "\n",
    "result_df = pd.DataFrame(result_rows)\n",
    "output_cols = list(feeder_df.columns) + [f\"{m}_cumulative_diff\" for m in month_cols]\n",
    "result_df.to_csv(output_path, index=False, columns=output_cols)\n",
    "print(f\"Saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc4db3",
   "metadata": {},
   "source": [
    "AFTER THE RANKING AND USE ONLY MAIN CHAIN DONT USING SUBCHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25cd01dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      DT_LOAD  DT_LOAD_CAL\n",
      "0  616.320000    34.050959\n",
      "1  674.080488    37.242158\n",
      "2  473.442574    26.157148\n",
      "3  148.855367     8.224085\n",
      "4  400.698507    22.138124\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Read your CSV\n",
    "df = pd.read_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/11_WITHOUTDT_CONNECTED_RANKED.csv\")\n",
    "\n",
    "# 2. Compute the constant denominator: 11 * 0.95 * √3\n",
    "den = 11 * 0.95 * np.sqrt(3)\n",
    "\n",
    "# 3. Create the new column\n",
    "df[\"DT_LOAD_CAL\"] = df[\"DT_LOAD\"] / den\n",
    "\n",
    "# 4. (Optional) inspect first few rows\n",
    "print(df[[\"DT_LOAD\", \"DT_LOAD_CAL\"]].head())\n",
    "\n",
    "# 5. Save to a new CSV (or overwrite)\n",
    "df.to_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/fedder_11KV_with_DT_LOAD_CAL.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0c7dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load your files\n",
    "sw = pd.read_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/CSV FILE/Monthly_average_from_scada/monthly_SWNO_matrix_numpy.csv\")       # contains SWNO, Month_01…Month_12\n",
    "feeder = pd.read_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/fedder_11KV_with_DT_LOAD_CAL.csv\")      # contains feeder_id, DT_LOAD_CAL\n",
    "\n",
    "# 2) Build a lookup from feeder_id → DT_LOAD_CAL\n",
    "dt_map = feeder.set_index(\"FEEDER_ID\")[\"DT_LOAD_CAL\"].to_dict()\n",
    "\n",
    "# 3) Prepare for the walk:\n",
    "#    We'll keep track of the \"current\" DT_LOAD_CAL and the last calculated values for each month.\n",
    "current_dt = None\n",
    "prev_vals = {f\"Month_{i:02d}\": None for i in range(1,13)}\n",
    "\n",
    "# 4) Iterate rows in order, computing chain‐subtract columns:\n",
    "out_rows = []\n",
    "for _, row in sw.iterrows():\n",
    "    swno = row[\"SWNO\"]\n",
    "    # If this SWNO is a feeder anchor, reset current_dt and prev_vals\n",
    "    if swno in dt_map:\n",
    "        current_dt = dt_map[swno]\n",
    "        # initialize prev_vals for this anchor: first subtraction\n",
    "        for i in range(1,13):\n",
    "            m = f\"Month_{i:02d}\"\n",
    "            prev_vals[m] = row[m] - current_dt\n",
    "    else:\n",
    "        # continue chain: subtract the same current_dt from last prev_vals\n",
    "        for i in range(1,13):\n",
    "            m = f\"Month_{i:02d}\"\n",
    "            prev_vals[m] = prev_vals[m] - current_dt\n",
    "\n",
    "    # build output row\n",
    "    out = row.to_dict()\n",
    "    # add the new _CAL columns\n",
    "    for i in range(1,13):\n",
    "        m = f\"Month_{i:02d}\"\n",
    "        out[f\"{m}_CAL\"] = prev_vals[m]\n",
    "    out_rows.append(out)\n",
    "\n",
    "# 5) Assemble and write out\n",
    "out_df = pd.DataFrame(out_rows)\n",
    "out_df.to_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/feeder_11Kv_sw_with_chain_calculations.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f50870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths (change as needed)\n",
    "feeder_path = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/fedder_11KV_with_DT_LOAD_CAL.csv\"\n",
    "daily_path_1 = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/daily_SWNO_matrix_11KV_YEAR2025.csv\"\n",
    "daily_path_2 = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/daily_SWNO_matrix_11KV_YEAR2024.csv\"\n",
    "output_path_1 = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/final_feeder_11kv_daily_cumulative_diff_2025.csv\"\n",
    "output_path_2 = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/final_feeder_11kv_daily_cumulative_diff_2024.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "542534c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feeder_df = pd.read_csv(feeder_path)\n",
    "\n",
    "def is_main_chain_only(ranks):\n",
    "    return all('.' not in str(r) for r in ranks)\n",
    "\n",
    "main_chain_flags = (\n",
    "    feeder_df.groupby('FEEDER_ID')['RANK']\n",
    "    .apply(is_main_chain_only)\n",
    "    .rename('IS_MAIN_CHAIN_ONLY')\n",
    "    .reset_index()\n",
    ")\n",
    "main_chain_ids = main_chain_flags[main_chain_flags['IS_MAIN_CHAIN_ONLY']]['FEEDER_ID'].tolist()\n",
    "main_chain_feeder_df = feeder_df[feeder_df['FEEDER_ID'].isin(main_chain_ids)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7788661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/final_feeder_11kv_daily_cumulative_diff_2025.csv\n",
      "Saved to /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/final_feeder_11kv_daily_cumulative_diff_2024.csv\n"
     ]
    }
   ],
   "source": [
    "def process_year(main_chain_feeder_df, daily_path, output_path):\n",
    "    daily_df = pd.read_csv(daily_path)\n",
    "    day_cols = [col for col in daily_df.columns if col.startswith(\"Day_\")]\n",
    "\n",
    "    merged = main_chain_feeder_df.merge(daily_df, left_on='FEEDER_ID', right_on='SWNO', how='left')\n",
    "\n",
    "    result_rows = []\n",
    "\n",
    "    for feeder_id, group in merged.groupby('FEEDER_ID', sort=False):\n",
    "        group = group.copy()\n",
    "        group['__orig_idx'] = group.index\n",
    "        ranks_in_order = group['RANK'].drop_duplicates().tolist()\n",
    "\n",
    "        dt_load_sums = {\n",
    "            rank: group[group['RANK'] == rank]['DT_LOAD_CAL'].fillna(0).sum()\n",
    "            for rank in ranks_in_order\n",
    "        }\n",
    "\n",
    "        prev_cum = {d: None for d in day_cols}\n",
    "\n",
    "        for rank in ranks_in_order:\n",
    "            this_rank_rows = group[group['RANK'] == rank].sort_values('__orig_idx')\n",
    "            first = True\n",
    "            for idx, row in this_rank_rows.iterrows():\n",
    "                base = {col: row[col] for col in feeder_df.columns}\n",
    "                for d in day_cols:\n",
    "                    day_val = row.get(d, None)\n",
    "                    if first:\n",
    "                        if rank == ranks_in_order[0]:\n",
    "                            cum_val = day_val - dt_load_sums[rank] if pd.notna(day_val) else None\n",
    "                        else:\n",
    "                            prev = prev_cum[d]\n",
    "                            cum_val = prev - dt_load_sums[rank] if prev is not None else None\n",
    "                        prev_cum[d] = cum_val\n",
    "                    else:\n",
    "                        cum_val = None\n",
    "                    base[f\"{d}_cumulative_diff\"] = cum_val\n",
    "                result_rows.append(base)\n",
    "                first = False\n",
    "\n",
    "    result_df = pd.DataFrame(result_rows)\n",
    "    output_cols = list(feeder_df.columns) + [f\"{d}_cumulative_diff\" for d in day_cols]\n",
    "    result_df.to_csv(output_path, index=False, columns=output_cols)\n",
    "    print(f\"Saved to {output_path}\")\n",
    "\n",
    "# Process each year\n",
    "process_year(main_chain_feeder_df, daily_path_1, output_path_1)\n",
    "process_year(main_chain_feeder_df, daily_path_2, output_path_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17c84863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/final_feeder_11kv_daily_cumulative_diff_2025.csv\n",
      "Saved to /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/final_feeder_11kv_daily_cumulative_diff_2024.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Identify only main-chain feeders (no sub-branches) ---\n",
    "feeder_df = pd.read_csv(feeder_path)\n",
    "\n",
    "def is_main_chain_only(ranks):\n",
    "    return all('.' not in str(r) for r in ranks)\n",
    "\n",
    "main_chain_flags = (\n",
    "    feeder_df.groupby('FEEDER_ID')['RANK']\n",
    "    .apply(is_main_chain_only)\n",
    "    .rename('IS_MAIN_CHAIN_ONLY')\n",
    "    .reset_index()\n",
    ")\n",
    "main_chain_ids = main_chain_flags[main_chain_flags['IS_MAIN_CHAIN_ONLY']]['FEEDER_ID'].tolist()\n",
    "main_chain_feeder_df = feeder_df[feeder_df['FEEDER_ID'].isin(main_chain_ids)].copy()\n",
    "\n",
    "# --- Processing function ---\n",
    "def process_year(main_chain_feeder_df, daily_path, output_path):\n",
    "    daily_df = pd.read_csv(daily_path)\n",
    "    day_cols = [col for col in daily_df.columns if col.startswith(\"Day_\")]\n",
    "\n",
    "    merged = main_chain_feeder_df.merge(daily_df, left_on='FEEDER_ID', right_on='SWNO', how='left')\n",
    "\n",
    "    result_rows = []\n",
    "\n",
    "    for feeder_id, group in merged.groupby('FEEDER_ID', sort=False):\n",
    "        group = group.copy()\n",
    "        group['__orig_idx'] = group.index\n",
    "        ranks_in_order = group['RANK'].drop_duplicates().tolist()\n",
    "\n",
    "        dt_load_sums = {\n",
    "            rank: group[group['RANK'] == rank]['DT_LOAD_CAL'].fillna(0).sum()\n",
    "            for rank in ranks_in_order\n",
    "        }\n",
    "\n",
    "        prev_cum = {d: None for d in day_cols}\n",
    "\n",
    "        for rank in ranks_in_order:\n",
    "            this_rank_rows = group[group['RANK'] == rank].sort_values('__orig_idx')\n",
    "            first = True\n",
    "            for idx, row in this_rank_rows.iterrows():\n",
    "                base = {col: row[col] for col in main_chain_feeder_df.columns}\n",
    "                for d in day_cols:\n",
    "                    day_val = row.get(d, None)\n",
    "                    if first:\n",
    "                        if rank == ranks_in_order[0]:\n",
    "                            cum_val = day_val - dt_load_sums[rank] if pd.notna(day_val) else None\n",
    "                        else:\n",
    "                            prev = prev_cum[d]\n",
    "                            cum_val = prev - dt_load_sums[rank] if prev is not None else None\n",
    "                        prev_cum[d] = cum_val\n",
    "                    else:\n",
    "                        cum_val = None\n",
    "                    base[f\"{d}_cumulative_diff\"] = cum_val\n",
    "                result_rows.append(base)\n",
    "                first = False\n",
    "\n",
    "    result_df = pd.DataFrame(result_rows)\n",
    "    # --- Add FROM_SWITCH and TO_SWITCH columns ---\n",
    "    if \"FROM_TO\" in result_df.columns:\n",
    "        from_to_split = result_df[\"FROM_TO\"].str.split(\"-\", n=1, expand=True)\n",
    "        result_df[\"FROM_SWITCH\"] = from_to_split[0]\n",
    "        result_df[\"TO_SWITCH\"] = from_to_split[1]\n",
    "        cols = list(result_df.columns)\n",
    "        i = cols.index(\"FROM_TO\") + 1\n",
    "        for col in [\"FROM_SWITCH\", \"TO_SWITCH\"]:\n",
    "            cols.insert(i, cols.pop(cols.index(col)))\n",
    "            i += 1\n",
    "        result_df = result_df[cols]\n",
    "    # --- Save ---\n",
    "    output_cols = list(result_df.columns)  # Save all columns, now includes FROM_SWITCH and TO_SWITCH\n",
    "    result_df.to_csv(output_path, index=False, columns=output_cols)\n",
    "    print(f\"Saved to {output_path}\")\n",
    "\n",
    "# --- Process each year ---\n",
    "process_year(main_chain_feeder_df, daily_path_1, output_path_1)\n",
    "process_year(main_chain_feeder_df, daily_path_2, output_path_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b89993",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52fbb275",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c3bf827",
   "metadata": {},
   "source": [
    "NEW LOGIC TO CALCULATE THE LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbbda874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Wrote with integer node_load_* and integer sec_cum_* → /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/fedder_11KV_with_all_monthly_loads.csv\n",
      "FEEDER_ID   DESTINATION_LOCATION  node_load_2024_01  sec_cum_2024_03\n",
      "    10205 1S-MH-MU-ZSC-CL09-2382                  8             52.0\n",
      "    10205 1S-MH-MU-ZSC-CL09-2383                  8             27.0\n",
      "    10205 1S-MH-MU-ZSC-CL09-2723                  6            114.0\n",
      "    10205 1S-MH-MU-ZSC-CL09-2366                  2             32.0\n",
      "    10205 1S-MH-MU-ZSC-CL09-2348                  5             26.0\n",
      "    10205 1S-MH-MU-ZSC-CL09-2019                  3              9.0\n",
      "    10205 1S-MH-MU-ZSC-CL09-2074                  3             63.0\n",
      "    10205 1S-MH-MU-ZSC-CL06-3404                  0             29.0\n",
      "    10205 1S-MH-MU-ZSC-CL06-3489                  3             29.0\n",
      "    10205 1S-MH-MU-ZSC-CL06-2088                  6             20.0\n",
      "    10205 1S-MH-MU-ZSC-CL09-3373                  1             25.0\n",
      "    10205 1S-MH-MU-ZSC-CL09-3058                  2             22.0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Per-node monthly loads (integer) + cumulative section loads (integer, sec_cum_* only)\n",
    "using monthly_SWNO_matrix_11KV.csv (SWNO == FEEDER_ID).\n",
    "\n",
    "Output columns added:\n",
    "  - SUM_DT_LOAD_CAL, FRACTION\n",
    "  - feeder_2024_01.._12      (float)\n",
    "  - node_load_2024_01.._12   (Int64, rounded BEFORE cumulative)\n",
    "  - sec_cum_2024_01.._12     (Int64)\n",
    "\n",
    "Writes:\n",
    "  /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/fedder_11KV_with_all_monthly_loads.csv\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ── Paths ───────────────────────────────────────────────────────────────────\n",
    "BASE      = \"/media/sagark24/New Volume/MERGE CDIS\"\n",
    "IN_FEDDER = Path(f\"{BASE}/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/11_WITHOUTDT_CONNECTED_RANKED.csv\")\n",
    "IN_MATRIX = Path(f\"{BASE}/IPYNB_FILE/DATA_GENERATION/monthly_SWNO_matrix_11KV.csv\")\n",
    "OUT_FILE  = Path(f\"{BASE}/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/fedder_11KV_with_all_monthly_loads.csv\")\n",
    "\n",
    "# ── Months & columns ────────────────────────────────────────────────────────\n",
    "YEAR = 2024\n",
    "MONTH_KEYS = [f\"Month_{i:02d}\" for i in range(1, 13)]          # in matrix\n",
    "MONTH_TAGS = [f\"{YEAR}-{i:02d}\" for i in range(1, 13)]         # long tags\n",
    "NODE_COLS  = [f\"node_load_{YEAR}_{i:02d}\" for i in range(1, 13)]\n",
    "CUM_COLS   = [f\"sec_cum_{YEAR}_{i:02d}\"   for i in range(1, 13)]\n",
    "FEED_COLS  = [f\"feeder_{YEAR}_{i:02d}\"    for i in range(1, 13)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ── Helpers ─────────────────────────────────────────────────────────────────\n",
    "def make_feed_long(mat: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return FEEDER_ID|MONTH|FEEDER_LOAD_AVG (Jan..Dec complete for all feeders).\"\"\"\n",
    "    need = {\"SWNO\", *MONTH_KEYS}\n",
    "    miss = [c for c in need if c not in mat.columns]\n",
    "    if miss:\n",
    "        raise RuntimeError(f\"monthly matrix missing columns: {miss}\")\n",
    "\n",
    "    m = mat.copy()\n",
    "    m[\"FEEDER_ID\"] = m[\"SWNO\"].astype(str).str.strip()\n",
    "    for c in MONTH_KEYS:\n",
    "        m[c] = pd.to_numeric(m[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    long = (m.melt(id_vars=[\"FEEDER_ID\"], value_vars=MONTH_KEYS,\n",
    "                   var_name=\"MONTH_KEY\", value_name=\"FEEDER_LOAD_AVG\")\n",
    "              .assign(MONTH=lambda d: d[\"MONTH_KEY\"].str.replace(\"Month_\", f\"{YEAR}-\", regex=False))\n",
    "              .drop(columns=[\"MONTH_KEY\"]))\n",
    "    complete = (pd.MultiIndex.from_product([m[\"FEEDER_ID\"].unique(), MONTH_TAGS],\n",
    "                                           names=[\"FEEDER_ID\",\"MONTH\"]).to_frame(index=False))\n",
    "    return (complete.merge(long, on=[\"FEEDER_ID\",\"MONTH\"], how=\"left\")\n",
    "                    .fillna({\"FEEDER_LOAD_AVG\": 0.0}))\n",
    "\n",
    "def build_graph(df_f: pd.DataFrame) -> dict[str, list[str]]:\n",
    "    \"\"\"Adjacency: SOURCE_LOCATION -> [DESTINATION_LOCATION...] (dedup).\"\"\"\n",
    "    adj: dict[str, list[str]] = {}\n",
    "    for s, d in zip(df_f[\"SOURCE_LOCATION\"].astype(str), df_f[\"DESTINATION_LOCATION\"].astype(str)):\n",
    "        adj.setdefault(s, [])\n",
    "        if d not in adj[s]:\n",
    "            adj[s].append(d)\n",
    "        adj.setdefault(d, [])  # ensure leaf key\n",
    "    return adj\n",
    "\n",
    "def dfs_subtree_cum(adj: dict[str, list[str]],\n",
    "                    node_vec: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Post-order DFS (include-self). Each node gets a length-12 int vector:\n",
    "      agg[node] = node_vec[node] + sum(agg[child]).\n",
    "    \"\"\"\n",
    "    agg: dict[str, np.ndarray] = {}\n",
    "    state: dict[str, int] = {}  # 0=unseen,1=visiting,2=done\n",
    "\n",
    "    all_nodes = set(adj.keys())\n",
    "    for kids in adj.values():\n",
    "        all_nodes.update(kids)\n",
    "\n",
    "    for root in list(all_nodes):\n",
    "        if state.get(root, 0) != 0:\n",
    "            continue\n",
    "        stack: list[tuple[str, int]] = [(root, 0)]\n",
    "        while stack:\n",
    "            node, phase = stack.pop()\n",
    "            if phase == 0:\n",
    "                if state.get(node, 0) == 1:\n",
    "                    continue  # cycle guard\n",
    "                if state.get(node, 0) == 2:\n",
    "                    continue\n",
    "                state[node] = 1\n",
    "                stack.append((node, 1))\n",
    "                for ch in adj.get(node, []):\n",
    "                    if state.get(ch, 0) != 2:\n",
    "                        stack.append((ch, 0))\n",
    "            else:\n",
    "                subtotal = node_vec.get(node, np.zeros(12, dtype=np.int64)).copy()\n",
    "                for ch in adj.get(node, []):\n",
    "                    subtotal += agg.get(ch, np.zeros(12, dtype=np.int64))\n",
    "                agg[node] = subtotal\n",
    "                state[node] = 2\n",
    "    return agg\n",
    "\n",
    "\n",
    "# ── Main ────────────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    if not IN_FEDDER.exists():\n",
    "        raise FileNotFoundError(f\"Feeder file not found: {IN_FEDDER}\")\n",
    "    if not IN_MATRIX.exists():\n",
    "        raise FileNotFoundError(f\"Monthly matrix not found: {IN_MATRIX}\")\n",
    "\n",
    "    # 1) Feeder trace\n",
    "    df = pd.read_csv(IN_FEDDER, low_memory=False)\n",
    "    need = {\"FEEDER_ID\",\"SOURCE_LOCATION\",\"DESTINATION_LOCATION\"}\n",
    "    miss = [c for c in need if c not in df.columns]\n",
    "    if miss:\n",
    "        raise RuntimeError(f\"Input missing columns: {miss}\")\n",
    "    df[\"FEEDER_ID\"] = df[\"FEEDER_ID\"].astype(str).str.strip()\n",
    "    if \"LOCATION\" not in df.columns:\n",
    "        df[\"LOCATION\"] = df[\"DESTINATION_LOCATION\"]\n",
    "\n",
    "    # Ensure DT_LOAD_CAL (if missing, compute from DT_LOAD with nominal denominator)\n",
    "    if \"DT_LOAD_CAL\" not in df.columns:\n",
    "        den = 11 * 0.95 * np.sqrt(3.0)\n",
    "        df[\"DT_LOAD_CAL\"] = pd.to_numeric(df.get(\"DT_LOAD\", np.nan), errors=\"coerce\") / den\n",
    "\n",
    "    # 2) Fractions per feeder on UNIQUE nodes (sum to avoid double counting)\n",
    "    uniq = (df[[\"FEEDER_ID\",\"LOCATION\",\"DT_LOAD_CAL\"]]\n",
    "            .dropna(subset=[\"LOCATION\"])\n",
    "            .groupby([\"FEEDER_ID\",\"LOCATION\"], as_index=False)\n",
    "            .agg(DT_LOAD_CAL=(\"DT_LOAD_CAL\",\"sum\")))\n",
    "    sum_by_fid = uniq.groupby(\"FEEDER_ID\")[\"DT_LOAD_CAL\"].sum().rename(\"SUM_DT_LOAD_CAL\")\n",
    "    uniq = uniq.merge(sum_by_fid, on=\"FEEDER_ID\", how=\"left\")\n",
    "    uniq[\"FRACTION\"] = uniq[\"DT_LOAD_CAL\"] / uniq[\"SUM_DT_LOAD_CAL\"].replace(0, np.nan)\n",
    "\n",
    "    # Bring only what's needed back to df\n",
    "    loc_key = \"DESTINATION_LOCATION\"\n",
    "    df = df.merge(\n",
    "        uniq.rename(columns={\"LOCATION\": loc_key})[[\"FEEDER_ID\", loc_key, \"SUM_DT_LOAD_CAL\", \"FRACTION\"]],\n",
    "        on=[\"FEEDER_ID\", loc_key],\n",
    "        how=\"left\",\n",
    "        validate=\"many_to_one\"\n",
    "    )\n",
    "\n",
    "    # 3) Feeder monthly averages (from matrix)\n",
    "    matrix = pd.read_csv(IN_MATRIX, low_memory=False)\n",
    "    feed_long = make_feed_long(matrix)  # FEEDER_ID|MONTH|FEEDER_LOAD_AVG\n",
    "    # wide copy (optional, kept for reference/output)\n",
    "    feed_wide = (feed_long.pivot_table(index=\"FEEDER_ID\", columns=\"MONTH\",\n",
    "                                       values=\"FEEDER_LOAD_AVG\", aggfunc=\"mean\", fill_value=0.0)\n",
    "                            .reindex(columns=MONTH_TAGS, fill_value=0.0)\n",
    "                            .reset_index())\n",
    "    feed_wide.columns = [\"FEEDER_ID\"] + FEED_COLS\n",
    "    df = df.merge(feed_wide, on=\"FEEDER_ID\", how=\"left\")\n",
    "\n",
    "    # 4) Per-node monthly corrected loads → integer node_load_YYYY_MM (round BEFORE cumulative)\n",
    "    out_nodes = (uniq[[\"FEEDER_ID\",\"LOCATION\",\"FRACTION\"]]\n",
    "                 .merge(feed_long, on=\"FEEDER_ID\", how=\"left\"))\n",
    "    out_nodes[\"NODE_LOAD\"] = out_nodes[\"FRACTION\"].fillna(0.0) * out_nodes[\"FEEDER_LOAD_AVG\"].fillna(0.0)\n",
    "\n",
    "    node_wide = (out_nodes.pivot_table(index=[\"FEEDER_ID\",\"LOCATION\"], columns=\"MONTH\",\n",
    "                                       values=\"NODE_LOAD\", aggfunc=\"sum\", fill_value=0.0)\n",
    "                           .reindex(columns=MONTH_TAGS, fill_value=0.0)\n",
    "                           .reset_index())\n",
    "\n",
    "\n",
    "    node_wide[MONTH_TAGS] = np.ceil(node_wide[MONTH_TAGS].astype(float)).astype(\"Int64\")\n",
    "\n",
    "    node_wide_out = node_wide.rename(columns={\"LOCATION\": loc_key}).copy()\n",
    "    node_wide_out.columns = [\"FEEDER_ID\", loc_key] + NODE_COLS\n",
    "    df = df.merge(node_wide_out, on=[\"FEEDER_ID\", loc_key], how=\"left\")\n",
    "\n",
    "    # 5) Bottom-up DFS per feeder → integer cumulative section loads (sec_cum_*)\n",
    "    sec_parts: list[pd.DataFrame] = []\n",
    "    for fid, gdf in df.groupby(\"FEEDER_ID\", sort=False):\n",
    "        # Graphfor fid, gdf in df.groupby(\"FEEDER_ID\", sort=False):\n",
    "        # Graph\n",
    "        adj = build_graph(gdf[[\"SOURCE_LOCATION\",\"DESTINATION_LOCATION\"]])\n",
    "\n",
    "        # Node vectors for this feeder (Int64 arrays length 12)\n",
    "        sub_nodes = node_wide[node_wide[\"FEEDER_ID\"] == fid]\n",
    "        node_vec: dict[str, np.ndarray] = {str(loc): sub_nodes.loc[sub_nodes[\"LOCATION\"] == loc, MONTH_TAGS]\n",
    "                                                       .to_numpy(dtype=\"int64\")\n",
    "                                                       .reshape(-1, 12)[0]\n",
    "                                           for loc in sub_nodes[\"LOCATION\"].astype(str).unique()}\n",
    "        # ensure every node in the graph has a vector\n",
    "        all_nodes = set(adj.keys())\n",
    "        for kids in adj.values():\n",
    "            all_nodes.update(kids)\n",
    "        for n in all_nodes:\n",
    "            node_vec.setdefault(n, np.zeros(12, dtype=np.int64))\n",
    "\n",
    "        # Aggregate subtree monthly totals (include self), then cumulative Jan→Dec\n",
    "        agg = dfs_subtree_cum(adj, node_vec)\n",
    "        rows = []\n",
    "        for loc, vec in agg.items():\n",
    "            cum = np.cumsum(vec, dtype=np.int64)\n",
    "            rows.append(\n",
    "                {\"FEEDER_ID\": fid, \"DESTINATION_LOCATION\": loc, **{\n",
    "                    f\"sec_cum_{m.replace('-', '_')}\": int(cum[j]) for j, m in enumerate(MONTH_TAGS)\n",
    "                }}\n",
    "            )\n",
    "        sec_parts.append(pd.DataFrame(rows))\n",
    "\n",
    "    sec_all = pd.concat(sec_parts, ignore_index=True) if sec_parts else pd.DataFrame(\n",
    "        columns=[\"FEEDER_ID\",\"DESTINATION_LOCATION\"] + CUM_COLS\n",
    "    )\n",
    "    df = df.merge(sec_all, on=[\"FEEDER_ID\",\"DESTINATION_LOCATION\"], how=\"left\")\n",
    "\n",
    "    # 6) Write\n",
    "    df.to_csv(OUT_FILE, index=False)\n",
    "    print(f\"✓ Wrote with integer node_load_* and integer sec_cum_* → {OUT_FILE}\")\n",
    "\n",
    "    # Peek\n",
    "    cols = [\"FEEDER_ID\",\"DESTINATION_LOCATION\",\"node_load_2024_01\",\"sec_cum_2024_03\"]\n",
    "    print(df[[c for c in cols if c in df.columns]].head(12).to_string(index=False))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e785354d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1099\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/fedder_11KV_with_all_monthly_loads.csv\")\n",
    "col = df['FEEDER_ID'].unique()\n",
    "print(len(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e85d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
