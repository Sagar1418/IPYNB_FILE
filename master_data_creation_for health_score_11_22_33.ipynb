{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "856d625f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Cable_ID Voltage_Level Feeder_ID Sub_Feeder_ID From_Switch  \\\n",
      "0     33KV-SW1-SW2          33kV                                 SW1   \n",
      "1     22KV-SW3-SW4          22kV                                 SW3   \n",
      "2       FDR1-DT001          11kV      FDR1                             \n",
      "3  FDR2-SUB1-DT005          11kV      FDR2          SUB1               \n",
      "\n",
      "  To_Switch Cable_Type  Cable_Age_Years  Length_m Installation_Environment  \\\n",
      "0       SW2       XLPE                7      1200              Underground   \n",
      "1       SW4       PILC                3       800                 Overhead   \n",
      "2                 XLPE                8       400              Underground   \n",
      "3                 XLPE                4       220              Underground   \n",
      "\n",
      "   ... Partial_Discharge_Frequency Partial_Discharge_Intensity  \\\n",
      "0  ...                           0                           0   \n",
      "1  ...                           0                           0   \n",
      "2  ...                           1                           3   \n",
      "3  ...                           0                           0   \n",
      "\n",
      "  Thermal_History_Excursions  Num_Faults   Fault_Type  Repairs_Count  \\\n",
      "0                          0           0         None              0   \n",
      "1                          1           1  Earth Fault              1   \n",
      "2                          1           2  Earth Fault              2   \n",
      "3                          0           0         None              0   \n",
      "\n",
      "     Joint_History  Corrosivity  Water_Ingress  Remarks  \n",
      "0         Original          Low             No  Healthy  \n",
      "1    Repaired once       Medium             No  Monitor  \n",
      "2  Multiple joints         High            Yes  Healthy  \n",
      "3         Original          Low             No           \n",
      "\n",
      "[4 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = [\n",
    "    \"Cable_ID\",\"Voltage_Level\",\"Feeder_ID\",\"Sub_Feeder_ID\",\"From_Switch\",\"To_Switch\",\n",
    "    \"Cable_Type\",\"Cable_Age_Years\",\"Length_m\",\"Installation_Environment\",\n",
    "    \"Soil_Type\",\"Humidity\",\"Proximity_to_Water\",\"Load_History_Avg_Load\",\"Load_History_Peak_Load\", \n",
    "    \"Loading_Cycles\",\"Overload_Events\",\"IR_Measurement_MOhm\",\"Tan_Delta\",\"Partial_Discharge_Frequency\",\n",
    "    \"Partial_Discharge_Intensity\",\"Thermal_History_Excursions\",\"Num_Faults\",\"Fault_Type\",\"Repairs_Count\",\n",
    "    \"Joint_History\",\"Corrosivity\",\"Water_Ingress\",\"Remarks\"\n",
    "]\n",
    "\n",
    "# Sample data for different voltage levels. You can add or import your actual data here.\n",
    "data = [\n",
    "    [\n",
    "        \"33KV-SW1-SW2\", \"33kV\", \"\", \"\", \"SW1\", \"SW2\", \"XLPE\", 7, 1200, \"Underground\",\n",
    "        \"Sandy\", \"Medium\", \"Far\", 60, 100, 150, 1, 500, 0.001, 0, 0, 0, 0, \"None\", 0, \"Original\", \"Low\", \"No\", \"Healthy\"\n",
    "    ],\n",
    "    [\n",
    "        \"22KV-SW3-SW4\", \"22kV\", \"\", \"\", \"SW3\", \"SW4\", \"PILC\", 3, 800, \"Overhead\",\n",
    "        \"Clay\", \"High\", \"Near\", 50, 90, 100, 0, 400, 0.002, 0, 0, 1, 1, \"Earth Fault\", 1, \"Repaired once\", \"Medium\", \"No\", \"Monitor\"\n",
    "    ],\n",
    "    [\n",
    "        \"FDR1-DT001\", \"11kV\", \"FDR1\", \"\", \"\", \"\", \"XLPE\", 8, 400, \"Underground\",\n",
    "        \"Loam\", \"Medium\", \"Near\", 40, 80, 120, 2, 120, 0.003, 1, 3, 1, 2, \"Earth Fault\", 2, \"Multiple joints\", \"High\", \"Yes\", \"Healthy\"\n",
    "    ],\n",
    "    [\n",
    "        \"FDR2-SUB1-DT005\", \"11kV\", \"FDR2\", \"SUB1\", \"\", \"\", \"XLPE\", 4, 220, \"Underground\",\n",
    "        \"Rocky\", \"Low\", \"Far\", 30, 50, 90, 0, 200, 0.001, 0, 0, 0, 0, \"None\", 0, \"Original\", \"Low\", \"No\", \"\"\n",
    "    ],\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Save as Excel and CSV for use\n",
    "# df.to_excel(\"/media/sagarkumar/New Volume/SAGAR/DATA_GENERATION/master_cable_data_final.xlsx\", index=False)\n",
    "df.to_csv(\"/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/master_cable_data_final.csv\", index=False)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f32a353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted file written.\n",
      "                        Size Insulation Voltage          Type  \\\n",
      "0                      SQ.MM  PILC+XLPE    11KV  HTCF SECTION   \n",
      "1            .06+70+120SQ.MM  PILC+XLPE    11KV  HTCF SECTION   \n",
      "2  0.15+185+225+300+240SQ.MM  PILC+XLPE    11KV  HTCF SECTION   \n",
      "3                   120SQ.MM       PILC    11KV  HTCF SECTION   \n",
      "4                   120SQ.MM       PILC    11KV  HTCF SECTION   \n",
      "5                   120SQ.MM       PILC    11KV  HTCF SECTION   \n",
      "6                   120SQ.MM       PILC    11KV  HTCF SECTION   \n",
      "7                   120SQ.MM       PILC    11KV  HTCF SECTION   \n",
      "8                   120SQ.MM       PILC    11KV  HTCF SECTION   \n",
      "9                   120SQ.MM       PILC    11KV  HTCF SECTION   \n",
      "\n",
      "                       FROM FROM_SWITCH                      TO TO_SWITCH  \\\n",
      "0  VISHWESHWAR NAGAR HETALI       40478                  SAMANT     05587   \n",
      "1        JAWAHAR NAGAR NO.3       13847    SONAVALA ESTATE NO.1     00714   \n",
      "2                                                                           \n",
      "3                AAREY NO.2       18066   AAREY UNIT NO.7 KIOSK     18008   \n",
      "4    AAREY SANKRAMAN STUDIO       19112         AAREY UNIT NO.2     19710   \n",
      "5                 AJIT PARK       15725      TUREL PAKHADI NO.1     06900   \n",
      "6       BABREKAR NAGAR NO.1       18669   KANDIVLI HOUSING NO.3     18671   \n",
      "7    BHAGATSINGH NAGAR NO.1       28860  BHAGATSINGH NAGAR NO.3     34127   \n",
      "8  DINDOSHI VASAHAT CENTRAL       17141  DINDOSHI VASAHAT SOUTH     14101   \n",
      "9            GOKULDHAM NO.2       07636          GOKULDHAM NO.1     06877   \n",
      "\n",
      "                                      DELAYED_REASON  \\\n",
      "0                                                      \n",
      "1  1)FAILED TO OPEN JAWAHAR NAGAR ROAD NO.12 S/S ...   \n",
      "2  1)FAILED TO CLOSE GOREGAON SHOPPING CENTRE S/S...   \n",
      "3  1)AAREY NO.2 S/S (DMS) WENT OFFLINE AFTER GIVI...   \n",
      "4  1) NON DMS SUBSTATION ARE IN ARREY AREA. DMS U...   \n",
      "5  1)TUREL PAKHADI NO.1 S/S (DMS) WENT OFFLINE AF...   \n",
      "6       1) DMS OFFLINE AT KANDIVLI HOUSING NO.3 S/S.   \n",
      "7  1)FAILED TO OPEN BANGUR NAGAR NO.3 S/S DMS SW....   \n",
      "8                                                      \n",
      "9  1) FPI MALFUNCTION AT GOKULDHAM NO.1 S/S SW.NO...   \n",
      "\n",
      "                                        FAULT_NATURE  \\\n",
      "0                                                      \n",
      "1  FAILED TO OPEN JAWAHAR NAGAR ROAD NO; FAILED T...   \n",
      "2  FAILED TO CLOSE GOREGAON SHOPPING CENTRE S/S D...   \n",
      "3                  WENT OFFLINE AFTER GIVING COMMAND   \n",
      "4                                                      \n",
      "5  FAILED TO OPEN AJIT PARK S/S DMS SW; WENT OFFL...   \n",
      "6                 DMS OFFLINE AT KANDIVLI HOUSING NO   \n",
      "7                     FAILED TO OPEN BANGUR NAGAR NO   \n",
      "8  CABLE DAMAGED BY BMC ON DINDOSHI VASAHAT SOUTH RD   \n",
      "9  FPI MALFUNCTION AT GOKULDHAM NO; FPI MALFUNCTI...   \n",
      "\n",
      "                 TIME_OUTAGE        MAIN_REPORTED_TIME  \\\n",
      "0  2020-04-30 17:15:00+00:00 2020-04-30 17:54:08+00:00   \n",
      "1  2020-06-03 14:53:00+00:00 2020-06-03 15:41:43+00:00   \n",
      "2  2020-02-19 22:11:22+00:00 2020-02-19 23:07:51+00:00   \n",
      "3  2020-06-05 15:09:00+00:00 2020-06-05 17:41:23+00:00   \n",
      "4  2020-07-16 06:58:00+00:00 2020-07-16 07:50:19+00:00   \n",
      "5  2020-04-24 04:15:00+00:00 2020-04-24 06:46:57+00:00   \n",
      "6  2020-01-24 15:02:00+00:00 2020-01-24 15:15:03+00:00   \n",
      "7  2020-10-20 15:27:02+00:00 2020-10-20 17:51:23+00:00   \n",
      "8  2020-01-21 11:18:00+00:00 2020-01-21 12:09:16+00:00   \n",
      "9  2020-02-02 17:57:00+00:00 2020-02-02 20:39:00+00:00   \n",
      "\n",
      "              TIME_RESTORED  TIME_DIFFERENCE  \n",
      "0 2020-04-30 17:33:00+00:00       -21.133333  \n",
      "1 2020-06-03 15:41:43+00:00         0.000000  \n",
      "2 2020-02-19 22:45:31+00:00       -22.333333  \n",
      "3 2020-06-05 16:09:00+00:00       -92.383333  \n",
      "4 2020-07-16 07:50:19+00:00         0.000000  \n",
      "5 2020-04-24 04:55:00+00:00      -111.950000  \n",
      "6 2020-01-24 15:15:03+00:00         0.000000  \n",
      "7 2020-10-20 17:51:24+00:00         0.016667  \n",
      "8 2020-01-21 11:42:00+00:00       -27.266667  \n",
      "9 2020-02-02 20:39:00+00:00         0.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Load file (final clean file)\n",
    "df_original = pd.read_csv('/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/TXN_NMS_HTLOGSHEET_cleaned.csv', low_memory=False)\n",
    "\n",
    "# 2. Filter ENTRY_TYPE == 1\n",
    "df_entry1 = df_original[df_original['ENTRY_TYPE'] == 1].copy().reset_index(drop=True)\n",
    "\n",
    "# 3. UPPERCASE remarks\n",
    "df_entry1['FREE_REMARKS_UPPER'] = df_entry1['FREE_REMARKS'].astype(str).str.upper()\n",
    "\n",
    "# 4. Extraction functions\n",
    "def extract_size(text):\n",
    "    m = re.search(r'X\\s*([\\d\\.\\+\\-\\s]*SQ\\.?\\s*MM)', text)\n",
    "    return m.group(1).replace(\" \", \"\") if m else \"\"\n",
    "\n",
    "def extract_insulation(text):\n",
    "    m = re.search(r'(PILC\\+XLPE|XLPE\\+PILC|PILC|XLPE)', text)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def extract_voltage(text):\n",
    "    m = re.search(r'(\\d{2,3})\\s*KV', text)\n",
    "    return f\"{m.group(1)}KV\" if m else \"\"\n",
    "\n",
    "def extract_type(text):\n",
    "    m = re.search(r'(HTCF SECTION|LT SECTION|HT SECTION)', text)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def extract_section(text):\n",
    "    m = re.search(r'BETWEEN\\s+(.+?)\\s+S/S\\s+SWITCH:([0-9]+)\\s+TO\\s+(.+?)\\s+S/S\\s+SWITCH:([0-9]+)', text)\n",
    "    if m:\n",
    "        return pd.Series([m.group(1).strip(), m.group(2), m.group(3).strip(), m.group(4)])\n",
    "    return pd.Series([\"\", \"\", \"\", \"\"])\n",
    "\n",
    "def extract_delayed_reason(text):\n",
    "    m = re.search(r'DELAYED DUE TO(.*?)(?:NOTIFICATION NO|$)', text)\n",
    "    return m.group(1).strip() if m else \"\"\n",
    "\n",
    "def extract_fault_nature(text):\n",
    "    patterns = [\n",
    "        'CABLE DAMAGED BY', 'DMS OFFLINE AT', 'FPI MALFUNCTION',\n",
    "        'FAILED TO OPEN', 'FAILED TO CLOSE', 'WENT OFFLINE',\n",
    "        'FEEDER TRIPPING', 'TRAFFIC ON', 'SUPPLY RESTORATION DELAYED'\n",
    "    ]\n",
    "    found = []\n",
    "    for pat in patterns:\n",
    "        for m in re.finditer(r'({0}.*?)(?:\\.|,|;|NOTIFICATION NO|$)'.format(re.escape(pat)), text):\n",
    "            found.append(m.group(1).strip())\n",
    "    return '; '.join(found) if found else \"\"\n",
    "\n",
    "# 5. Extraction (create new DataFrame for output)\n",
    "out = pd.DataFrame()\n",
    "out['Size'] = df_entry1['FREE_REMARKS_UPPER'].apply(extract_size)\n",
    "out['Insulation'] = df_entry1['FREE_REMARKS_UPPER'].apply(extract_insulation)\n",
    "out['Voltage'] = df_entry1['FREE_REMARKS_UPPER'].apply(extract_voltage)\n",
    "out['Type'] = df_entry1['FREE_REMARKS_UPPER'].apply(extract_type)\n",
    "\n",
    "section_cols = df_entry1['FREE_REMARKS_UPPER'].apply(extract_section)\n",
    "section_cols.columns = ['FROM', 'FROM_SWITCH', 'TO', 'TO_SWITCH']\n",
    "out = pd.concat([out, section_cols], axis=1)\n",
    "\n",
    "out['DELAYED_REASON'] = df_entry1['FREE_REMARKS_UPPER'].apply(extract_delayed_reason)\n",
    "out['FAULT_NATURE'] = df_entry1['FREE_REMARKS_UPPER'].apply(extract_fault_nature)\n",
    "\n",
    "# Time columns\n",
    "out['TIME_OUTAGE'] = df_entry1['TIME_OUTAGE'].astype(str)\n",
    "out['MAIN_REPORTED_TIME'] = pd.to_datetime(df_entry1['MAIN_REPORTED_TIME'], errors='coerce')\n",
    "out['TIME_RESTORED'] = pd.to_datetime(df_entry1['TIME_RESTORED'], errors='coerce')\n",
    "out['TIME_DIFFERENCE'] = (out['TIME_RESTORED'] - out['MAIN_REPORTED_TIME']).dt.total_seconds() / 60\n",
    "\n",
    "# 6. Save\n",
    "final_cols = [\n",
    "    'Size', 'Insulation', 'Voltage', 'Type',\n",
    "    'FROM', 'FROM_SWITCH', 'TO', 'TO_SWITCH',\n",
    "    'DELAYED_REASON', 'FAULT_NATURE',\n",
    "    'TIME_OUTAGE', 'MAIN_REPORTED_TIME', 'TIME_RESTORED', 'TIME_DIFFERENCE'\n",
    "]\n",
    "out[final_cols].to_csv('/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/processed_fault_data_FINAL.csv', index=False)\n",
    "print(\"Extracted file written.\")\n",
    "print(out[final_cols].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76c63a6",
   "metadata": {},
   "source": [
    "CHECK THE DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26004c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total linebreak cells found: 0\n",
      "Total control-char cells found: 0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file_path = '/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/TXN_NMS_HTLOGSHEET_cleaned.csv'\n",
    "linebreak_count = 0\n",
    "ctrl_count = 0\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for i, row in enumerate(reader):\n",
    "        for j, cell in enumerate(row):\n",
    "            if '\\n' in cell or '\\r' in cell:\n",
    "                print(f'Linebreak found in row {i+1}, column {j+1}: {repr(cell)}')\n",
    "                linebreak_count += 1\n",
    "            # Any ASCII control char except tab (9) and newline (10, 13)\n",
    "            if any(ord(c) < 32 and ord(c) not in (9, 10, 13) for c in cell):\n",
    "                print(f'Ctrl char found in row {i+1}, column {j+1}: {repr(cell)}')\n",
    "                ctrl_count += 1\n",
    "      \n",
    "\n",
    "print(f'Total linebreak cells found: {linebreak_count}')\n",
    "print(f'Total control-char cells found: {ctrl_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b63a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    expected = None\n",
    "    for i, line in enumerate(f):\n",
    "        cols = line.count(',') + 1\n",
    "        if i == 0:\n",
    "            expected = cols\n",
    "        elif cols != expected:\n",
    "            print(f\"Row {i+1} has {cols} columns (Expected: {expected})\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89ce6331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 23705\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Raw file read, no parse (just lines)\n",
    "filename = \"/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/TXN_NMS_HTLOGSHEET_cleaned.csv\"\n",
    "with open(filename, encoding='utf-8', errors='ignore') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(\"Total lines:\", len(lines))\n",
    "\n",
    "# Line breaks within a field detection (very likely in 'FREE_REMARKS' column)\n",
    "for i, line in enumerate(lines):\n",
    "    if '\\n' in line or '\\r' in line:\n",
    "        # \\n to har line me hoga, but internal bhi ho sakta hai\n",
    "        if line.count('\\n') > 1 or line.count('\\r') > 1:\n",
    "            print(f\"Line {i+1}: Multiple linebreaks in a single line\")\n",
    "\n",
    "    # Look for possible tabs, weird ascii\n",
    "    if re.search(r\"[\\t\\x0b\\x0c\\x1b]\", line):\n",
    "        print(f\"Line {i+1}: Contains tab or control char\")\n",
    "\n",
    "# Check for unclosed quotes\n",
    "for i, line in enumerate(lines):\n",
    "    if line.count('\"') % 2 != 0:\n",
    "        print(f\"Line {i+1}: Unmatched double quote\")\n",
    "\n",
    "# Column count (by comma)\n",
    "col_counts = [l.count(',') for l in lines]\n",
    "mode_col_count = max(set(col_counts), key=col_counts.count)\n",
    "for idx, c in enumerate(col_counts):\n",
    "    if c != mode_col_count:\n",
    "        print(f\"Line {idx+1}: {c} columns (Expected: {mode_col_count})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d58d2ed",
   "metadata": {},
   "source": [
    "CLEANED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b0be9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned rows: 23704\n",
      "Deleted rows: 1433\n",
      "Cleaned file: /media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/TXN_NMS_HTLOGSHEET_cleaned.csv\n",
      "Deleted file: /media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/TXN_NMS_HTLOGSHEET_deleted.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "filename = \"/media/sagark24/New Volume/MERGE CDIS/2-Year-data/TXN_NMS_HTLOGSHEET.csv\"\n",
    "cleaned_filename = \"/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/TXN_NMS_HTLOGSHEET_cleaned.csv\"\n",
    "deleted_filename = \"/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/TXN_NMS_HTLOGSHEET_deleted.csv\"\n",
    "\n",
    "# === READ ALL LINES ===\n",
    "with open(filename, encoding='utf-8', errors='ignore') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# === HEADER AND COLUMN COUNT ===\n",
    "header = lines[0]\n",
    "expected_col_count = header.count(',')\n",
    "\n",
    "def check_line_issues(line, expected_col_count):\n",
    "    issues = []\n",
    "    # Remove newline for checks\n",
    "    line_no_nl = line.replace('\\r', '').replace('\\n', '')\n",
    "    # Tab or control characters\n",
    "    if re.search(r\"[\\t\\x0b\\x0c\\x1b]\", line_no_nl):\n",
    "        issues.append(\"Tab/control char\")\n",
    "    # Unmatched quotes\n",
    "    if line_no_nl.count('\"') % 2 != 0:\n",
    "        issues.append(\"Unmatched double quote\")\n",
    "    # Wrong column count\n",
    "    col_count = line_no_nl.count(',')\n",
    "    if col_count != expected_col_count:\n",
    "        issues.append(f\"Column count {col_count} (Expected {expected_col_count})\")\n",
    "    # Multiple linebreaks INSIDE line (almost never happens, as we're reading lines)\n",
    "    if line.count('\\n') > 1 or line.count('\\r') > 1:\n",
    "        issues.append(\"Multiple linebreaks in line\")\n",
    "    return issues\n",
    "\n",
    "# === FILTER LINES ===\n",
    "cleaned_lines = [header]\n",
    "deleted_lines = [header.strip() + \",REASON\\n\"]  # Add a reason column to deleted\n",
    "\n",
    "for idx, line in enumerate(lines[1:], start=2):  # 2nd line onwards\n",
    "    issues = check_line_issues(line, expected_col_count)\n",
    "    if issues:\n",
    "        reason = \"; \".join(issues)\n",
    "        deleted_lines.append(line.strip() + f',\"{reason}\"\\n')\n",
    "    else:\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "# === WRITE OUTPUT FILES ===\n",
    "with open(cleaned_filename, 'w', encoding='utf-8') as f:\n",
    "    f.writelines(cleaned_lines)\n",
    "\n",
    "with open(deleted_filename, 'w', encoding='utf-8') as f:\n",
    "    f.writelines(deleted_lines)\n",
    "\n",
    "print(f\"Cleaned rows: {len(cleaned_lines) - 1}\")\n",
    "print(f\"Deleted rows: {len(deleted_lines) - 1}\")\n",
    "print(f\"Cleaned file: {cleaned_filename}\")\n",
    "print(f\"Deleted file: {deleted_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93e9ccfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done separating good and bad rows.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "rawfile = '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/TXN_NMS_HTLOGSHEET.csv'\n",
    "goodfile = '/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/TXN_NMS_HTLOGSHEET_good.csv'\n",
    "badfile = '/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/TXN_NMS_HTLOGSHEET_bad.csv'\n",
    "\n",
    "with open(rawfile, encoding='utf-8', errors='ignore') as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)\n",
    "    ncol = len(header)\n",
    "\n",
    "with open(rawfile, encoding='utf-8', errors='ignore') as f, \\\n",
    "     open(goodfile, 'w', encoding='utf-8', newline='') as fgood, \\\n",
    "     open(badfile, 'w', encoding='utf-8', newline='') as fbad:\n",
    "    reader = csv.reader(f)\n",
    "    writer_good = csv.writer(fgood)\n",
    "    writer_bad = csv.writer(fbad)\n",
    "    header = next(reader)\n",
    "    writer_good.writerow(header)\n",
    "    writer_bad.writerow(header)\n",
    "    for row in reader:\n",
    "        if len(row) == ncol:\n",
    "            writer_good.writerow(row)\n",
    "        else:\n",
    "            writer_bad.writerow(row)\n",
    "print(\"Done separating good and bad rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85942c4",
   "metadata": {},
   "source": [
    "ONLY FREE_REMARKS ANALYZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17930d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted file written.\n",
      "                        Size Insulation Voltage          Type  \\\n",
      "0                                         220KV                 \n",
      "1                                                               \n",
      "2                      SQ.MM  PILC+XLPE    11KV  HTCF SECTION   \n",
      "3            .06+70+120SQ.MM  PILC+XLPE    11KV  HTCF SECTION   \n",
      "4  0.15+185+225+300+240SQ.MM  PILC+XLPE    11KV  HTCF SECTION   \n",
      "5                   120SQ.MM       PILC    11KV  HTCF SECTION   \n",
      "6                   120SQ.MM       PILC    11KV  HTCF SECTION   \n",
      "7                   120SQ.MM       PILC    11KV  HTCF SECTION   \n",
      "8                   120SQ.MM       PILC    11KV  HTCF SECTION   \n",
      "9                   120SQ.MM       PILC    11KV  HTCF SECTION   \n",
      "\n",
      "                       FROM FROM_SWITCH                      TO TO_SWITCH  \\\n",
      "0                                                                           \n",
      "1                                                                           \n",
      "2  VISHWESHWAR NAGAR HETALI       40478                  SAMANT     05587   \n",
      "3        JAWAHAR NAGAR NO.3       13847    SONAVALA ESTATE NO.1     00714   \n",
      "4                                                                           \n",
      "5                AAREY NO.2       18066   AAREY UNIT NO.7 KIOSK     18008   \n",
      "6    AAREY SANKRAMAN STUDIO       19112         AAREY UNIT NO.2     19710   \n",
      "7                 AJIT PARK       15725      TUREL PAKHADI NO.1     06900   \n",
      "8       BABREKAR NAGAR NO.1       18669   KANDIVLI HOUSING NO.3     18671   \n",
      "9    BHAGATSINGH NAGAR NO.1       28860  BHAGATSINGH NAGAR NO.3     34127   \n",
      "\n",
      "                                      DELAYED_REASON  \\\n",
      "0                                                      \n",
      "1                                                      \n",
      "2                                                      \n",
      "3  1)FAILED TO OPEN JAWAHAR NAGAR ROAD NO.12 S/S ...   \n",
      "4  1)FAILED TO CLOSE GOREGAON SHOPPING CENTRE S/S...   \n",
      "5  1)AAREY NO.2 S/S (DMS) WENT OFFLINE AFTER GIVI...   \n",
      "6  1) NON DMS SUBSTATION ARE IN ARREY AREA. DMS U...   \n",
      "7  1)TUREL PAKHADI NO.1 S/S (DMS) WENT OFFLINE AF...   \n",
      "8       1) DMS OFFLINE AT KANDIVLI HOUSING NO.3 S/S.   \n",
      "9  1)FAILED TO OPEN BANGUR NAGAR NO.3 S/S DMS SW....   \n",
      "\n",
      "                                        FAULT_NATURE  \n",
      "0                                                     \n",
      "1                                                     \n",
      "2                                                     \n",
      "3  FAILED TO OPEN JAWAHAR NAGAR ROAD NO; FAILED T...  \n",
      "4  FAILED TO CLOSE GOREGAON SHOPPING CENTRE S/S D...  \n",
      "5                  WENT OFFLINE AFTER GIVING COMMAND  \n",
      "6                                                     \n",
      "7  FAILED TO OPEN AJIT PARK S/S DMS SW; WENT OFFL...  \n",
      "8                 DMS OFFLINE AT KANDIVLI HOUSING NO  \n",
      "9                     FAILED TO OPEN BANGUR NAGAR NO  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Load the file\n",
    "df = pd.read_csv('/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/TXN_NMS_HTLOGSHEET_FREE_REMARKS_ONLY.csv', usecols=['FREE_REMARKS'], low_memory=False)\n",
    "\n",
    "# 2. UPPERCASE remarks for consistency\n",
    "df['FREE_REMARKS_UPPER'] = df['FREE_REMARKS'].astype(str).str.upper()\n",
    "\n",
    "# 3. Extraction functions\n",
    "def extract_size(text):\n",
    "    m = re.search(r'X\\s*([\\d\\.\\+\\-\\s]*SQ\\.?\\s*MM)', text)\n",
    "    return m.group(1).replace(\" \", \"\") if m else \"\"\n",
    "\n",
    "def extract_insulation(text):\n",
    "    m = re.search(r'(PILC\\+XLPE|XLPE\\+PILC|PILC|XLPE)', text)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def extract_voltage(text):\n",
    "    m = re.search(r'(\\d{2,3})\\s*KV', text)\n",
    "    return f\"{m.group(1)}KV\" if m else \"\"\n",
    "\n",
    "def extract_type(text):\n",
    "    m = re.search(r'(HTCF SECTION|LT SECTION|HT SECTION)', text)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def extract_section(text):\n",
    "    m = re.search(r'BETWEEN\\s+(.+?)\\s+S/S\\s+SWITCH:([0-9]+)\\s+TO\\s+(.+?)\\s+S/S\\s+SWITCH:([0-9]+)', text)\n",
    "    if m:\n",
    "        return pd.Series([m.group(1).strip(), m.group(2), m.group(3).strip(), m.group(4)])\n",
    "    return pd.Series([\"\", \"\", \"\", \"\"])\n",
    "\n",
    "def extract_delayed_reason(text):\n",
    "    m = re.search(r'DELAYED DUE TO(.*?)(?:NOTIFICATION NO|$)', text)\n",
    "    return m.group(1).strip() if m else \"\"\n",
    "\n",
    "def extract_fault_nature(text):\n",
    "    patterns = [\n",
    "        'CABLE DAMAGED BY', 'DMS OFFLINE AT', 'FPI MALFUNCTION',\n",
    "        'FAILED TO OPEN', 'FAILED TO CLOSE', 'WENT OFFLINE',\n",
    "        'FEEDER TRIPPING', 'TRAFFIC ON', 'SUPPLY RESTORATION DELAYED'\n",
    "    ]\n",
    "    found = []\n",
    "    for pat in patterns:\n",
    "        for m in re.finditer(r'({0}.*?)(?:\\.|,|;|NOTIFICATION NO|$)'.format(re.escape(pat)), text):\n",
    "            found.append(m.group(1).strip())\n",
    "    return '; '.join(found) if found else \"\"\n",
    "\n",
    "# 4. Apply extraction functions\n",
    "out = pd.DataFrame()\n",
    "out['Size'] = df['FREE_REMARKS_UPPER'].apply(extract_size)\n",
    "out['Insulation'] = df['FREE_REMARKS_UPPER'].apply(extract_insulation)\n",
    "out['Voltage'] = df['FREE_REMARKS_UPPER'].apply(extract_voltage)\n",
    "out['Type'] = df['FREE_REMARKS_UPPER'].apply(extract_type)\n",
    "\n",
    "section_cols = df['FREE_REMARKS_UPPER'].apply(extract_section)\n",
    "section_cols.columns = ['FROM', 'FROM_SWITCH', 'TO', 'TO_SWITCH']\n",
    "out = pd.concat([out, section_cols], axis=1)\n",
    "\n",
    "out['DELAYED_REASON'] = df['FREE_REMARKS_UPPER'].apply(extract_delayed_reason)\n",
    "out['FAULT_NATURE'] = df['FREE_REMARKS_UPPER'].apply(extract_fault_nature)\n",
    "\n",
    "# 5. Save results\n",
    "out.to_csv('/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/processed_fault_data_ONLY_FREEREMARKS.csv', index=False)\n",
    "print(\"Extracted file written.\")\n",
    "print(out.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37b0aee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done extracting FREE_REMARKS column!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "input_file = '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/TXN_NMS_HTLOGSHEET.csv'\n",
    "output_file = '/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/TXN_NMS_HTLOGSHEET_FREE_REMARKS_ONLY.csv'\n",
    "\n",
    "with open(input_file, encoding='utf-8', errors='ignore') as f, \\\n",
    "     open(output_file, 'w', encoding='utf-8', newline='') as fout:\n",
    "    reader = csv.reader(f)\n",
    "    writer = csv.writer(fout)\n",
    "    header = next(reader)\n",
    "    idx = header.index('FREE_REMARKS')\n",
    "    writer.writerow(['FREE_REMARKS'])\n",
    "    for row in reader:\n",
    "        # If row is too short, skip\n",
    "        if len(row) <= idx:\n",
    "            continue\n",
    "        # If row is too long, join back everything after 'FREE_REMARKS' index\n",
    "        remarks = ','.join(row[idx:])\n",
    "        writer.writerow([remarks])\n",
    "print('Done extracting FREE_REMARKS column!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f8be676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done extracting FREE_REMARKS column!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "input_file = '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/TXN_NMS_HTLOGSHEET.csv'\n",
    "output_file = '/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/TXN_NMS_HTLOGSHEET_FREE_REMARKS_ONLY_clean.csv'\n",
    "\n",
    "with open(input_file, encoding='utf-8', errors='ignore') as f, \\\n",
    "     open(output_file, 'w', encoding='utf-8', newline='') as fout:\n",
    "    lines = f.readlines()\n",
    "    header = lines[0]\n",
    "    header_fields = next(csv.reader([header]))\n",
    "    idx = header_fields.index('FREE_REMARKS')\n",
    "\n",
    "    writer = csv.writer(fout)\n",
    "    writer.writerow(['FREE_REMARKS'])\n",
    "\n",
    "    for line in lines[1:]:\n",
    "        # Try CSV split first\n",
    "        fields = next(csv.reader([line]))\n",
    "        if len(fields) > idx:\n",
    "            # Extract the field, but also join if too many fields\n",
    "            remarks = fields[idx]\n",
    "            # If row is too long, join the rest (likely misalignment)\n",
    "            if len(fields) > len(header_fields):\n",
    "                remarks = ','.join(fields[idx:len(fields)-(len(fields)-len(header_fields))])\n",
    "            elif len(fields) > idx + 1:\n",
    "                # Conservative: join all after idx\n",
    "                remarks = ','.join(fields[idx:])\n",
    "            writer.writerow([remarks])\n",
    "        else:\n",
    "            # Fallback: take the biggest text-y chunk\n",
    "            import re\n",
    "            matches = re.findall(r'([A-Za-z0-9].{10,})', line)\n",
    "            guess = max(matches, key=len) if matches else ''\n",
    "            writer.writerow([guess])\n",
    "\n",
    "print('Done extracting FREE_REMARKS column!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d55a5b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Clean remarks written to: /media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/TXN_NMS_HTLOGSHEET_FREE_REMARKS_ONLY_fixed.csv\n"
     ]
    }
   ],
   "source": [
    "import csv, re, pathlib\n",
    "\n",
    "src  = pathlib.Path(\n",
    "    \"/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/\"\n",
    "    \"TXN_NMS_HTLOGSHEET_FREE_REMARKS_ONLY.csv\"\n",
    ")\n",
    "dst  = src.with_name(src.stem + \"_fixed.csv\")\n",
    "\n",
    "def keep_from_first_real_token(raw: str) -> str:\n",
    "    \"\"\"\n",
    "    Drop leading junk like ',0,14,,,,' and return the actual sentence.\n",
    "    Strategy:\n",
    "      1. Split on ','               (they’re cheap now – only in this row)\n",
    "      2. Keep dropping tokens until one *looks* like text,\n",
    "         i.e. contains at least one alphabetic char **and**\n",
    "         either 'KV', 'SQ', 'C X', 'C X', or 'REC-STN'.\n",
    "    \"\"\"\n",
    "    # Fast path – many rows already clean\n",
    "    if raw.lstrip().startswith((\"1)\", \"C\", \"2\", \"3\", \"4\", \"5\")):\n",
    "        return raw.lstrip(\", \")\n",
    "\n",
    "    tokens = raw.split(\",\")\n",
    "    pat    = re.compile(r\"[A-Z]\", re.I)\n",
    "    key    = (\"KV\", \"SQ\", \"C X\", \"C X\", \"REC-STN\", \"CABLE\", \"TRIPPED\")\n",
    "\n",
    "    while tokens:\n",
    "        if pat.search(tokens[0]) and any(k in tokens[0].upper() for k in key):\n",
    "            break\n",
    "        tokens.pop(0)\n",
    "\n",
    "    return \",\".join(tokens).lstrip(\" ,\")\n",
    "\n",
    "with src.open(encoding=\"utf-8\", errors=\"ignore\") as f_in, \\\n",
    "     dst.open(\"w\", newline=\"\", encoding=\"utf-8\") as f_out:\n",
    "\n",
    "    reader = csv.reader(f_in)\n",
    "    writer = csv.writer(f_out, quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    header = next(reader)            # ['FREE_REMARKS']\n",
    "    writer.writerow(header)          # keep same header\n",
    "\n",
    "    for row in reader:\n",
    "        raw = row[0] if row else \"\"\n",
    "        writer.writerow([keep_from_first_real_token(raw)])\n",
    "\n",
    "print(f\"✅  Clean remarks written to: {dst}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad937e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Parsed 24,228 records  →  /media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/processed_fault_data_ONLY_FREEREMARKS.csv\n",
      "|    | Size                      | Insulation   | Voltage   | Type         | FROM                     |   FROM_SWITCH | TO                     |   TO_SWITCH | DELAYED_REASON                                                                                                                                                                                                                                                                                                                                                                    | FAULT_NATURE   |\n",
      "|---:|:--------------------------|:-------------|:----------|:-------------|:-------------------------|--------------:|:-----------------------|------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------|\n",
      "|  0 | SQ.MM                     | PILC+XLPE    | 11KV      | HTCF SECTION | VISHWESHWAR NAGAR HETALI |         40478 | SAMANT                 |        5587 |                                                                                                                                                                                                                                                                                                                                                                                   |                |\n",
      "|  1 | .06+70+120SQ.MM           | PILC+XLPE    | 11KV      | HTCF SECTION | JAWAHAR NAGAR NO.3       |         13847 | SONAVALA ESTATE NO.1   |         714 | 1)FAILED TO OPEN JAWAHAR NAGAR ROAD NO.12 S/S DMS SW.NO.25658 FAILED TO OPERATE FROM SCADA. 2)FAILED TO OPEN JAWAHAR NAGAR ROAD NO.12 S/S DMS SW.NO.25657 FAILED TO OPERATE FROM SCADA. 3)KASTURI KUNJ S/S (DMS) WENT OFFLINE AFTER TRIPPING.                                                                                                                                     |                |\n",
      "|  2 | 0.15+185+225+300+240SQ.MM | PILC+XLPE    | 11KV      | HTCF SECTION | nan                      |           nan | nan                    |         nan | 1)FAILED TO CLOSE GOREGAON SHOPPING CENTRE S/S DMS SW.NO.04673 FAILED TO OPERATE FROM SCADA.                                                                                                                                                                                                                                                                                      |                |\n",
      "|  3 | 120SQ.MM                  | PILC         | 11KV      | HTCF SECTION | AAREY NO.2               |         18066 | AAREY UNIT NO.7 KIOSK  |       18008 | 1)AAREY NO.2 S/S (DMS) WENT OFFLINE AFTER GIVING COMMAND. 2)FPI FAULTY AT AAREY UNIT NO.7 KIOSK S/S..                                                                                                                                                                                                                                                                             |                |\n",
      "|  4 | 120SQ.MM                  | PILC         | 11KV      | HTCF SECTION | AAREY SANKRAMAN STUDIO   |         19112 | AAREY UNIT NO.2        |       19710 | 1) NON DMS SUBSTATION ARE IN ARREY AREA. DMS USED FOR ISOLATION AND RESTORATION.                                                                                                                                                                                                                                                                                                  |                |\n",
      "|  5 | 120SQ.MM                  | PILC         | 11KV      | HTCF SECTION | AJIT PARK                |         15725 | TUREL PAKHADI NO.1     |        6900 | 1)TUREL PAKHADI NO.1 S/S (DMS) WENT OFFLINE AFTER TRIPPING. 2)LIBERTY GARDEN SOUTH NO.2 S/S (DMS) WENT OFFLINE AFTER TRIPPING. 3) FEEDER TRIPPING AT PALM COURT REC-STN SW.NO.27081. 4)FAILED TO OPEN AJIT PARK S/S DMS SW.NO.15725 FAILED TO OPERATE FROM SCADA.                                                                                                                 |                |\n",
      "|  6 | 120SQ.MM                  | PILC         | 11KV      | HTCF SECTION | BABREKAR NAGAR NO.1      |         18669 | KANDIVLI HOUSING NO.3  |       18671 | 1) DMS OFFLINE AT KANDIVLI HOUSING NO.3 S/S.                                                                                                                                                                                                                                                                                                                                      |                |\n",
      "|  7 | 120SQ.MM                  | PILC         | 11KV      | HTCF SECTION | BHAGATSINGH NAGAR NO.1   |         28860 | BHAGATSINGH NAGAR NO.3 |       34127 | 1)FAILED TO OPEN BANGUR NAGAR NO.3 S/S DMS SW.NO.31181 FAILED TO OPERATE FROM SCADA. DMS OPERATED AFTER GIVING MULTIPLE COMMANDS.                                                                                                                                                                                                                                                 |                |\n",
      "|  8 | 120SQ.MM                  | PILC         | 11KV      | HTCF SECTION | DINDOSHI VASAHAT CENTRAL |         17141 | DINDOSHI VASAHAT SOUTH |       14101 |                                                                                                                                                                                                                                                                                                                                                                                   |                |\n",
      "|  9 | 120SQ.MM                  | PILC         | 11KV      | HTCF SECTION | GOKULDHAM NO.2           |          7636 | GOKULDHAM NO.1         |        6877 | 1) FPI MALFUNCTION AT GOKULDHAM NO.1 S/S SW.NO.06877 (GLOWN). 2) FPI MALFUNCTION AT GOKULDHAM NO.1 S/S SW.NO.06878 (GLOWN). 3)FAILED TO OPEN GOKULDHAM NO.1 S/S DMS SW.NO.06877 FAILED TO OPERATE FROM SCADA. 4)FAILED TO OPEN GOKULDHAM NO.1 S/S DMS SW.NO.06878 FAILED TO OPERATE FROM SCADA. 5)FAILED TO OPEN GOKULDHAM NO.1 S/S DMS SW.NO.06892 FAILED TO OPERATE FROM SCADA. |                |\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -----------------------------------------------------------\n",
    "#  Pull structured fields out of HT-cable FREE_REMARKS strings\n",
    "# -----------------------------------------------------------\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# ─── Edit these two paths ───────────────────────────────────\n",
    "SRC  = r'/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/Book1.csv'   # has FREE_REMARKS column\n",
    "DEST = r'/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/processed_fault_data_ONLY_FREEREMARKS.csv'\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1) Load and normalise ----------------------------------------------------------\n",
    "df_txt = (\n",
    "    pd.read_csv(SRC, usecols=['FREE_REMARKS'], low_memory=False)\n",
    "      .fillna('')\n",
    "      .assign(TXT=lambda d: d['FREE_REMARKS'].str.upper())\n",
    ")\n",
    "\n",
    "# 2) Pre-compile the regexes -----------------------------------------------------\n",
    "RE_SIZE        = re.compile(r'\\bX\\s*([\\d+\\-.\\s]*SQ\\.?\\s*MM)')            # 120SQ.MM  0.15+185+240SQ.MM\n",
    "RE_INSULATION  = re.compile(r'(PILC\\+XLPE|XLPE\\+PILC|PILC|XLPE)')\n",
    "RE_VOLTAGE     = re.compile(r'(\\d{2,3})\\s*KV')\n",
    "RE_TYPE        = re.compile(r'(HTCF SECTION|LT SECTION|HT SECTION)')\n",
    "RE_SECTION     = re.compile(\n",
    "    r'BETWEEN\\s+(.+?)\\s+S/S\\s+SW\\.?ITCH:?\\s*([0-9]+)\\s+TO\\s+(.+?)\\s+S/S\\s+SW\\.?ITCH:?\\s*([0-9]+)',\n",
    "    flags=re.I\n",
    ")\n",
    "RE_DELAYED     = re.compile(\n",
    "    r'(?:ISOLATION\\s*&?\\s*RESTORATION\\s+)?DELAYED (?:AS|DUE TO)\\s*(.*?)(?:NOTIFICATION NO|$)',\n",
    "    flags=re.I\n",
    ")\n",
    "\n",
    "FAULT_TOKENS = [\n",
    "    'CABLE DAMAGED BY', 'CABLE DAMAGED',     # covers slightly shorter variant\n",
    "    'DMS OFFLINE AT', 'WENT OFFLINE',\n",
    "    'FPI MALFUNCTION', 'FPI FAULTY',\n",
    "    'FAILED TO OPEN',  'FAILED TO CLOSE',\n",
    "    'FEEDER TRIPPING', 'TRAFFIC ON',\n",
    "    'SUPPLY RESTORTION DELAYED', 'SUPPLY RESTORATION DELAYED'\n",
    "]\n",
    "# create one big alternation for speed\n",
    "RE_FAULT = re.compile(\n",
    "    '(' + '|'.join(re.escape(tok) for tok in FAULT_TOKENS) + r'.*?)(?:\\.|,|;|NOTIFICATION NO|$)',\n",
    "    flags=re.I\n",
    ")\n",
    "\n",
    "# 3) Utility ---------------------------------------------------------------------\n",
    "def _first(regex, text, fmt=lambda m: m.group(1)):\n",
    "    m = regex.search(text)\n",
    "    return fmt(m) if m else ''\n",
    "\n",
    "def _faults(text: str) -> str:\n",
    "    return '; '.join(m.group(1).strip() for m in RE_FAULT.finditer(text))\n",
    "\n",
    "# 4) Column extraction -----------------------------------------------------------\n",
    "out = pd.DataFrame({\n",
    "    'Size'       : df_txt['TXT'].apply(lambda t: _first(RE_SIZE, t, lambda m: m.group(1).replace(' ', ''))),\n",
    "    'Insulation' : df_txt['TXT'].apply(lambda t: _first(RE_INSULATION, t)),\n",
    "    'Voltage'    : df_txt['TXT'].apply(lambda t: _first(RE_VOLTAGE, t, lambda m: f\"{m.group(1)}KV\")),\n",
    "    'Type'       : df_txt['TXT'].apply(lambda t: _first(RE_TYPE, t))\n",
    "})\n",
    "\n",
    "# --- FROM / TO / SWITCH columns -------------------------------------------------\n",
    "sec = df_txt['TXT'].str.extract(RE_SECTION)\n",
    "sec.columns = ['FROM', 'FROM_SWITCH', 'TO', 'TO_SWITCH']\n",
    "out = pd.concat([out, sec], axis=1)\n",
    "\n",
    "# --- delay / fault --------------------------------------------------------------\n",
    "out['DELAYED_REASON'] = df_txt['TXT'].apply(lambda t: _first(RE_DELAYED, t).strip())\n",
    "out['FAULT_NATURE']   = df_txt['TXT'].apply(_faults)\n",
    "\n",
    "# 5) Final tidy-up & write -------------------------------------------------------\n",
    "out = out[['Size','Insulation','Voltage','Type',\n",
    "           'FROM','FROM_SWITCH','TO','TO_SWITCH',\n",
    "           'DELAYED_REASON','FAULT_NATURE']]\n",
    "\n",
    "out.to_csv(DEST, index=False)\n",
    "print(f'✓ Parsed {len(out):,} records  →  {DEST}')\n",
    "print(out.head(10).to_markdown())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e7390b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
