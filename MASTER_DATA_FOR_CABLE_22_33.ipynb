{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83880e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cable_ID</th>\n",
       "      <th>Voltage_Level</th>\n",
       "      <th>Feeder_ID</th>\n",
       "      <th>Sub_Feeder_ID</th>\n",
       "      <th>From_Switch</th>\n",
       "      <th>To_Switch</th>\n",
       "      <th>Cable_Type</th>\n",
       "      <th>Cable_Age_Years</th>\n",
       "      <th>Length_m</th>\n",
       "      <th>Installation_Environment</th>\n",
       "      <th>...</th>\n",
       "      <th>Partial_Discharge_Frequency</th>\n",
       "      <th>Partial_Discharge_Intensity</th>\n",
       "      <th>Thermal_History_Excursions</th>\n",
       "      <th>Num_Faults</th>\n",
       "      <th>Fault_Type</th>\n",
       "      <th>Repairs_Count</th>\n",
       "      <th>Joint_History</th>\n",
       "      <th>Corrosivity</th>\n",
       "      <th>Water_Ingress</th>\n",
       "      <th>Remarks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33KV-SW1-SW2</td>\n",
       "      <td>33kV</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>SW1</td>\n",
       "      <td>SW2</td>\n",
       "      <td>XLPE</td>\n",
       "      <td>7</td>\n",
       "      <td>1200</td>\n",
       "      <td>Underground</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>Original</td>\n",
       "      <td>Low</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22KV-SW3-SW4</td>\n",
       "      <td>22kV</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>SW3</td>\n",
       "      <td>SW4</td>\n",
       "      <td>PILC</td>\n",
       "      <td>3</td>\n",
       "      <td>800</td>\n",
       "      <td>Overhead</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Earth Fault</td>\n",
       "      <td>1</td>\n",
       "      <td>Repaired once</td>\n",
       "      <td>Medium</td>\n",
       "      <td>No</td>\n",
       "      <td>Monitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FDR1-DT001</td>\n",
       "      <td>11kV</td>\n",
       "      <td>FDR1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>XLPE</td>\n",
       "      <td>8</td>\n",
       "      <td>400</td>\n",
       "      <td>Underground</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Earth Fault</td>\n",
       "      <td>2</td>\n",
       "      <td>Multiple joints</td>\n",
       "      <td>High</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FDR2-SUB1-DT005</td>\n",
       "      <td>11kV</td>\n",
       "      <td>FDR2</td>\n",
       "      <td>SUB1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>XLPE</td>\n",
       "      <td>4</td>\n",
       "      <td>220</td>\n",
       "      <td>Underground</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>Original</td>\n",
       "      <td>Low</td>\n",
       "      <td>No</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Cable_ID Voltage_Level Feeder_ID Sub_Feeder_ID From_Switch  \\\n",
       "0     33KV-SW1-SW2          33kV                                 SW1   \n",
       "1     22KV-SW3-SW4          22kV                                 SW3   \n",
       "2       FDR1-DT001          11kV      FDR1                             \n",
       "3  FDR2-SUB1-DT005          11kV      FDR2          SUB1               \n",
       "\n",
       "  To_Switch Cable_Type  Cable_Age_Years  Length_m Installation_Environment  \\\n",
       "0       SW2       XLPE                7      1200              Underground   \n",
       "1       SW4       PILC                3       800                 Overhead   \n",
       "2                 XLPE                8       400              Underground   \n",
       "3                 XLPE                4       220              Underground   \n",
       "\n",
       "   ... Partial_Discharge_Frequency Partial_Discharge_Intensity  \\\n",
       "0  ...                           0                           0   \n",
       "1  ...                           0                           0   \n",
       "2  ...                           1                           3   \n",
       "3  ...                           0                           0   \n",
       "\n",
       "  Thermal_History_Excursions  Num_Faults   Fault_Type  Repairs_Count  \\\n",
       "0                          0           0         None              0   \n",
       "1                          1           1  Earth Fault              1   \n",
       "2                          1           2  Earth Fault              2   \n",
       "3                          0           0         None              0   \n",
       "\n",
       "     Joint_History  Corrosivity  Water_Ingress  Remarks  \n",
       "0         Original          Low             No  Healthy  \n",
       "1    Repaired once       Medium             No  Monitor  \n",
       "2  Multiple joints         High            Yes  Healthy  \n",
       "3         Original          Low             No           \n",
       "\n",
       "[4 rows x 29 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = [\n",
    "    \"Cable_ID\",\"Voltage_Level\",\"Feeder_ID\",\"Sub_Feeder_ID\",\"From_Switch\",\"To_Switch\",\n",
    "    \"Cable_Type\",\"Cable_Age_Years\",\"Length_m\",\"Installation_Environment\",\n",
    "    \"Soil_Type\",\"Humidity\",\"Proximity_to_Water\",\"Load_History_Avg_Load\",\"Load_History_Peak_Load\", \n",
    "    \"Loading_Cycles\",\"Overload_Events\",\"IR_Measurement_MOhm\",\"Tan_Delta\",\"Partial_Discharge_Frequency\",\n",
    "    \"Partial_Discharge_Intensity\",\"Thermal_History_Excursions\",\"Num_Faults\",\"Fault_Type\",\"Repairs_Count\",\n",
    "    \"Joint_History\",\"Corrosivity\",\"Water_Ingress\",\"Remarks\"\n",
    "]\n",
    "\n",
    "# Sample data for different voltage levels. You can add or import your actual data here.\n",
    "data = [\n",
    "    [\n",
    "        \"33KV-SW1-SW2\", \"33kV\", \"\", \"\", \"SW1\", \"SW2\", \"XLPE\", 7, 1200, \"Underground\",\n",
    "        \"Sandy\", \"Medium\", \"Far\", 60, 100, 150, 1, 500, 0.001, 0, 0, 0, 0, \"None\", 0, \"Original\", \"Low\", \"No\", \"Healthy\"\n",
    "    ],\n",
    "    [\n",
    "        \"22KV-SW3-SW4\", \"22kV\", \"\", \"\", \"SW3\", \"SW4\", \"PILC\", 3, 800, \"Overhead\",\n",
    "        \"Clay\", \"High\", \"Near\", 50, 90, 100, 0, 400, 0.002, 0, 0, 1, 1, \"Earth Fault\", 1, \"Repaired once\", \"Medium\", \"No\", \"Monitor\"\n",
    "    ],\n",
    "    [\n",
    "        \"FDR1-DT001\", \"11kV\", \"FDR1\", \"\", \"\", \"\", \"XLPE\", 8, 400, \"Underground\",\n",
    "        \"Loam\", \"Medium\", \"Near\", 40, 80, 120, 2, 120, 0.003, 1, 3, 1, 2, \"Earth Fault\", 2, \"Multiple joints\", \"High\", \"Yes\", \"Healthy\"\n",
    "    ],\n",
    "    [\n",
    "        \"FDR2-SUB1-DT005\", \"11kV\", \"FDR2\", \"SUB1\", \"\", \"\", \"XLPE\", 4, 220, \"Underground\",\n",
    "        \"Rocky\", \"Low\", \"Far\", 30, 50, 90, 0, 200, 0.001, 0, 0, 0, 0, \"None\", 0, \"Original\", \"Low\", \"No\", \"\"\n",
    "    ],\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Save as Excel and CSV for use\n",
    "# df.to_excel(\"/media/sagarkumar/New Volume/SAGAR/DATA_GENERATION/master_cable_data_final.xlsx\", index=False)\n",
    "df.to_csv(\"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/master_cable_data_final.csv\", index=False)\n",
    "\n",
    "df.head()  # Display the first few rows of the DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a71682e",
   "metadata": {},
   "source": [
    "FOR 22 AND 33 MASTER DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72a5aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_22_33 = pd.read_csv(\"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/monthly_SWNO_matrix_22KV_33KV.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f491c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = df_22_33['SWNO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "979d3ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_df = pd.DataFrame(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ba358c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_df.to_csv(\"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7bb30b",
   "metadata": {},
   "source": [
    "ENERGYAUDIT VS SWNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8d5f49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv...\n",
      "  - Found 343 unique, clean values.\n",
      "Processing file: /media/sagarkumar/New Volume/SAGAR/2-year-data/ENERGYAUDIT.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_227956/1560782353.py:50: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath, usecols=[actual_col_name], on_bad_lines='skip', encoding=encoding) # <--- CHANGED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Found 9954 unique, clean values.\n",
      "\n",
      "--- Comparison Report ---\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv' (Column: SWNO): 343\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/2-year-data/ENERGYAUDIT.csv' (Column: SWITCH_NO): 9954\n",
      "-------------------------\n",
      "Number of values that matched: 0\n",
      "-------------------------\n",
      "No matching values were found to save.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION: PLEASE EDIT THIS SECTION ---\n",
    "\n",
    "# File 1 Details\n",
    "file1_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv\"\n",
    "file1_column_name = \"SWNO\"\n",
    "# This file processed correctly before, so 'utf-8' is likely correct.\n",
    "file1_encoding = 'utf-8'  # <--- ADDED\n",
    "\n",
    "# File 2 Details\n",
    "file2_path = \"/media/sagarkumar/New Volume/SAGAR/2-year-data/ENERGYAUDIT.csv\"\n",
    "file2_column_name = \"SWITCH_NO\"\n",
    "# This is the file that had an error. 'latin1' is a safe choice.\n",
    "file2_encoding = 'utf-8' # <--- ADDED ('windows-1252' is also a good option)\n",
    "\n",
    "# Output File Path\n",
    "# This file will contain only the values that exist in BOTH files.\n",
    "# Changed to a simpler path. It will save in the same directory you run the script.\n",
    "output_file_path = \"matching_values.csv\" # <--- CHANGED for simplicity\n",
    "\n",
    "\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "\n",
    "def find_actual_column_name(columns, target_name):\n",
    "    \"\"\"Helper function to find a column name, ignoring case.\"\"\"\n",
    "    for col in columns:\n",
    "        if str(col).lower() == str(target_name).lower():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def get_unique_values_from_file(filepath, column_name, encoding): # <--- CHANGED (added encoding)\n",
    "    \"\"\"\n",
    "    Reads a file with a specific encoding, extracts unique values from a\n",
    "    column, cleans them, and returns them as a set.\n",
    "    \"\"\"\n",
    "    print(f\"Processing file: {filepath}...\")\n",
    "    try:\n",
    "        # Read just the header to find the correct column name (case-insensitive)\n",
    "        # Pass the encoding parameter here\n",
    "        header_df = pd.read_csv(filepath, nrows=0, on_bad_lines='skip', encoding=encoding) # <--- CHANGED\n",
    "        actual_col_name = find_actual_column_name(header_df.columns, column_name)\n",
    "\n",
    "        if not actual_col_name:\n",
    "            print(f\"  - Error: Column '{column_name}' not found. Please check the column name.\")\n",
    "            return set()\n",
    "\n",
    "        # Read the full column using the correct name and encoding\n",
    "        df = pd.read_csv(filepath, usecols=[actual_col_name], on_bad_lines='skip', encoding=encoding) # <--- CHANGED\n",
    "\n",
    "        # --- Data Cleaning ---\n",
    "        # Convert all values to string, extract digits, and then convert to numbers.\n",
    "        # This handles mixed data types (e.g., '123' vs 123) and text prefixes (e.g., 'SW-123').\n",
    "        s = pd.Series(df[actual_col_name].dropna().unique(), dtype=str)\n",
    "        s = s.str.extract('(\\d+)').iloc[:, 0]\n",
    "        s = pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "        cleaned_values = set(s.dropna().astype(int))\n",
    "\n",
    "        print(f\"  - Found {len(cleaned_values)} unique, clean values.\")\n",
    "        return cleaned_values\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  - Error: File not found. Please check the path: {filepath}\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"  - An unexpected error occurred: {e}\")\n",
    "        return set()\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# Get the unique values from both files, passing the encoding for each\n",
    "unique_values_from_file1 = get_unique_values_from_file(file1_path, file1_column_name, file1_encoding) # <--- CHANGED\n",
    "unique_values_from_file2 = get_unique_values_from_file(file2_path, file2_column_name, file2_encoding) # <--- CHANGED\n",
    "\n",
    "# --- Comparison and Reporting ---\n",
    "\n",
    "print(\"\\n--- Comparison Report ---\")\n",
    "if not unique_values_from_file1 or not unique_values_from_file2:\n",
    "    print(\"Could not perform comparison because one of the files could not be processed or contained no valid data.\")\n",
    "else:\n",
    "    # Use set intersection to find the values that exist in both sets\n",
    "    matching_values = unique_values_from_file1.intersection(unique_values_from_file2)\n",
    "\n",
    "    # Print the final report\n",
    "    print(f\"Unique values in '{file1_path}' (Column: {file1_column_name}): {len(unique_values_from_file1)}\")\n",
    "    print(f\"Unique values in '{file2_path}' (Column: {file2_column_name}): {len(unique_values_from_file2)}\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"Number of values that matched: {len(matching_values)}\")\n",
    "    print(\"-\" * 25)\n",
    "\n",
    "    # --- Save the results to the output file --- # <--- ADDED SECTION\n",
    "    if matching_values:\n",
    "        # Convert the set of matching values to a DataFrame\n",
    "        matching_df = pd.DataFrame(sorted(list(matching_values)), columns=['Matching_Switch_Numbers'])\n",
    "        # Save the DataFrame to a CSV file\n",
    "      \n",
    "        print(f\"Success! Matching values have been saved to: {output_file_path}\")\n",
    "    else:\n",
    "        print(\"No matching values were found to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54d5df0",
   "metadata": {},
   "source": [
    "FEEDERDETAILS VS SWNO BUT WRONG MATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b23f6a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv...\n",
      "  - Found 343 unique, clean values.\n",
      "Processing file: /media/sagarkumar/New Volume/SAGAR/2-year-data/FEEDERDETAILS.csv...\n",
      "  - Found 69702 unique, clean values.\n",
      "\n",
      "--- Comparison Report ---\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv' (Column: SWNO): 343\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/2-year-data/FEEDERDETAILS.csv' (Column: ID): 69702\n",
      "-------------------------\n",
      "Number of values that matched: 326\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION: PLEASE EDIT THIS SECTION ---\n",
    "\n",
    "# File 1 Details\n",
    "file1_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv\"\n",
    "file1_column_name = \"SWNO\" \n",
    "\n",
    "# File 2 Details\n",
    "file2_path = \"/media/sagarkumar/New Volume/SAGAR/2-year-data/FEEDERDETAILS.csv\"\n",
    "\n",
    "file2_column_name = \"ID\" \n",
    "\n",
    "# Output File Path\n",
    "# This file will contain only the values that exist in BOTH files.\n",
    "output_file_path = \"/path/to/your/output/matching_values.csv\"\n",
    "\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "\n",
    "def find_actual_column_name(columns, target_name):\n",
    "    \"\"\"Helper function to find a column name, ignoring case.\"\"\"\n",
    "    for col in columns:\n",
    "        if str(col).lower() == str(target_name).lower():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def get_unique_values_from_file(filepath, column_name):\n",
    "    \"\"\"\n",
    "    Reads a file, extracts unique values from a specific column,\n",
    "    cleans them, and returns them as a set.\n",
    "    \"\"\"\n",
    "    print(f\"Processing file: {filepath}...\")\n",
    "    try:\n",
    "        # Read just the header to find the correct column name (case-insensitive)\n",
    "        header_df = pd.read_csv(filepath, nrows=0, on_bad_lines='skip')\n",
    "        actual_col_name = find_actual_column_name(header_df.columns, column_name)\n",
    "\n",
    "        if not actual_col_name:\n",
    "            print(f\"  - Error: Column '{column_name}' not found. Please check the column name.\")\n",
    "            return set()\n",
    "\n",
    "        # Read the full column using the correct name\n",
    "        df = pd.read_csv(filepath, usecols=[actual_col_name], on_bad_lines='skip')\n",
    "        \n",
    "        # --- Data Cleaning ---\n",
    "        # Convert all values to string, extract digits, and then convert to numbers.\n",
    "        # This handles mixed data types (e.g., '123' vs 123) and text prefixes (e.g., 'SW-123').\n",
    "        s = pd.Series(df[actual_col_name].dropna().unique(), dtype=str)\n",
    "        s = s.str.extract('(\\d+)').iloc[:, 0]\n",
    "        s = pd.to_numeric(s, errors='coerce')\n",
    "        \n",
    "        cleaned_values = set(s.dropna().astype(int))\n",
    "        \n",
    "        print(f\"  - Found {len(cleaned_values)} unique, clean values.\")\n",
    "        return cleaned_values\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  - Error: File not found. Please check the path: {filepath}\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"  - An unexpected error occurred: {e}\")\n",
    "        return set()\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# Get the unique values from both files\n",
    "unique_values_from_file1 = get_unique_values_from_file(file1_path, file1_column_name)\n",
    "unique_values_from_file2 = get_unique_values_from_file(file2_path, file2_column_name)\n",
    "\n",
    "# --- Comparison and Reporting ---\n",
    "\n",
    "print(\"\\n--- Comparison Report ---\")\n",
    "if not unique_values_from_file1 or not unique_values_from_file2:\n",
    "    print(\"Could not perform comparison because one of the files could not be processed or contained no valid data.\")\n",
    "else:\n",
    "    # Use set intersection to find the values that exist in both sets\n",
    "    matching_values = unique_values_from_file1.intersection(unique_values_from_file2)\n",
    "    \n",
    "    # Print the final report\n",
    "    print(f\"Unique values in '{file1_path}' (Column: {file1_column_name}): {len(unique_values_from_file1)}\")\n",
    "    print(f\"Unique values in '{file2_path}' (Column: {file2_column_name}): {len(unique_values_from_file2)}\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"Number of values that matched: {len(matching_values)}\")\n",
    "    print(\"-\" * 25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d658d7",
   "metadata": {},
   "source": [
    "SWNO VS HTCABLE CLEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56b34ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv...\n",
      "  - Found 343 unique, clean values.\n",
      "Processing file: /media/sagarkumar/New Volume/SAGAR/2-year-data/HTCABLE_Clean.csv...\n",
      "  - Found 10244 unique, clean values.\n",
      "\n",
      "--- Comparison Report ---\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv' (Column: SWNO): 343\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/2-year-data/HTCABLE_Clean.csv' (Column: DESTINATION_SWITCH_ID): 10244\n",
      "-------------------------\n",
      "Number of values that matched: 258\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION: PLEASE EDIT THIS SECTION ---\n",
    "\n",
    "# File 1 Details\n",
    "file1_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv\"\n",
    "file1_column_name = \"SWNO\" \n",
    "\n",
    "# File 2 Details\n",
    "file2_path = \"/media/sagarkumar/New Volume/SAGAR/2-year-data/HTCABLE_Clean.csv\"\n",
    "\n",
    "file2_column_name = \"DESTINATION_SWITCH_ID\" \n",
    "\n",
    "# Output File Path\n",
    "# This file will contain only the values that exist in BOTH files.\n",
    "output_file_path = \"/path/to/your/output/matching_values.csv\"\n",
    "\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "\n",
    "def find_actual_column_name(columns, target_name):\n",
    "    \"\"\"Helper function to find a column name, ignoring case.\"\"\"\n",
    "    for col in columns:\n",
    "        if str(col).lower() == str(target_name).lower():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def get_unique_values_from_file(filepath, column_name):\n",
    "    \"\"\"\n",
    "    Reads a file, extracts unique values from a specific column,\n",
    "    cleans them, and returns them as a set.\n",
    "    \"\"\"\n",
    "    print(f\"Processing file: {filepath}...\")\n",
    "    try:\n",
    "        # Read just the header to find the correct column name (case-insensitive)\n",
    "        header_df = pd.read_csv(filepath, nrows=0, on_bad_lines='skip')\n",
    "        actual_col_name = find_actual_column_name(header_df.columns, column_name)\n",
    "\n",
    "        if not actual_col_name:\n",
    "            print(f\"  - Error: Column '{column_name}' not found. Please check the column name.\")\n",
    "            return set()\n",
    "\n",
    "        # Read the full column using the correct name\n",
    "        df = pd.read_csv(filepath, usecols=[actual_col_name], on_bad_lines='skip')\n",
    "        \n",
    "        # --- Data Cleaning ---\n",
    "        # Convert all values to string, extract digits, and then convert to numbers.\n",
    "        # This handles mixed data types (e.g., '123' vs 123) and text prefixes (e.g., 'SW-123').\n",
    "        s = pd.Series(df[actual_col_name].dropna().unique(), dtype=str)\n",
    "        s = s.str.extract('(\\d+)').iloc[:, 0]\n",
    "        s = pd.to_numeric(s, errors='coerce')\n",
    "        \n",
    "        cleaned_values = set(s.dropna().astype(int))\n",
    "        \n",
    "        print(f\"  - Found {len(cleaned_values)} unique, clean values.\")\n",
    "        return cleaned_values\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  - Error: File not found. Please check the path: {filepath}\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"  - An unexpected error occurred: {e}\")\n",
    "        return set()\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# Get the unique values from both files\n",
    "unique_values_from_file1 = get_unique_values_from_file(file1_path, file1_column_name)\n",
    "unique_values_from_file2 = get_unique_values_from_file(file2_path, file2_column_name)\n",
    "\n",
    "# --- Comparison and Reporting ---\n",
    "\n",
    "print(\"\\n--- Comparison Report ---\")\n",
    "if not unique_values_from_file1 or not unique_values_from_file2:\n",
    "    print(\"Could not perform comparison because one of the files could not be processed or contained no valid data.\")\n",
    "else:\n",
    "    # Use set intersection to find the values that exist in both sets\n",
    "    matching_values = unique_values_from_file1.intersection(unique_values_from_file2)\n",
    "    \n",
    "    # Print the final report\n",
    "    print(f\"Unique values in '{file1_path}' (Column: {file1_column_name}): {len(unique_values_from_file1)}\")\n",
    "    print(f\"Unique values in '{file2_path}' (Column: {file2_column_name}): {len(unique_values_from_file2)}\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"Number of values that matched: {len(matching_values)}\")\n",
    "    print(\"-\" * 25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b74ac9",
   "metadata": {},
   "source": [
    "SWNO VS NETWORKDETAILS BUT TO THE SERIAL NO MAY BE WRONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3a66ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv...\n",
      "  - Found 343 unique, clean values.\n",
      "Processing file: /media/sagarkumar/New Volume/SAGAR/2-year-data/NETWORKDETAILS.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_227956/2537703470.py:44: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath, usecols=[actual_col_name], on_bad_lines='skip')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Found 52927 unique, clean values.\n",
      "\n",
      "--- Comparison Report ---\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv' (Column: SWNO): 343\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/2-year-data/NETWORKDETAILS.csv' (Column: SERIAL_NO): 52927\n",
      "-------------------------\n",
      "Number of values that matched: 250\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION: PLEASE EDIT THIS SECTION ---\n",
    "\n",
    "# File 1 Details\n",
    "file1_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv\"\n",
    "file1_column_name = \"SWNO\" \n",
    "\n",
    "# File 2 Details\n",
    "file2_path = \"/media/sagarkumar/New Volume/SAGAR/2-year-data/NETWORKDETAILS.csv\"\n",
    "\n",
    "file2_column_name = \"SERIAL_NO\" \n",
    "\n",
    "# Output File Path\n",
    "# This file will contain only the values that exist in BOTH files.\n",
    "output_file_path = \"/path/to/your/output/matching_values.csv\"\n",
    "\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "\n",
    "def find_actual_column_name(columns, target_name):\n",
    "    \"\"\"Helper function to find a column name, ignoring case.\"\"\"\n",
    "    for col in columns:\n",
    "        if str(col).lower() == str(target_name).lower():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def get_unique_values_from_file(filepath, column_name):\n",
    "    \"\"\"\n",
    "    Reads a file, extracts unique values from a specific column,\n",
    "    cleans them, and returns them as a set.\n",
    "    \"\"\"\n",
    "    print(f\"Processing file: {filepath}...\")\n",
    "    try:\n",
    "        # Read just the header to find the correct column name (case-insensitive)\n",
    "        header_df = pd.read_csv(filepath, nrows=0, on_bad_lines='skip')\n",
    "        actual_col_name = find_actual_column_name(header_df.columns, column_name)\n",
    "\n",
    "        if not actual_col_name:\n",
    "            print(f\"  - Error: Column '{column_name}' not found. Please check the column name.\")\n",
    "            return set()\n",
    "\n",
    "        # Read the full column using the correct name\n",
    "        df = pd.read_csv(filepath, usecols=[actual_col_name], on_bad_lines='skip')\n",
    "        \n",
    "        # --- Data Cleaning ---\n",
    "        # Convert all values to string, extract digits, and then convert to numbers.\n",
    "        # This handles mixed data types (e.g., '123' vs 123) and text prefixes (e.g., 'SW-123').\n",
    "        s = pd.Series(df[actual_col_name].dropna().unique(), dtype=str)\n",
    "        s = s.str.extract('(\\d+)').iloc[:, 0]\n",
    "        s = pd.to_numeric(s, errors='coerce')\n",
    "        \n",
    "        cleaned_values = set(s.dropna().astype(int))\n",
    "        \n",
    "        print(f\"  - Found {len(cleaned_values)} unique, clean values.\")\n",
    "        return cleaned_values\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  - Error: File not found. Please check the path: {filepath}\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"  - An unexpected error occurred: {e}\")\n",
    "        return set()\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# Get the unique values from both files\n",
    "unique_values_from_file1 = get_unique_values_from_file(file1_path, file1_column_name)\n",
    "unique_values_from_file2 = get_unique_values_from_file(file2_path, file2_column_name)\n",
    "\n",
    "# --- Comparison and Reporting ---\n",
    "\n",
    "print(\"\\n--- Comparison Report ---\")\n",
    "if not unique_values_from_file1 or not unique_values_from_file2:\n",
    "    print(\"Could not perform comparison because one of the files could not be processed or contained no valid data.\")\n",
    "else:\n",
    "    # Use set intersection to find the values that exist in both sets\n",
    "    matching_values = unique_values_from_file1.intersection(unique_values_from_file2)\n",
    "    \n",
    "    # Print the final report\n",
    "    print(f\"Unique values in '{file1_path}' (Column: {file1_column_name}): {len(unique_values_from_file1)}\")\n",
    "    print(f\"Unique values in '{file2_path}' (Column: {file2_column_name}): {len(unique_values_from_file2)}\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"Number of values that matched: {len(matching_values)}\")\n",
    "    print(\"-\" * 25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1381a135",
   "metadata": {},
   "source": [
    "SWNO VS TXN_JOINT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5fc01364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv...\n",
      "  - Found 343 unique, clean values.\n",
      "Processing file: //media/sagarkumar/New Volume/SAGAR/2-year-data/TXN_JOINT.csv...\n",
      "  - Found 6924 unique, clean values.\n",
      "\n",
      "--- Comparison Report ---\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv' (Column: SWNO): 343\n",
      "Unique values in '//media/sagarkumar/New Volume/SAGAR/2-year-data/TXN_JOINT.csv' (Column: DESTINATION_SWITCH_ID): 6924\n",
      "-------------------------\n",
      "Number of values that matched: 233\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION: PLEASE EDIT THIS SECTION ---\n",
    "\n",
    "# File 1 Details\n",
    "file1_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv\"\n",
    "file1_column_name = \"SWNO\" \n",
    "\n",
    "# File 2 Details\n",
    "file2_path = \"//media/sagarkumar/New Volume/SAGAR/2-year-data/TXN_JOINT.csv\"\n",
    "\n",
    "file2_column_name = \"DESTINATION_SWITCH_ID\" \n",
    "\n",
    "# Output File Path\n",
    "# This file will contain only the values that exist in BOTH files.\n",
    "output_file_path = \"/path/to/your/output/matching_values.csv\"\n",
    "\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "\n",
    "def find_actual_column_name(columns, target_name):\n",
    "    \"\"\"Helper function to find a column name, ignoring case.\"\"\"\n",
    "    for col in columns:\n",
    "        if str(col).lower() == str(target_name).lower():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def get_unique_values_from_file(filepath, column_name):\n",
    "    \"\"\"\n",
    "    Reads a file, extracts unique values from a specific column,\n",
    "    cleans them, and returns them as a set.\n",
    "    \"\"\"\n",
    "    print(f\"Processing file: {filepath}...\")\n",
    "    try:\n",
    "        # Read just the header to find the correct column name (case-insensitive)\n",
    "        header_df = pd.read_csv(filepath, nrows=0, on_bad_lines='skip')\n",
    "        actual_col_name = find_actual_column_name(header_df.columns, column_name)\n",
    "\n",
    "        if not actual_col_name:\n",
    "            print(f\"  - Error: Column '{column_name}' not found. Please check the column name.\")\n",
    "            return set()\n",
    "\n",
    "        # Read the full column using the correct name\n",
    "        df = pd.read_csv(filepath, usecols=[actual_col_name], on_bad_lines='skip')\n",
    "        \n",
    "        # --- Data Cleaning ---\n",
    "        # Convert all values to string, extract digits, and then convert to numbers.\n",
    "        # This handles mixed data types (e.g., '123' vs 123) and text prefixes (e.g., 'SW-123').\n",
    "        s = pd.Series(df[actual_col_name].dropna().unique(), dtype=str)\n",
    "        s = s.str.extract('(\\d+)').iloc[:, 0]\n",
    "        s = pd.to_numeric(s, errors='coerce')\n",
    "        \n",
    "        cleaned_values = set(s.dropna().astype(int))\n",
    "        \n",
    "        print(f\"  - Found {len(cleaned_values)} unique, clean values.\")\n",
    "        return cleaned_values\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  - Error: File not found. Please check the path: {filepath}\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"  - An unexpected error occurred: {e}\")\n",
    "        return set()\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# Get the unique values from both files\n",
    "unique_values_from_file1 = get_unique_values_from_file(file1_path, file1_column_name)\n",
    "unique_values_from_file2 = get_unique_values_from_file(file2_path, file2_column_name)\n",
    "\n",
    "# --- Comparison and Reporting ---\n",
    "\n",
    "print(\"\\n--- Comparison Report ---\")\n",
    "if not unique_values_from_file1 or not unique_values_from_file2:\n",
    "    print(\"Could not perform comparison because one of the files could not be processed or contained no valid data.\")\n",
    "else:\n",
    "    # Use set intersection to find the values that exist in both sets\n",
    "    matching_values = unique_values_from_file1.intersection(unique_values_from_file2)\n",
    "    \n",
    "    # Print the final report\n",
    "    print(f\"Unique values in '{file1_path}' (Column: {file1_column_name}): {len(unique_values_from_file1)}\")\n",
    "    print(f\"Unique values in '{file2_path}' (Column: {file2_column_name}): {len(unique_values_from_file2)}\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"Number of values that matched: {len(matching_values)}\")\n",
    "    print(\"-\" * 25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fedb0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00af8748",
   "metadata": {},
   "source": [
    "SWNO VS FAULTDAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c210cdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv...\n",
      "  - Found 343 unique, clean values.\n",
      "Processing file: /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/HT_fault_cable_info_processed2.csv...\n",
      "  - Found 4990 unique, clean values.\n",
      "\n",
      "--- Comparison Report ---\n",
      "Unique values in '/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv' (Column: SWNO): 343\n",
      "Unique values in '/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/HT_fault_cable_info_processed2.csv' (Column: TO_SWITCH): 4990\n",
      "-------------------------\n",
      "Number of values that matched: 196\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION: PLEASE EDIT THIS SECTION ---\n",
    "\n",
    "# File 1 Details\n",
    "file1_path = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv\"\n",
    "file1_column_name = \"SWNO\" \n",
    "\n",
    "# File 2 Details\n",
    "file2_path = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/HT_fault_cable_info_processed2.csv\"\n",
    "\n",
    "file2_column_name = \"TO_SWITCH\" \n",
    "\n",
    "# Output File Path\n",
    "# This file will contain only the values that exist in BOTH files.\n",
    "output_file_path = \"/path/to/your/output/matching_values.csv\"\n",
    "\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "\n",
    "def find_actual_column_name(columns, target_name):\n",
    "    \"\"\"Helper function to find a column name, ignoring case.\"\"\"\n",
    "    for col in columns:\n",
    "        if str(col).lower() == str(target_name).lower():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def get_unique_values_from_file(filepath, column_name):\n",
    "    \"\"\"\n",
    "    Reads a file, extracts unique values from a specific column,\n",
    "    cleans them, and returns them as a set.\n",
    "    \"\"\"\n",
    "    print(f\"Processing file: {filepath}...\")\n",
    "    try:\n",
    "        # Read just the header to find the correct column name (case-insensitive)\n",
    "        header_df = pd.read_csv(filepath, nrows=0, on_bad_lines='skip')\n",
    "        actual_col_name = find_actual_column_name(header_df.columns, column_name)\n",
    "\n",
    "        if not actual_col_name:\n",
    "            print(f\"  - Error: Column '{column_name}' not found. Please check the column name.\")\n",
    "            return set()\n",
    "\n",
    "        # Read the full column using the correct name\n",
    "        df = pd.read_csv(filepath, usecols=[actual_col_name], on_bad_lines='skip')\n",
    "        \n",
    "        # --- Data Cleaning ---\n",
    "        # Convert all values to string, extract digits, and then convert to numbers.\n",
    "        # This handles mixed data types (e.g., '123' vs 123) and text prefixes (e.g., 'SW-123').\n",
    "        s = pd.Series(df[actual_col_name].dropna().unique(), dtype=str)\n",
    "        s = s.str.extract('(\\d+)').iloc[:, 0]\n",
    "        s = pd.to_numeric(s, errors='coerce')\n",
    "        \n",
    "        cleaned_values = set(s.dropna().astype(int))\n",
    "        \n",
    "        print(f\"  - Found {len(cleaned_values)} unique, clean values.\")\n",
    "        return cleaned_values\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  - Error: File not found. Please check the path: {filepath}\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"  - An unexpected error occurred: {e}\")\n",
    "        return set()\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# Get the unique values from both files\n",
    "unique_values_from_file1 = get_unique_values_from_file(file1_path, file1_column_name)\n",
    "unique_values_from_file2 = get_unique_values_from_file(file2_path, file2_column_name)\n",
    "\n",
    "# --- Comparison and Reporting ---\n",
    "\n",
    "print(\"\\n--- Comparison Report ---\")\n",
    "if not unique_values_from_file1 or not unique_values_from_file2:\n",
    "    print(\"Could not perform comparison because one of the files could not be processed or contained no valid data.\")\n",
    "else:\n",
    "    # Use set intersection to find the values that exist in both sets\n",
    "    matching_values = unique_values_from_file1.intersection(unique_values_from_file2)\n",
    "    \n",
    "    # Print the final report\n",
    "    print(f\"Unique values in '{file1_path}' (Column: {file1_column_name}): {len(unique_values_from_file1)}\")\n",
    "    print(f\"Unique values in '{file2_path}' (Column: {file2_column_name}): {len(unique_values_from_file2)}\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"Number of values that matched: {len(matching_values)}\")\n",
    "    print(\"-\" * 25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071b2422",
   "metadata": {},
   "source": [
    "SWWNO VS LOGSHEETHT CABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f09907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv...\n",
      "  - Found 343 unique, clean values.\n",
      "Processing file: /media/sagarkumar/New Volume/SAGAR/2-year-data/TXN_NMS_HTLOGSHEET.csv...\n",
      "  - Found 5145 unique, clean values.\n",
      "\n",
      "--- Comparison Report ---\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv' (Column: SWNO): 343\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/2-year-data/TXN_NMS_HTLOGSHEET.csv' (Column: SWITCH_NO): 5145\n",
      "-------------------------\n",
      "Number of values that matched: 239\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION: PLEASE EDIT THIS SECTION ---\n",
    "\n",
    "# File 1 Details\n",
    "file1_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv\"\n",
    "file1_column_name = \"SWNO\" \n",
    "\n",
    "# File 2 Details\n",
    "file2_path = \"/media/sagarkumar/New Volume/SAGAR/2-year-data/TXN_NMS_HTLOGSHEET.csv\"\n",
    "\n",
    "file2_column_name = \"SWITCH_NO\" \n",
    "\n",
    "# Output File Path\n",
    "# This file will contain only the values that exist in BOTH files.\n",
    "output_file_path = \"/path/to/your/output/matching_values.csv\"\n",
    "\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "\n",
    "def find_actual_column_name(columns, target_name):\n",
    "    \"\"\"Helper function to find a column name, ignoring case.\"\"\"\n",
    "    for col in columns:\n",
    "        if str(col).lower() == str(target_name).lower():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def get_unique_values_from_file(filepath, column_name):\n",
    "    \"\"\"\n",
    "    Reads a file, extracts unique values from a specific column,\n",
    "    cleans them, and returns them as a set.\n",
    "    \"\"\"\n",
    "    print(f\"Processing file: {filepath}...\")\n",
    "    try:\n",
    "        # Read just the header to find the correct column name (case-insensitive)\n",
    "        header_df = pd.read_csv(filepath, nrows=0, on_bad_lines='skip')\n",
    "        actual_col_name = find_actual_column_name(header_df.columns, column_name)\n",
    "\n",
    "        if not actual_col_name:\n",
    "            print(f\"  - Error: Column '{column_name}' not found. Please check the column name.\")\n",
    "            return set()\n",
    "\n",
    "        # Read the full column using the correct name\n",
    "        df = pd.read_csv(filepath, usecols=[actual_col_name], on_bad_lines='skip')\n",
    "        \n",
    "        # --- Data Cleaning ---\n",
    "        # Convert all values to string, extract digits, and then convert to numbers.\n",
    "        # This handles mixed data types (e.g., '123' vs 123) and text prefixes (e.g., 'SW-123').\n",
    "        s = pd.Series(df[actual_col_name].dropna().unique(), dtype=str)\n",
    "        s = s.str.extract('(\\d+)').iloc[:, 0]\n",
    "        s = pd.to_numeric(s, errors='coerce')\n",
    "        \n",
    "        cleaned_values = set(s.dropna().astype(int))\n",
    "        \n",
    "        print(f\"  - Found {len(cleaned_values)} unique, clean values.\")\n",
    "        return cleaned_values\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  - Error: File not found. Please check the path: {filepath}\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"  - An unexpected error occurred: {e}\")\n",
    "        return set()\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# Get the unique values from both files\n",
    "unique_values_from_file1 = get_unique_values_from_file(file1_path, file1_column_name)\n",
    "unique_values_from_file2 = get_unique_values_from_file(file2_path, file2_column_name)\n",
    "\n",
    "# --- Comparison and Reporting ---\n",
    "\n",
    "print(\"\\n--- Comparison Report ---\")\n",
    "if not unique_values_from_file1 or not unique_values_from_file2:\n",
    "    print(\"Could not perform comparison because one of the files could not be processed or contained no valid data.\")\n",
    "else:\n",
    "    # Use set intersection to find the values that exist in both sets\n",
    "    matching_values = unique_values_from_file1.intersection(unique_values_from_file2)\n",
    "    \n",
    "    # Print the final report\n",
    "    print(f\"Unique values in '{file1_path}' (Column: {file1_column_name}): {len(unique_values_from_file1)}\")\n",
    "    print(f\"Unique values in '{file2_path}' (Column: {file2_column_name}): {len(unique_values_from_file2)}\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"Number of values that matched: {len(matching_values)}\")\n",
    "    print(\"-\" * 25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289c2eb9",
   "metadata": {},
   "source": [
    "IMPTT VS IMRG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e0bd4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /media/sagark24/New Volume/MERGE CDIS/2-Year-data/IMRG.csv...\n",
      "  - Found 1239058 unique, clean values.\n",
      "Processing file: /media/sagark24/New Volume/MERGE CDIS/2-Year-data/IMPTT.csv...\n",
      "  - Found 400912 unique, clean values.\n",
      "\n",
      "--- Comparison Report ---\n",
      "Unique values in '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/IMRG.csv' (Column: POINT): 1239058\n",
      "Unique values in '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/IMPTT.csv' (Column: POINT): 400912\n",
      "-------------------------\n",
      "Number of values that matched: 9178\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION: PLEASE EDIT THIS SECTION ---\n",
    "\n",
    "# File 1 Details\n",
    "file1_path = \"/media/sagark24/New Volume/MERGE CDIS/2-Year-data/IMRG.csv\"\n",
    "file1_column_name = \"POINT\" \n",
    "\n",
    "# File 2 Details\n",
    "file2_path = \"/media/sagark24/New Volume/MERGE CDIS/2-Year-data/IMPTT.csv\"\n",
    "\n",
    "file2_column_name = \"POINT\" \n",
    "\n",
    "# Output File Path\n",
    "# This file will contain only the values that exist in BOTH files.\n",
    "output_file_path = \"/path/to/your/output/matching_values.csv\"\n",
    "\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "\n",
    "def find_actual_column_name(columns, target_name):\n",
    "    \"\"\"Helper function to find a column name, ignoring case.\"\"\"\n",
    "    for col in columns:\n",
    "        if str(col).lower() == str(target_name).lower():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def get_unique_values_from_file(filepath, column_name):\n",
    "    \"\"\"\n",
    "    Reads a file, extracts unique values from a specific column,\n",
    "    cleans them, and returns them as a set.\n",
    "    \"\"\"\n",
    "    print(f\"Processing file: {filepath}...\")\n",
    "    try:\n",
    "        # Read just the header to find the correct column name (case-insensitive)\n",
    "        header_df = pd.read_csv(filepath, nrows=0, on_bad_lines='skip')\n",
    "        actual_col_name = find_actual_column_name(header_df.columns, column_name)\n",
    "\n",
    "        if not actual_col_name:\n",
    "            print(f\"  - Error: Column '{column_name}' not found. Please check the column name.\")\n",
    "            return set()\n",
    "\n",
    "        # Read the full column using the correct name\n",
    "        df = pd.read_csv(filepath, usecols=[actual_col_name], on_bad_lines='skip')\n",
    "        \n",
    "        # --- Data Cleaning ---\n",
    "        # Convert all values to string, extract digits, and then convert to numbers.\n",
    "        # This handles mixed data types (e.g., '123' vs 123) and text prefixes (e.g., 'SW-123').\n",
    "        s = pd.Series(df[actual_col_name].dropna().unique(), dtype=str)\n",
    "        s = s.str.extract('(\\d+)').iloc[:, 0]\n",
    "        s = pd.to_numeric(s, errors='coerce')\n",
    "        \n",
    "        cleaned_values = set(s.dropna().astype(int))\n",
    "        \n",
    "        print(f\"  - Found {len(cleaned_values)} unique, clean values.\")\n",
    "        return cleaned_values\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  - Error: File not found. Please check the path: {filepath}\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"  - An unexpected error occurred: {e}\")\n",
    "        return set()\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# Get the unique values from both files\n",
    "unique_values_from_file1 = get_unique_values_from_file(file1_path, file1_column_name)\n",
    "unique_values_from_file2 = get_unique_values_from_file(file2_path, file2_column_name)\n",
    "\n",
    "# --- Comparison and Reporting ---\n",
    "\n",
    "print(\"\\n--- Comparison Report ---\")\n",
    "if not unique_values_from_file1 or not unique_values_from_file2:\n",
    "    print(\"Could not perform comparison because one of the files could not be processed or contained no valid data.\")\n",
    "else:\n",
    "    # Use set intersection to find the values that exist in both sets\n",
    "    matching_values = unique_values_from_file1.intersection(unique_values_from_file2)\n",
    "    \n",
    "    # Print the final report\n",
    "    print(f\"Unique values in '{file1_path}' (Column: {file1_column_name}): {len(unique_values_from_file1)}\")\n",
    "    print(f\"Unique values in '{file2_path}' (Column: {file2_column_name}): {len(unique_values_from_file2)}\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"Number of values that matched: {len(matching_values)}\")\n",
    "    print(\"-\" * 25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d5f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "915d2e45",
   "metadata": {},
   "source": [
    "DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441cbc75",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8079f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56070/3907562845.py:73: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"TS\"]    = pd.to_datetime(df[\"SYSTIME\"], errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ…  Saved /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/SWNO_MASTER_COMBINED_FULL.csv | rows: 8110 | cols: 100\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd, numpy as np, glob, os, gc\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# â”€â”€ PATHS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BASE = \"/media/sagark24/New Volume/MERGE CDIS\"\n",
    "FILE_SWNO   = Path(f\"{BASE}/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv\")\n",
    "FILE_CABLE  = Path(f\"{BASE}/2-Year-data/CLEANED_DATA/ht_cleaned.csv\")\n",
    "FAULT_FILE  = Path(f\"{BASE}/IPYNB_FILE/DATA_GENERATION/HT_fault_cable_info_processed2.csv\")\n",
    "SCADA_FOLDERS = [\n",
    "    f\"{BASE}/2-Year-data/200/200\",\n",
    "    f\"{BASE}/2-Year-data/200-400/200-400\",\n",
    "    f\"{BASE}/2-Year-data/400-600/400-600\",\n",
    "    f\"{BASE}/2-Year-data/600-759/600-759\",\n",
    "    f\"{BASE}/2-Year-data/SCADA_JAN_24_TO_APR_25\",\n",
    "]\n",
    "OUT_FILE = Path(f\"{BASE}/IPYNB_FILE/DATA_GENERATION/SWNO_MASTER_COMBINED_FULL.csv\")\n",
    "\n",
    "# â”€â”€ HELPERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def norm(x): return np.nan if pd.isna(x) else str(x).strip().upper().lstrip(\"0\")\n",
    "def norm_volt(v):\n",
    "    v=str(v).upper().replace(' ','')\n",
    "    return \"22KV\" if v in (\"22\",\"22KV\") else \"33KV\" if v in (\"33\",\"33KV\") else v\n",
    "def mcols(tag): return [f\"{tag}_Month_{i:02}\" for i in range(1,13)]\n",
    "\n",
    "# â”€â”€â”€ 1.  SWNO master list â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "swno_master = pd.read_csv(FILE_SWNO, dtype=str)\n",
    "swno_master[\"SWNO\"] = swno_master[\"SWNO\"].apply(norm)\n",
    "SWNO_SET = set(swno_master[\"SWNO\"])\n",
    "\n",
    "# â”€â”€â”€ 2.  HT-CABLE  (filtered) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "cable = pd.read_csv(FILE_CABLE, dtype=str)\n",
    "cable.rename(columns={\n",
    "    \"CABEL_ID\":\"CABLEID\",\"CABLECOUNTUCTOR\":\"CABLECONDUCTORMATERIAL\",\n",
    "    \"NO.OFCORES\":\"NUMBEROFCORES\",\"NEUTRAL_MATERIAL\":\"NEUTRALMATERIAL\",\n",
    "    \"coments\":\"COMMENTS\"}, inplace=True)\n",
    "cable[\"DESTINATION_SWITCH_ID\"] = cable[\"DESTINATION_SWITCH_ID\"].apply(norm)\n",
    "cable = cable[cable[\"DESTINATION_SWITCH_ID\"].isin(SWNO_SET)]\n",
    "\n",
    "KEEP = [\"DESTINATION_SWITCH_ID\",\"COMMENTS\",\"CABLETYPE\",\"MEASUREDLENGTH\",\n",
    "        \"NUMBEROFCORES\",\"ARMOURED\",\"NEUTRALMATERIAL\",\"CABLEID\",\n",
    "        \"CABLECONDUCTORMATERIAL\",\"DIVISIONCODE\",\"ZONECODE\",\"REMARKS\",\n",
    "        \"SOURCE_SWITCH_ID\",\"SOURCE_SS\",\"DESTINATION_SS\",\"DATECREATED\"]\n",
    "cable = cable[[c for c in KEEP if c in cable.columns]]\n",
    "cable[\"COMMENTS\"] = cable[\"COMMENTS\"].fillna(\"\")\n",
    "cable[\"SWNO\"] = cable[\"DESTINATION_SWITCH_ID\"]\n",
    "\n",
    "missing = swno_master[~swno_master[\"SWNO\"].isin(cable[\"SWNO\"])]\n",
    "if not missing.empty:\n",
    "    pad = {col:\"\" for col in cable.columns if col!=\"SWNO\"}\n",
    "    cable = pd.concat([cable, missing.assign(**pad)], ignore_index=True)\n",
    "\n",
    "# â”€â”€â”€ 3.  SCADA worker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def scada_worker(path):\n",
    "    try:\n",
    "        df = pd.read_csv(path,\n",
    "            usecols=['SYSTIME','SWNO','VOLTAGE','PARA','VALUE'],\n",
    "            dtype={'SYSTIME':str,'SWNO':str,'VOLTAGE':'category',\n",
    "                   'PARA':'category','VALUE':'float32'},\n",
    "            low_memory=True)\n",
    "        df[\"SWNO\"] = df[\"SWNO\"].astype(str).str.strip().apply(norm)\n",
    "        df = df[df[\"SWNO\"].isin(SWNO_SET)]\n",
    "        if df.empty: return None\n",
    "        df[\"VOLTAGE\"] = df[\"VOLTAGE\"].map(norm_volt)\n",
    "        df = df[df[\"VOLTAGE\"].isin([\"22KV\",\"33KV\"])]\n",
    "        df[\"PARA\"] = df[\"PARA\"].str.upper().str.strip()\n",
    "        df = df[df[\"PARA\"].isin([\"I\",\"KW\"])]\n",
    "        df[\"VALUE\"] = pd.to_numeric(df[\"VALUE\"], errors=\"coerce\")\n",
    "        df = df[df[\"VALUE\"]>0]\n",
    "        df[\"TS\"]    = pd.to_datetime(df[\"SYSTIME\"], errors=\"coerce\")\n",
    "        df[\"MONTH\"] = df[\"TS\"].dt.month.astype(\"Int8\")\n",
    "        df.dropna(subset=[\"VALUE\",\"MONTH\"], inplace=True)\n",
    "        if df.empty: return None\n",
    "        return df\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "files=[f for fld in SCADA_FOLDERS for f in glob.glob(os.path.join(fld,'*.csv'))]\n",
    "parts=[]\n",
    "with ProcessPoolExecutor(max_workers=8) as pool:\n",
    "    for part in pool.map(scada_worker, files):\n",
    "        if part is not None: parts.append(part)\n",
    "\n",
    "if parts:\n",
    "    df = pd.concat(parts, ignore_index=True)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=['SWNO','VOLTAGE','PARA','VALUE','MONTH','TS'])\n",
    "\n",
    "# â”€â”€â”€â”€â”€ 3a  I-average â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "curr = (df[df[\"PARA\"]==\"I\"]\n",
    "        .groupby([\"SWNO\",\"VOLTAGE\",\"MONTH\"], observed=True)[\"VALUE\"]\n",
    "        .mean().reset_index())\n",
    "pI = (curr.pivot(index=[\"SWNO\",\"VOLTAGE\"], columns=\"MONTH\", values=\"VALUE\")\n",
    "           .reindex(columns=range(1,13), fill_value=np.nan))\n",
    "pI.columns = mcols(\"I\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€ 3b  KW metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "kw = df[df[\"PARA\"]==\"KW\"].copy()\n",
    "if not kw.empty:\n",
    "    # monthly avg & peak\n",
    "    kw_avg  = kw.groupby([\"SWNO\",\"VOLTAGE\",\"MONTH\"], observed=True)[\"VALUE\"].mean().reset_index(name=\"AVG\")\n",
    "    kw_peak = kw.groupby([\"SWNO\",\"VOLTAGE\",\"MONTH\"], observed=True)[\"VALUE\"].max().reset_index(name=\"PEAK\")\n",
    "\n",
    "    # add month averages for later thresholds\n",
    "    kw = kw.merge(kw_avg, on=[\"SWNO\",\"VOLTAGE\",\"MONTH\"])\n",
    "\n",
    "    # daily maximum for loading-cycle calc\n",
    "    kw[\"DATE\"] = kw[\"TS\"].dt.floor(\"D\").dt.date\n",
    "    day_max = kw.groupby([\"SWNO\",\"VOLTAGE\",\"DATE\"], observed=True)[\"VALUE\"].max().reset_index()\n",
    "    day_max.sort_values([\"SWNO\",\"VOLTAGE\",\"DATE\"], inplace=True)\n",
    "    day_max[\"PREV\"]  = day_max.groupby([\"SWNO\",\"VOLTAGE\"])[\"VALUE\"].shift(1)\n",
    "    day_max[\"MONTH\"] = pd.to_datetime(day_max[\"DATE\"]).dt.month.astype(\"Int8\")\n",
    "    day_max = day_max.merge(kw_avg, on=[\"SWNO\",\"VOLTAGE\",\"MONTH\"])\n",
    "    day_max[\"RAMP\"]  = ((day_max[\"PREV\"] < day_max[\"AVG\"]) &\n",
    "                        (day_max[\"VALUE\"] >= day_max[\"AVG\"])).astype(\"int8\")\n",
    "    cycles = (day_max.groupby([\"SWNO\",\"VOLTAGE\",\"MONTH\"], observed=True)[\"RAMP\"]\n",
    "                    .sum().reset_index(name=\"CYCLE\"))\n",
    "\n",
    "    # overload events > 200 % monthly average\n",
    "    kw[\"OVR\"] = (kw[\"VALUE\"] > kw[\"AVG\"]*2.0).astype(\"int8\")\n",
    "    ovr = kw.groupby([\"SWNO\",\"VOLTAGE\",\"MONTH\"], observed=True)[\"OVR\"].sum().reset_index(name=\"OVR\")\n",
    "\n",
    "    # pivot each metric\n",
    "    def piv(metric, tag):\n",
    "        p = metric.pivot(index=[\"SWNO\",\"VOLTAGE\"], columns=\"MONTH\", values=tag) \\\n",
    "                  .reindex(columns=range(1,13), fill_value=np.nan)\n",
    "        p.columns = mcols(tag if tag!=\"AVG\" else \"Avg_Load\")\n",
    "        return p\n",
    "    pAVG   = piv(kw_avg, \"AVG\")\n",
    "    pPEAK  = piv(kw_peak, \"PEAK\")\n",
    "    pCYCLE = piv(cycles,  \"CYCLE\")\n",
    "    pOVR   = piv(ovr,     \"OVR\")\n",
    "    sc_wide = pI.join([pAVG,pPEAK,pCYCLE,pOVR]).reset_index()\n",
    "else:\n",
    "    sc_wide = pI.reset_index()\n",
    "\n",
    "# â”€â”€â”€ 4.  Fault log (Num_Faults + FAULT_* columns) â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fault_cols = [\n",
    "    \"STATION_NAME\",\"STD_CABLE_SIZE\",\"CABLE_TYPE\",\"TIME_OUTAGE\",\"SOURCE_SS\",\n",
    "    \"FROM_SWITCH\",\"DESTINATION_SS\",\"TO_SWITCH\",\"VOLTAGE\",\"TIME_DIFFERENCE_HOURS\",\n",
    "    \"RELAY_FUSE_B\",\"RELAY_FUSE_N\",\"RELAY_FUSE_Y\",\"DIVISION\",\"RELAY_FUSE\",\n",
    "    \"AFFECTED_STATION\",\"AFFECTED_SWITCH\"\n",
    "]\n",
    "fault = pd.read_csv(FAULT_FILE, dtype=str)\n",
    "fault[\"SWNO\"] = fault[\"SWITCH_NO\"].apply(norm)\n",
    "fault = fault[fault[\"SWNO\"].isin(SWNO_SET)]\n",
    "\n",
    "# prefix\n",
    "fault = fault.rename(columns={c: f\"FAULT_{c}\" for c in fault_cols})\n",
    "FAULT_COLS = [f\"FAULT_{c}\" for c in fault_cols]\n",
    "\n",
    "fault_cnt = fault.groupby(\"SWNO\", observed=True).size().reset_index(name=\"Num_Faults\")\n",
    "agg = {col:(lambda s:\" | \".join(pd.unique(s.dropna()))) for col in FAULT_COLS}\n",
    "fault_agg = fault.groupby(\"SWNO\", observed=True).agg(agg).reset_index()\n",
    "fault_data = fault_cnt.merge(fault_agg, on=\"SWNO\", how=\"left\")\n",
    "\n",
    "# â”€â”€â”€ 5.  Merge layers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "full = (cable\n",
    "        .merge(sc_wide,   how=\"left\", on=\"SWNO\")\n",
    "        .merge(fault_data, how=\"left\", on=\"SWNO\"))\n",
    "\n",
    "full[\"Num_Faults\"] = full[\"Num_Faults\"].fillna(0).astype(\"int16\")\n",
    "for col in FAULT_COLS:\n",
    "    full[col] = full[col].fillna(\"\")\n",
    "\n",
    "for blank in [\"IR_Measurement_MOhm\",\"Tan_Delta\",\n",
    "              \"Partial_Discharge_Frequency\",\"Partial_Discharge_Intensity\"]:\n",
    "    full[blank] = \"\"\n",
    "\n",
    "front = [\"SWNO\",\"COMMENTS\",\"CABLETYPE\",\"MEASUREDLENGTH\",\"NUMBEROFCORES\",\"ARMOURED\",\n",
    "         \"NEUTRALMATERIAL\",\"CABLEID\",\"CABLECONDUCTORMATERIAL\",\n",
    "         \"DIVISIONCODE\",\"ZONECODE\",\"REMARKS\",\"DATECREATED\",\n",
    "         \"SOURCE_SWITCH_ID\",\"DESTINATION_SWITCH_ID\",\"SOURCE_SS\",\"DESTINATION_SS\",\n",
    "         \"Num_Faults\"] + FAULT_COLS\n",
    "front = [c for c in front if c in full.columns]\n",
    "full  = full[front + [c for c in full.columns if c not in front]]\n",
    "\n",
    "# â”€â”€â”€ 6.  Save â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "full.to_csv(OUT_FILE, index=False, float_format=\"%.3f\")\n",
    "print(\"  Saved\", OUT_FILE, \"| rows:\", len(full), \"| cols:\", len(full.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50503af2",
   "metadata": {},
   "source": [
    "REMOVE KW USE I T OCALCULATE LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed8e98e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total SCADA files: 763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_530221/1727671945.py:123: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"TS\"] = pd.to_datetime(df[\"SYSTIME\"], errors=\"coerce\" , utc=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SCADA files with data: 762\n",
      " Total SCADA rows: 16859880\n",
      " Saved: /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/SWNO_MASTER_COMBINED_FULL2.csv | Rows: 321 | Columns: 88\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd, numpy as np, glob, os, re\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# â”€â”€ PATHS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BASE = \"/media/sagark24/New Volume/MERGE CDIS\"\n",
    "FILE_SWNO   = Path(f\"{BASE}/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv\")\n",
    "FILE_CABLE  = Path(f\"{BASE}/2-Year-data/CLEANED_DATA/ht_cleaned.csv\")\n",
    "FAULT_FILE  = Path(f\"{BASE}/IPYNB_FILE/DATA_GENERATION/HT_fault_cable_info_processed_without_affected.csv\")\n",
    "SCADA_FOLDERS = [\n",
    "    f\"{BASE}/2-Year-data/200/200\",\n",
    "    f\"{BASE}/2-Year-data/200-400/200-400\",\n",
    "    f\"{BASE}/2-Year-data/400-600/400-600\",\n",
    "    f\"{BASE}/2-Year-data/600-759/600-759\",\n",
    "    f\"{BASE}/2-Year-data/SCADA_JAN_24_TO_APR_25\",\n",
    "]\n",
    "OUT_FILE = Path(f\"{BASE}/IPYNB_FILE/DATA_GENERATION/SWNO_MASTER_COMBINED_FULL2.csv\")\n",
    "\n",
    "# â”€â”€ HELPERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def norm(x): return np.nan if pd.isna(x) else str(x).strip().upper().lstrip(\"0\")\n",
    "def norm_volt(v):\n",
    "    v=str(v).upper().replace(' ','')\n",
    "    return \"22KV\" if v in (\"22\",\"22KV\") else \"33KV\" if v in (\"33\",\"33KV\") else v\n",
    "def mcols(tag): return [f\"{tag}_Month_{i:02}\" for i in range(1,13)]\n",
    "\n",
    "def extract_ordered_path(comment):\n",
    "    \"\"\"If any SWNO_... or JT.NO... is in the comment, extract & join.\n",
    "    Else, return comment as PATH. If blank, return blank.\"\"\"\n",
    "    if not comment or not isinstance(comment, str):\n",
    "        return \"\"\n",
    "    comment_up = comment.upper()\n",
    "    matches = re.findall(r'(SWNO_\\w+|JT\\.NO\\.\\d+[A-Z]?)', comment_up)\n",
    "    if matches:\n",
    "        return \" â†’ \".join(matches)\n",
    "    comment_strip = comment.strip()\n",
    "    return comment_strip if comment_strip else \"\"\n",
    "\n",
    "# â”€â”€â”€ 1.  SWNO master list â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "swno_master = pd.read_csv(FILE_SWNO, dtype=str)\n",
    "swno_master[\"SWNO\"] = swno_master[\"SWNO\"].apply(norm)\n",
    "SWNO_SET = set(swno_master[\"SWNO\"])\n",
    "\n",
    "# â”€â”€â”€ 2.  HT-CABLE  (filtered) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "cable = pd.read_csv(FILE_CABLE, dtype=str)\n",
    "cable.rename(columns={\n",
    "    \"CABEL_ID\":\"CABLEID\",\"CABLECOUNTUCTOR\":\"CABLECONDUCTORMATERIAL\",\n",
    "    \"NO.OFCORES\":\"NUMBEROFCORES\",\"NEUTRAL_MATERIAL\":\"NEUTRALMATERIAL\",\n",
    "    \"coments\":\"COMMENTS\"}, inplace=True)\n",
    "\n",
    "cable[\"DESTINATION_SWITCH_ID\"] = cable[\"DESTINATION_SWITCH_ID\"].apply(norm)\n",
    "cable = cable[cable[\"DESTINATION_SWITCH_ID\"].isin(SWNO_SET)]\n",
    "\n",
    "KEEP = [\"DESTINATION_SWITCH_ID\",\"COMMENTS\",\"CABLETYPE\",\"MEASUREDLENGTH\",\n",
    "        \"NUMBEROFCORES\",\"ARMOURED\",\"NEUTRALMATERIAL\",\"CABLEID\",\n",
    "        \"CABLECONDUCTORMATERIAL\",\"DIVISIONCODE\",\"ZONECODE\",\"REMARKS\",\n",
    "        \"SOURCE_SWITCH_ID\",\"SOURCE_SS\",\"DESTINATION_SS\",\"DATECREATED\"]\n",
    "cable = cable[[c for c in KEEP if c in cable.columns]]\n",
    "cable[\"COMMENTS\"] = cable[\"COMMENTS\"].fillna(\"\")\n",
    "cable[\"SWNO\"] = cable[\"DESTINATION_SWITCH_ID\"]\n",
    "\n",
    "# Handle missing switches from SWNO master\n",
    "missing = swno_master[~swno_master[\"SWNO\"].isin(cable[\"SWNO\"])]\n",
    "if not missing.empty:\n",
    "    pad = {col:\"\" for col in cable.columns if col!=\"SWNO\"}\n",
    "    cable = pd.concat([cable, missing.assign(**pad)], ignore_index=True)\n",
    "\n",
    "# â”€â”€â”€ Extract connection path â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "cable[\"PATH\"] = cable[\"COMMENTS\"].apply(extract_ordered_path)\n",
    "\n",
    "# â”€â”€â”€ Count segments per switch pair â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "cable[\"SEGMENT_KEY\"] = cable[\"SOURCE_SWITCH_ID\"].fillna('') + \"â†’\" + cable[\"DESTINATION_SWITCH_ID\"].fillna('')\n",
    "segment_counts = cable.groupby(\"SEGMENT_KEY\").size().reset_index(name=\"NO_OF_SEGMENT\")\n",
    "\n",
    "# â”€â”€â”€ Deduplicate rows by SOURCE and DESTINATION SWITCH ID â”€â”€\n",
    "cable[\"MEASUREDLENGTH\"] = pd.to_numeric(cable[\"MEASUREDLENGTH\"], errors=\"coerce\")\n",
    "cable = (cable\n",
    "         .groupby([\"SOURCE_SWITCH_ID\", \"DESTINATION_SWITCH_ID\"], dropna=False)\n",
    "         .agg({\n",
    "             \"MEASUREDLENGTH\": \"sum\",\n",
    "             \"PATH\": lambda x: \" â†’ \".join(sorted(set(x))),\n",
    "             \"CABLETYPE\": \"first\",\n",
    "             \"NUMBEROFCORES\": \"first\",\n",
    "             \"ARMOURED\": \"first\",\n",
    "             \"NEUTRALMATERIAL\": \"first\",\n",
    "             \"CABLEID\": \"first\",\n",
    "             \"CABLECONDUCTORMATERIAL\": \"first\",\n",
    "             \"DIVISIONCODE\": \"first\",\n",
    "             \"ZONECODE\": \"first\",\n",
    "             \"REMARKS\": \"first\",\n",
    "             \"SOURCE_SS\": \"first\",\n",
    "             \"DESTINATION_SS\": \"first\",\n",
    "             \"DATECREATED\": \"first\",\n",
    "         }).reset_index())\n",
    "\n",
    "# â”€â”€â”€ Merge segment count â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "cable[\"SEGMENT_KEY\"] = cable[\"SOURCE_SWITCH_ID\"].fillna('') + \"->\" + cable[\"DESTINATION_SWITCH_ID\"].fillna('')\n",
    "cable = cable.merge(segment_counts, on=\"SEGMENT_KEY\", how=\"left\")\n",
    "cable.drop(columns=[\"SEGMENT_KEY\"], inplace=True)\n",
    "\n",
    "# â”€â”€â”€ Recreate SWNO column â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "cable[\"SWNO\"] = cable[\"DESTINATION_SWITCH_ID\"]\n",
    "\n",
    "# â”€â”€â”€ 3.  SCADA worker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def scada_worker(path):\n",
    "    try:\n",
    "        df = pd.read_csv(path,\n",
    "            usecols=['SYSTIME','SWNO','VOLTAGE','PARA','VALUE'],\n",
    "            dtype={'SYSTIME':str,'SWNO':str,'VOLTAGE':'category',\n",
    "                   'PARA':'category','VALUE':'float32'},\n",
    "            low_memory=True)\n",
    "        df[\"SWNO\"] = df[\"SWNO\"].astype(str).str.strip().apply(norm)\n",
    "        df = df[df[\"SWNO\"].isin(SWNO_SET)]\n",
    "        if df.empty: return None\n",
    "        df[\"VOLTAGE\"] = df[\"VOLTAGE\"].map(norm_volt)\n",
    "        df = df[df[\"VOLTAGE\"].isin([\"22KV\",\"33KV\"])]\n",
    "        df[\"PARA\"] = df[\"PARA\"].str.upper().str.strip()\n",
    "        df = df[df[\"PARA\"] == \"I\"]\n",
    "        df[\"VALUE\"] = pd.to_numeric(df[\"VALUE\"], errors=\"coerce\")\n",
    "        df = df[df[\"VALUE\"] > 0]\n",
    "        df[\"TS\"] = pd.to_datetime(df[\"SYSTIME\"], errors=\"coerce\" , utc=True)\n",
    "        df[\"MONTH\"] = df[\"TS\"].dt.month.astype(\"Int8\")\n",
    "        df.dropna(subset=[\"VALUE\",\"MONTH\"], inplace=True)\n",
    "        if df.empty: return None\n",
    "        return df\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "files = [f for fld in SCADA_FOLDERS for f in glob.glob(os.path.join(fld, '*.csv'))]\n",
    "print(\" Total SCADA files:\", len(files))\n",
    "parts = []\n",
    "with ProcessPoolExecutor(max_workers=8) as pool:\n",
    "    for part in pool.map(scada_worker, files):\n",
    "        if part is not None:\n",
    "            parts.append(part)\n",
    "print(\" SCADA files with data:\", len(parts))\n",
    "print(\" Total SCADA rows:\", sum(len(p) for p in parts))\n",
    "\n",
    "df = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame(columns=['SWNO','VOLTAGE','VALUE','MONTH','TS'])\n",
    "\n",
    "# â”€â”€â”€ 3a. I metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "pI = (df.groupby([\"SWNO\",\"VOLTAGE\",\"MONTH\"], observed=True)[\"VALUE\"]\n",
    "        .mean().unstack().reindex(columns=range(1,13)).fillna(np.nan))\n",
    "pI.columns = mcols(\"I\")\n",
    "\n",
    "# â”€â”€â”€ 3b. Additional metrics (Peak, Cycle, OVR) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def pivot_metric(metric, tag):\n",
    "    p = metric.pivot(index=[\"SWNO\", \"VOLTAGE\"], columns=\"MONTH\", values=tag)\n",
    "    p = p.reindex(columns=range(1, 13), fill_value=np.nan)\n",
    "    p.columns = mcols(tag)\n",
    "    return p\n",
    "\n",
    "if not df.empty:\n",
    "    i_df = df.copy()\n",
    "    i_df[\"DATE\"] = i_df[\"TS\"].dt.date\n",
    "\n",
    "    pI_long = pI.reset_index().melt(id_vars=[\"SWNO\",\"VOLTAGE\"], var_name=\"MONTH_COL\", value_name=\"AVG\")\n",
    "    pI_long[\"MONTH\"] = pI_long[\"MONTH_COL\"].str.extract(r\"(\\d+)$\").astype(int)\n",
    "    i_df = i_df.merge(pI_long.drop(columns=\"MONTH_COL\"), on=[\"SWNO\", \"VOLTAGE\", \"MONTH\"], how=\"left\")\n",
    "\n",
    "    peak = i_df.groupby([\"SWNO\", \"VOLTAGE\", \"MONTH\"], observed=True)[\"VALUE\"].max().reset_index(name=\"PEAK\")\n",
    "\n",
    "    day_max = i_df.groupby([\"SWNO\", \"VOLTAGE\", \"DATE\"], observed=True)[\"VALUE\"].max().reset_index()\n",
    "    day_max.sort_values([\"SWNO\", \"VOLTAGE\", \"DATE\"], inplace=True)\n",
    "    day_max[\"PREV\"] = day_max.groupby([\"SWNO\", \"VOLTAGE\"])[\"VALUE\"].shift()\n",
    "    day_max[\"MONTH\"] = pd.to_datetime(day_max[\"DATE\"]).dt.month.astype(\"Int8\")\n",
    "    day_max = day_max.merge(pI_long.drop(columns=\"MONTH_COL\"), on=[\"SWNO\", \"VOLTAGE\", \"MONTH\"], how=\"left\")\n",
    "    day_max[\"RAMP\"] = ((day_max[\"PREV\"] < day_max[\"AVG\"]) & (day_max[\"VALUE\"] >= day_max[\"AVG\"])).astype(\"int8\")\n",
    "    cycle = day_max.groupby([\"SWNO\", \"VOLTAGE\", \"MONTH\"], observed=True)[\"RAMP\"].sum().reset_index(name=\"CYCLE\")\n",
    "\n",
    "    i_df[\"OVR\"] = (i_df[\"VALUE\"] > 2 * i_df[\"AVG\"]).astype(\"int8\")\n",
    "    ovr = i_df.groupby([\"SWNO\", \"VOLTAGE\", \"MONTH\"], observed=True)[\"OVR\"].sum().reset_index(name=\"OVR\")\n",
    "\n",
    "    pPEAK = pivot_metric(peak, \"PEAK\")\n",
    "    pCYCLE = pivot_metric(cycle, \"CYCLE\")\n",
    "    pOVR = pivot_metric(ovr, \"OVR\")\n",
    "\n",
    "    sc_wide = pI.join([pPEAK, pCYCLE, pOVR]).reset_index()\n",
    "else:\n",
    "    sc_wide = pI.reset_index()\n",
    "\n",
    "# â”€â”€â”€ 4. Faults â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fault = pd.read_csv(FAULT_FILE, dtype=str)\n",
    "fault[\"SWNO\"] = fault[\"SWITCH_NO\"].apply(norm)\n",
    "fault = fault[fault[\"SWNO\"].isin(SWNO_SET)]\n",
    "\n",
    "fault_cols = [\n",
    "   \"TIME_OUTAGE\",\n",
    "    \"FROM_SWITCH\",\"TO_SWITCH\",\"VOLTAGE\",\"TIME_DIFFERENCE_HOURS\",\n",
    "    \"RELAY_FUSE_B\",\"RELAY_FUSE_N\",\"RELAY_FUSE_Y\",\"DIVISION\"\n",
    "]\n",
    "fault.rename(columns={c: f\"FAULT_{c}\" for c in fault_cols}, inplace=True)\n",
    "FAULT_COLS = [f\"FAULT_{c}\" for c in fault_cols]\n",
    "\n",
    "fault_cnt = fault.groupby(\"SWNO\", observed=True).size().reset_index(name=\"Num_Faults\")\n",
    "fault_agg = fault.groupby(\"SWNO\", observed=True).agg(lambda s: \" | \".join(pd.unique(s.dropna()))).reset_index()\n",
    "fault_data = fault_cnt.merge(fault_agg, on=\"SWNO\", how=\"left\")\n",
    "\n",
    "# â”€â”€â”€ 5. Merge all â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "full = (cable\n",
    "        .merge(sc_wide, how=\"left\", on=[\"SWNO\"])\n",
    "        .merge(fault_data, how=\"left\", on=\"SWNO\"))\n",
    "\n",
    "full[\"Num_Faults\"] = full[\"Num_Faults\"].fillna(0).astype(\"int16\")\n",
    "for col in FAULT_COLS:\n",
    "    full[col] = full[col].fillna(\"\")\n",
    "for blank in [\"IR_Measurement_MOhm\",\"Tan_Delta\",\"Partial_Discharge_Frequency\",\"Partial_Discharge_Intensity\"]:\n",
    "    full[blank] = \"\"\n",
    "full = full.dropna(how=\"all\")\n",
    "\n",
    "# â”€â”€â”€ 6. Save â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "full.to_csv(OUT_FILE, index=False, float_format=\"%.3f\")\n",
    "print(f\" Saved: {OUT_FILE} | Rows: {len(full)} | Columns: {len(full.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79cbd29",
   "metadata": {},
   "source": [
    "SCADA LOGIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15ca7f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_530221/4043561826.py:43: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"TS\"] = pd.to_datetime(df[\"SYSTIME\"], errors=\"coerce\" , utc=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/SCADA_WIDE_ONLY.csv | Rows: 311 | Columns: 50\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd, numpy as np, glob, os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "BASE = \"/media/sagark24/New Volume/MERGE CDIS\"\n",
    "FILE_SWNO   = f\"{BASE}/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv\"\n",
    "SCADA_FOLDERS = [\n",
    "    f\"{BASE}/2-Year-data/200/200\",\n",
    "    f\"{BASE}/2-Year-data/200-400/200-400\",\n",
    "    f\"{BASE}/2-Year-data/400-600/400-600\",\n",
    "    f\"{BASE}/2-Year-data/600-759/600-759\",\n",
    "    f\"{BASE}/2-Year-data/SCADA_JAN_24_TO_APR_25\",\n",
    "]\n",
    "SCADA_OUT = f\"{BASE}/IPYNB_FILE/DATA_GENERATION/SCADA_WIDE_ONLY.csv\"\n",
    "\n",
    "def norm(x): return np.nan if pd.isna(x) else str(x).strip().upper().lstrip(\"0\")\n",
    "def norm_volt(v):\n",
    "    v = str(v).upper().replace(' ','')\n",
    "    return \"22KV\" if v in (\"22\",\"22KV\") else \"33KV\" if v in (\"33\",\"33KV\") else v\n",
    "def mcols(tag): return [f\"{tag}_Month_{i:02}\" for i in range(1,13)]\n",
    "\n",
    "swno_master = pd.read_csv(FILE_SWNO, dtype=str)\n",
    "SWNO_SET = set(swno_master[\"SWNO\"].apply(norm))\n",
    "\n",
    "def scada_worker(path):\n",
    "    try:\n",
    "        df = pd.read_csv(path,\n",
    "            usecols=['SYSTIME','SWNO','VOLTAGE','PARA','VALUE'],\n",
    "            dtype={'SYSTIME':str,'SWNO':str,'VOLTAGE':'category',\n",
    "                   'PARA':'category','VALUE':'float32'},\n",
    "            low_memory=True)\n",
    "        df[\"SWNO\"] = df[\"SWNO\"].astype(str).str.strip().apply(norm)\n",
    "        df = df[df[\"SWNO\"].isin(SWNO_SET)]\n",
    "        if df.empty: return None\n",
    "        df[\"VOLTAGE\"] = df[\"VOLTAGE\"].map(norm_volt)\n",
    "        df = df[df[\"VOLTAGE\"].isin([\"22KV\",\"33KV\"])]\n",
    "        df[\"PARA\"] = df[\"PARA\"].str.upper().str.strip()\n",
    "        df = df[df[\"PARA\"] == \"I\"]\n",
    "        df[\"VALUE\"] = pd.to_numeric(df[\"VALUE\"], errors=\"coerce\")\n",
    "        df = df[df[\"VALUE\"] > 0]\n",
    "        df[\"TS\"] = pd.to_datetime(df[\"SYSTIME\"], errors=\"coerce\" , utc=True)\n",
    "        df[\"MONTH\"] = df[\"TS\"].dt.month.astype(\"Int8\")\n",
    "        df.dropna(subset=[\"VALUE\",\"MONTH\"], inplace=True)\n",
    "        if df.empty: return None\n",
    "        return df\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "files = [f for fld in SCADA_FOLDERS for f in glob.glob(os.path.join(fld, '*.csv'))]\n",
    "parts = []\n",
    "with ProcessPoolExecutor(max_workers=8) as pool:\n",
    "    for part in pool.map(scada_worker, files):\n",
    "        if part is not None:\n",
    "            parts.append(part)\n",
    "df = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame(columns=['SWNO','VOLTAGE','VALUE','MONTH','TS'])\n",
    "\n",
    "def pivot_metric(metric, tag):\n",
    "    p = metric.pivot(index=[\"SWNO\", \"VOLTAGE\"], columns=\"MONTH\", values=tag)\n",
    "    p = p.reindex(columns=range(1, 13), fill_value=np.nan)\n",
    "    p.columns = mcols(tag)\n",
    "    return p\n",
    "\n",
    "if not df.empty:\n",
    "    pI = (df.groupby([\"SWNO\",\"VOLTAGE\",\"MONTH\"], observed=True)[\"VALUE\"]\n",
    "            .mean().unstack().reindex(columns=range(1,13)).fillna(np.nan))\n",
    "    pI.columns = mcols(\"I\")\n",
    "    i_df = df.copy()\n",
    "    i_df[\"DATE\"] = i_df[\"TS\"].dt.date\n",
    "\n",
    "    pI_long = pI.reset_index().melt(id_vars=[\"SWNO\",\"VOLTAGE\"], var_name=\"MONTH_COL\", value_name=\"AVG\")\n",
    "    pI_long[\"MONTH\"] = pI_long[\"MONTH_COL\"].str.extract(r\"(\\d+)$\").astype(int)\n",
    "    i_df = i_df.merge(pI_long.drop(columns=\"MONTH_COL\"), on=[\"SWNO\", \"VOLTAGE\", \"MONTH\"], how=\"left\")\n",
    "\n",
    "    peak = i_df.groupby([\"SWNO\", \"VOLTAGE\", \"MONTH\"], observed=True)[\"VALUE\"].max().reset_index(name=\"PEAK\")\n",
    "\n",
    "    day_max = i_df.groupby([\"SWNO\", \"VOLTAGE\", \"DATE\"], observed=True)[\"VALUE\"].max().reset_index()\n",
    "    day_max.sort_values([\"SWNO\", \"VOLTAGE\", \"DATE\"], inplace=True)\n",
    "    day_max[\"PREV\"] = day_max.groupby([\"SWNO\", \"VOLTAGE\"])[\"VALUE\"].shift()\n",
    "    day_max[\"MONTH\"] = pd.to_datetime(day_max[\"DATE\"]).dt.month.astype(\"Int8\")\n",
    "    day_max = day_max.merge(pI_long.drop(columns=\"MONTH_COL\"), on=[\"SWNO\", \"VOLTAGE\", \"MONTH\"], how=\"left\")\n",
    "    day_max[\"RAMP\"] = ((day_max[\"PREV\"] < day_max[\"AVG\"]) & (day_max[\"VALUE\"] >= day_max[\"AVG\"])).astype(\"int8\")\n",
    "    cycle = day_max.groupby([\"SWNO\", \"VOLTAGE\", \"MONTH\"], observed=True)[\"RAMP\"].sum().reset_index(name=\"CYCLE\")\n",
    "\n",
    "    i_df[\"OVR\"] = (i_df[\"VALUE\"] > 2 * i_df[\"AVG\"]).astype(\"int8\")\n",
    "    ovr = i_df.groupby([\"SWNO\", \"VOLTAGE\", \"MONTH\"], observed=True)[\"OVR\"].sum().reset_index(name=\"OVR\")\n",
    "\n",
    "    pPEAK = pivot_metric(peak, \"PEAK\")\n",
    "    pCYCLE = pivot_metric(cycle, \"CYCLE\")\n",
    "    pOVR = pivot_metric(ovr, \"OVR\")\n",
    "\n",
    "    sc_wide = pI.join([pPEAK, pCYCLE, pOVR]).reset_index()\n",
    "else:\n",
    "    sc_wide = pd.DataFrame()\n",
    "\n",
    "sc_wide.to_csv(SCADA_OUT, index=False, float_format=\"%.3f\")\n",
    "print(f\"Saved: {SCADA_OUT} | Rows: {len(sc_wide)} | Columns: {len(sc_wide.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b1762",
   "metadata": {},
   "source": [
    "AFTER THE SCADA FILE USED ATO THE AGRREGATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89d829fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SOURCE_SWITCH_ID DESTINATION_SWITCH_ID  MEASUREDLENGTH  \\\n",
      "0             1210                 33010      302.700000   \n",
      "1             1213                 33018      313.200000   \n",
      "2        220AAR102                 33008     7060.300000   \n",
      "3        220AAR103                 33035       29.563510   \n",
      "4        220AAR103                 33090      302.000000   \n",
      "5        220AAR103                 33302     2003.700000   \n",
      "6        220AAR108                 33246     4087.877981   \n",
      "7        220AAR109                 33150     6103.711252   \n",
      "8        220AAR112                 33004     1054.200000   \n",
      "9         220AAR32                 33097     4272.860093   \n",
      "\n",
      "                                                PATH CABLETYPE NUMBEROFCORES  \\\n",
      "0  POISAR REC-STN TO G.O.D.-1210 (JTNO.01 TO JTNO...      XLPE           3.0   \n",
      "1  POISAR REC-STN_33018 TO G.O.D.-1213 (JTNO.01A ...      XLPE           3.0   \n",
      "2  JT.NO.01 â†’ SWNO_33008 -> JT.NO.02 â†’ JT.NO.01 -...      XLPE           3.0   \n",
      "3                                         SWNO_33035      XLPE           1.0   \n",
      "4                       SWNO_220AAR103 -> SWNO_33090      XLPE           3.0   \n",
      "5  JT.NO.01 â†’ JT.NO.02 -> JT.NO.02 â†’ JT.NO.03 -> ...      XLPE           3.0   \n",
      "6  JT.NO.01 â†’ JT.NO.02 -> JT.NO.03 â†’ JT.NO.04 -> ...      XLPE           3.0   \n",
      "7  JT.NO.01 â†’ JT.NO.02 -> JT.NO.02 â†’ JT.NO.03 -> ...      XLPE           3.0   \n",
      "8  JT.NO.01 â†’ JT.NO.02A -> JT.NO.01 â†’ JT.NO.02B -...      XLPE           3.0   \n",
      "9  JT.NO.01 â†’ JT.NO.01A -> JT.NO.01A â†’ SWNO_33097...      XLPE           3.0   \n",
      "\n",
      "  ARMOURED NEUTRALMATERIAL           CABLEID CABLECONDUCTORMATERIAL  ...  \\\n",
      "0        Y              AL  HT33KV6908556121                     AL  ...   \n",
      "1        Y              AL  HT33KV6908556118                     AL  ...   \n",
      "2        Y              AL  HT33KV6908561OEW                     AL  ...   \n",
      "3        Y              AL              None                     AL  ...   \n",
      "4        Y              AL  HT33KV6908561OEW                     AL  ...   \n",
      "5        Y              AL              None                     AL  ...   \n",
      "6        Y              AL  HT33KV6908561OEW                     AL  ...   \n",
      "7        Y              AL            <Null>                     AL  ...   \n",
      "8        Y              AL  HT33KV6908561OEW                     AL  ...   \n",
      "9        Y              AL              None                     AL  ...   \n",
      "\n",
      "     FAULT_FROM_SWITCH FAULT_TO_SWITCH FAULT_VOLTAGE   FAULT_DIVISION  \\\n",
      "0                                                                       \n",
      "1                                                                       \n",
      "2            220AAR102           33008          33KV           VANDRE   \n",
      "3            220AAR103           33302          33KV          ANDHERI   \n",
      "4            220AAR103           33302          33KV          ANDHERI   \n",
      "5            220AAR103           33302          33KV          ANDHERI   \n",
      "6            220AAR108           33246          33KV          ANDHERI   \n",
      "7            220AAR109           33150          33KV            POWAI   \n",
      "8                                                                       \n",
      "9  220AAR32 | 220AAR38           33097          33KV  CHEMBUR | POWAI   \n",
      "\n",
      "  FAULT_TIME_DIFFERENCE_HOURS_AVG  FAULT_LATEST_OUTAGE_TIME  \\\n",
      "0                             NaN                       NaT   \n",
      "1                             NaN                       NaT   \n",
      "2                        6.484722 2021-11-09 12:11:11+00:00   \n",
      "3                        1.970556 2022-06-14 20:16:00+00:00   \n",
      "4                        1.970556 2022-06-14 20:16:00+00:00   \n",
      "5                        1.970556 2022-06-14 20:16:00+00:00   \n",
      "6                        0.025000 2023-03-17 18:38:00+00:00   \n",
      "7                        0.716944 2022-07-06 03:54:00+00:00   \n",
      "8                             NaN                       NaT   \n",
      "9                        0.427346 2025-02-20 11:08:00+00:00   \n",
      "\n",
      "   IR_Measurement_MOhm Tan_Delta Partial_Discharge_Frequency  \\\n",
      "0                                                              \n",
      "1                                                              \n",
      "2                                                              \n",
      "3                                                              \n",
      "4                                                              \n",
      "5                                                              \n",
      "6                                                              \n",
      "7                                                              \n",
      "8                                                              \n",
      "9                                                              \n",
      "\n",
      "  Partial_Discharge_Intensity  \n",
      "0                              \n",
      "1                              \n",
      "2                              \n",
      "3                              \n",
      "4                              \n",
      "5                              \n",
      "6                              \n",
      "7                              \n",
      "8                              \n",
      "9                              \n",
      "\n",
      "[10 rows x 79 columns]\n",
      "RangeIndex(start=0, stop=10, step=1)\n",
      "Saved: /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/SWNO_MASTER_COMBINED_FULL_FINAL3.csv | Rows: 318 | Columns: 79\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd, numpy as np, re\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = \"/media/sagark24/New Volume/MERGE CDIS\"\n",
    "FILE_SWNO   = Path(f\"{BASE}/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv\")\n",
    "FILE_CABLE  = Path(f\"{BASE}/2-Year-data/CLEANED_DATA/ht_cleaned.csv\")\n",
    "FAULT_FILE  = Path(f\"{BASE}/IPYNB_FILE/DATA_GENERATION/HT_fault_cable_info_processed_without_affected.csv\")\n",
    "SCADA_OUT   = Path(f\"{BASE}/IPYNB_FILE/DATA_GENERATION/SCADA_WIDE_ONLY.csv\")\n",
    "FINAL_OUT   = Path(f\"{BASE}/IPYNB_FILE/DATA_GENERATION/SWNO_MASTER_COMBINED_FULL_FINAL3.csv\")\n",
    "\n",
    "def norm(x): return np.nan if pd.isna(x) else str(x).strip().upper().lstrip(\"0\")\n",
    "def extract_ordered_path(comment):\n",
    "    if not comment or not isinstance(comment, str): return \"\"\n",
    "    comment_up = comment.upper()\n",
    "    matches = re.findall(r'(SWNO_\\w+|JT\\.NO\\.\\d+[A-Z]?)', comment_up)\n",
    "    if matches: return \" â†’ \".join(matches)\n",
    "    comment_strip = comment.strip()\n",
    "    return comment_strip if comment_strip else \"\"\n",
    "\n",
    "def drop_all_empty_strict(df):\n",
    "    \"\"\"Drop rows where all values are empty, NaN, '0', or 'nan' (as string, case-insensitive).\"\"\"\n",
    "    def is_empty_val(val):\n",
    "        s = str(val).strip().lower()\n",
    "        return s in [\"\", \"0\", \"nan\"] or pd.isna(val)\n",
    "    mask = df.apply(lambda row: all(is_empty_val(x) for x in row), axis=1)\n",
    "    return df.loc[~mask].reset_index(drop=True)\n",
    "\n",
    "# 1. SWNO master\n",
    "swno_master = pd.read_csv(FILE_SWNO, dtype=str)\n",
    "swno_master = drop_all_empty_strict(swno_master)\n",
    "swno_master[\"SWNO\"] = swno_master[\"SWNO\"].apply(norm)\n",
    "swno_master = drop_all_empty_strict(swno_master)\n",
    "SWNO_SET = set(swno_master[\"SWNO\"])\n",
    "\n",
    "# 2. Cable\n",
    "cable = pd.read_csv(FILE_CABLE, dtype=str)\n",
    "cable = drop_all_empty_strict(cable)\n",
    "cable.rename(columns={\n",
    "    \"CABEL_ID\":\"CABLEID\",\"CABLECOUNTUCTOR\":\"CABLECONDUCTORMATERIAL\",\n",
    "    \"NO.OFCORES\":\"NUMBEROFCORES\",\"NEUTRAL_MATERIAL\":\"NEUTRALMATERIAL\",\n",
    "    \"coments\":\"COMMENTS\"}, inplace=True)\n",
    "cable[\"DESTINATION_SWITCH_ID\"] = cable[\"DESTINATION_SWITCH_ID\"].apply(norm)\n",
    "cable[\"SOURCE_SWITCH_ID\"] = cable[\"SOURCE_SWITCH_ID\"].apply(norm)\n",
    "\n",
    "# Remove rows where either switch id is empty/blank/NaN\n",
    "cable = cable[\n",
    "    cable[\"SOURCE_SWITCH_ID\"].notna() & cable[\"DESTINATION_SWITCH_ID\"].notna()\n",
    "]\n",
    "cable = cable[\n",
    "    (cable[\"SOURCE_SWITCH_ID\"].str.strip() != \"\") &\n",
    "    (cable[\"DESTINATION_SWITCH_ID\"].str.strip() != \"\")\n",
    "]\n",
    "cable = drop_all_empty_strict(cable)\n",
    "\n",
    "cable = cable[cable[\"DESTINATION_SWITCH_ID\"].isin(SWNO_SET)]\n",
    "KEEP = [\"DESTINATION_SWITCH_ID\",\"COMMENTS\",\"CABLETYPE\",\"MEASUREDLENGTH\",\n",
    "        \"NUMBEROFCORES\",\"ARMOURED\",\"NEUTRALMATERIAL\",\"CABLEID\",\n",
    "        \"CABLECONDUCTORMATERIAL\",\"DIVISIONCODE\",\"ZONECODE\",\"REMARKS\",\n",
    "        \"SOURCE_SWITCH_ID\",\"SOURCE_SS\",\"DESTINATION_SS\",\"DATECREATED\"]\n",
    "cable = cable[[c for c in KEEP if c in cable.columns]]\n",
    "cable[\"COMMENTS\"] = cable[\"COMMENTS\"].fillna(\"\")\n",
    "cable[\"SWNO\"] = cable[\"DESTINATION_SWITCH_ID\"]\n",
    "\n",
    "missing = swno_master[~swno_master[\"SWNO\"].isin(cable[\"SWNO\"])]\n",
    "if not missing.empty:\n",
    "    pad = {col:\"\" for col in cable.columns if col!=\"SWNO\"}\n",
    "    cable = pd.concat([cable, missing.assign(**pad)], ignore_index=True)\n",
    "cable = drop_all_empty_strict(cable)\n",
    "\n",
    "cable[\"PATH\"] = cable[\"COMMENTS\"].apply(extract_ordered_path)\n",
    "\n",
    "# === NO_OF_SEGMENT: Count non-empty comments for each (SRC, DST) pair BEFORE grouping ===\n",
    "segment_comment_counts = (\n",
    "    cable\n",
    "    .assign(COMMENTS=cable[\"COMMENTS\"].fillna(\"\").astype(str).str.strip())\n",
    "    .groupby([\"SOURCE_SWITCH_ID\", \"DESTINATION_SWITCH_ID\"], dropna=False)[\"COMMENTS\"]\n",
    "    .apply(lambda x: (x != \"\").sum())\n",
    "    .reset_index()\n",
    "    .rename(columns={\"COMMENTS\": \"NO_OF_SEGMENT\"})\n",
    ")\n",
    "\n",
    "# === Deduplicate by SOURCE/DESTINATION (main groupby) ===\n",
    "cable[\"MEASUREDLENGTH\"] = pd.to_numeric(cable[\"MEASUREDLENGTH\"], errors=\"coerce\")\n",
    "cable = (cable\n",
    "         .groupby([\"SOURCE_SWITCH_ID\", \"DESTINATION_SWITCH_ID\"], dropna=False)\n",
    "         .agg({\n",
    "             \"MEASUREDLENGTH\": \"sum\",\n",
    "             \"PATH\": lambda x: \" -> \".join(sorted(set(x))),\n",
    "             \"CABLETYPE\": \"first\",\n",
    "             \"NUMBEROFCORES\": \"first\",\n",
    "             \"ARMOURED\": \"first\",\n",
    "             \"NEUTRALMATERIAL\": \"first\",\n",
    "             \"CABLEID\": \"first\",\n",
    "             \"CABLECONDUCTORMATERIAL\": \"first\",\n",
    "             \"DIVISIONCODE\": \"first\",\n",
    "             \"ZONECODE\": \"first\",\n",
    "             \"REMARKS\": \"first\",\n",
    "             \"SOURCE_SS\": \"first\",\n",
    "             \"DESTINATION_SS\": \"first\",\n",
    "             \"DATECREATED\": \"first\",\n",
    "         }).reset_index())\n",
    "cable = drop_all_empty_strict(cable)\n",
    "\n",
    "# === Merge in the real NO_OF_SEGMENT ===\n",
    "cable = cable.merge(segment_comment_counts, on=[\"SOURCE_SWITCH_ID\", \"DESTINATION_SWITCH_ID\"], how=\"left\")\n",
    "cable[\"SWNO\"] = cable[\"DESTINATION_SWITCH_ID\"]  # For SCADA merge\n",
    "\n",
    "# 3. Faults\n",
    "fault = pd.read_csv(FAULT_FILE, dtype=str)\n",
    "fault = drop_all_empty_strict(fault)\n",
    "fault[\"SWNO\"] = fault[\"SWITCH_NO\"].apply(norm)\n",
    "fault = drop_all_empty_strict(fault)\n",
    "\n",
    "# Only use SOURCE_SWITCH_ID for matching\n",
    "SOURCE_SWITCH_SET = set(norm(x) for x in cable[\"SOURCE_SWITCH_ID\"].dropna().unique())\n",
    "fault = fault[fault[\"SWNO\"].isin(SOURCE_SWITCH_SET)]\n",
    "fault = drop_all_empty_strict(fault)\n",
    "\n",
    "# Drop unwanted columns from fault if present\n",
    "fault.drop(columns=[col for col in [\"SOURCE_SS\", \"DESTINATION_SS\"] if col in fault.columns], inplace=True)\n",
    "\n",
    "fault_cols = [\n",
    "   \"TIME_OUTAGE\", \"FROM_SWITCH\",\"TO_SWITCH\",\"VOLTAGE\",\"TIME_DIFFERENCE_HOURS\",\n",
    "  \"DIVISION\"\n",
    "]\n",
    "fault.rename(columns={c: f\"FAULT_{c}\" for c in fault_cols}, inplace=True)\n",
    "FAULT_COLS = [f\"FAULT_{c}\" for c in fault_cols]\n",
    "\n",
    "# Aggregation: numeric and datetime for new features\n",
    "fault[\"FAULT_TIME_DIFFERENCE_HOURS\"] = pd.to_numeric(fault[\"FAULT_TIME_DIFFERENCE_HOURS\"], errors=\"coerce\")\n",
    "fault[\"FAULT_TIME_OUTAGE\"] = pd.to_datetime(fault[\"FAULT_TIME_OUTAGE\"], errors=\"coerce\")\n",
    "\n",
    "# Fault counts and concatenated fields\n",
    "fault_cnt = fault.groupby(\"SWNO\", observed=True).size().reset_index(name=\"Num_Faults\")\n",
    "fault_agg = fault.groupby(\"SWNO\", observed=True).agg(\n",
    "    lambda s: \" | \".join(str(x) for x in pd.unique(s.dropna()))\n",
    ").reset_index()\n",
    "\n",
    "# Aggregated new features\n",
    "avg_fault_time = fault.groupby(\"SWNO\")[\"FAULT_TIME_DIFFERENCE_HOURS\"].mean().reset_index()\n",
    "avg_fault_time.rename(columns={\"FAULT_TIME_DIFFERENCE_HOURS\": \"FAULT_TIME_DIFFERENCE_HOURS_AVG\"}, inplace=True)\n",
    "latest_outage = fault.groupby(\"SWNO\")[\"FAULT_TIME_OUTAGE\"].max().reset_index()\n",
    "latest_outage.rename(columns={\"FAULT_TIME_OUTAGE\": \"FAULT_LATEST_OUTAGE_TIME\"}, inplace=True)\n",
    "\n",
    "# Merge all fault data features\n",
    "fault_data = fault_cnt.merge(fault_agg, on=\"SWNO\", how=\"left\")\n",
    "fault_data = fault_data.merge(avg_fault_time, on=\"SWNO\", how=\"left\")\n",
    "fault_data = fault_data.merge(latest_outage, on=\"SWNO\", how=\"left\")\n",
    "fault_data.rename(columns={\"SWNO\": \"SOURCE_SWITCH_ID\"}, inplace=True)\n",
    "fault_data = drop_all_empty_strict(fault_data)\n",
    "\n",
    "# 4. SCADA metrics\n",
    "scada_df = pd.read_csv(SCADA_OUT, dtype=str)\n",
    "scada_df = drop_all_empty_strict(scada_df)\n",
    "\n",
    "# 5a. SCADA on DESTINATION_SWITCH_ID (stored in cable[\"SWNO\"])\n",
    "if \"VOLTAGE\" in cable.columns and \"VOLTAGE\" in scada_df.columns:\n",
    "    full = cable.merge(scada_df, on=[\"SWNO\", \"VOLTAGE\"], how=\"left\")\n",
    "else:\n",
    "    full = cable.merge(scada_df, on=\"SWNO\", how=\"left\")\n",
    "full = drop_all_empty_strict(full)\n",
    "\n",
    "# 5b. Faults on SOURCE_SWITCH_ID\n",
    "full = full.merge(fault_data, how=\"left\", on=\"SOURCE_SWITCH_ID\")\n",
    "full = drop_all_empty_strict(full)\n",
    "\n",
    "# Post-merge clean-up\n",
    "full[\"Num_Faults\"] = full[\"Num_Faults\"].fillna(0).astype(\"int16\")\n",
    "for col in FAULT_COLS:\n",
    "    full[col] = full[col].fillna(\"\")\n",
    "for blank in [\"IR_Measurement_MOhm\",\"Tan_Delta\",\n",
    "              \"Partial_Discharge_Frequency\",\"Partial_Discharge_Intensity\"]:\n",
    "    full[blank] = \"\"\n",
    "\n",
    "# Drop unwanted columns\n",
    "cols_to_drop = [\n",
    "    \"STD_CABLE_SIZE\", \"CABLE_TYPE\",\n",
    "    \"RELAY_FUSE_B\", \"RELAY_FUSE_N\", \"RELAY_FUSE_Y\", \"RELAY_FUSE\",\n",
    "    \"AFFECTED_STATION\", \"AFFECTED_SWITCH\", \"REASON_CATEGORY\", \"REASON_TEXT\",\n",
    "    \"STATION_NAME\", \"FAULT_TIME_OUTAGE\", \"FAULT_TIME_DIFFERENCE_HOURS\"\n",
    "]\n",
    "full.drop(columns=[c for c in cols_to_drop if c in full.columns], inplace=True)\n",
    "\n",
    "# FINAL CLEAN: Remove any fully-empty or all-zero-string rows from output\n",
    "full = drop_all_empty_strict(full)\n",
    "\n",
    "# Remove rows where MEASUREDLENGTH is 0 or missing\n",
    "if \"MEASUREDLENGTH\" in full.columns:\n",
    "    # This line handles int, float, or string representations of 0\n",
    "    full = full[full[\"MEASUREDLENGTH\"].notna()]\n",
    "    full = full[pd.to_numeric(full[\"MEASUREDLENGTH\"], errors=\"coerce\") != 0]\n",
    "\n",
    "full = full.reset_index(drop=True)\n",
    "\n",
    "# Optional: Print for quick debug\n",
    "print(full.head(10))\n",
    "print(full.index[:10])\n",
    "\n",
    "# Save\n",
    "full.to_csv(FINAL_OUT, index=False, float_format=\"%.3f\")\n",
    "print(f\"Saved: {FINAL_OUT} | Rows: {len(full)} | Columns: {len(full.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf72d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
