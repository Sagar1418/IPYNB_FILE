{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83880e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cable_ID</th>\n",
       "      <th>Voltage_Level</th>\n",
       "      <th>Feeder_ID</th>\n",
       "      <th>Sub_Feeder_ID</th>\n",
       "      <th>From_Switch</th>\n",
       "      <th>To_Switch</th>\n",
       "      <th>Cable_Type</th>\n",
       "      <th>Cable_Age_Years</th>\n",
       "      <th>Length_m</th>\n",
       "      <th>Installation_Environment</th>\n",
       "      <th>...</th>\n",
       "      <th>Partial_Discharge_Frequency</th>\n",
       "      <th>Partial_Discharge_Intensity</th>\n",
       "      <th>Thermal_History_Excursions</th>\n",
       "      <th>Num_Faults</th>\n",
       "      <th>Fault_Type</th>\n",
       "      <th>Repairs_Count</th>\n",
       "      <th>Joint_History</th>\n",
       "      <th>Corrosivity</th>\n",
       "      <th>Water_Ingress</th>\n",
       "      <th>Remarks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33KV-SW1-SW2</td>\n",
       "      <td>33kV</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>SW1</td>\n",
       "      <td>SW2</td>\n",
       "      <td>XLPE</td>\n",
       "      <td>7</td>\n",
       "      <td>1200</td>\n",
       "      <td>Underground</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>Original</td>\n",
       "      <td>Low</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22KV-SW3-SW4</td>\n",
       "      <td>22kV</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>SW3</td>\n",
       "      <td>SW4</td>\n",
       "      <td>PILC</td>\n",
       "      <td>3</td>\n",
       "      <td>800</td>\n",
       "      <td>Overhead</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Earth Fault</td>\n",
       "      <td>1</td>\n",
       "      <td>Repaired once</td>\n",
       "      <td>Medium</td>\n",
       "      <td>No</td>\n",
       "      <td>Monitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FDR1-DT001</td>\n",
       "      <td>11kV</td>\n",
       "      <td>FDR1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>XLPE</td>\n",
       "      <td>8</td>\n",
       "      <td>400</td>\n",
       "      <td>Underground</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Earth Fault</td>\n",
       "      <td>2</td>\n",
       "      <td>Multiple joints</td>\n",
       "      <td>High</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FDR2-SUB1-DT005</td>\n",
       "      <td>11kV</td>\n",
       "      <td>FDR2</td>\n",
       "      <td>SUB1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>XLPE</td>\n",
       "      <td>4</td>\n",
       "      <td>220</td>\n",
       "      <td>Underground</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>Original</td>\n",
       "      <td>Low</td>\n",
       "      <td>No</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Cable_ID Voltage_Level Feeder_ID Sub_Feeder_ID From_Switch  \\\n",
       "0     33KV-SW1-SW2          33kV                                 SW1   \n",
       "1     22KV-SW3-SW4          22kV                                 SW3   \n",
       "2       FDR1-DT001          11kV      FDR1                             \n",
       "3  FDR2-SUB1-DT005          11kV      FDR2          SUB1               \n",
       "\n",
       "  To_Switch Cable_Type  Cable_Age_Years  Length_m Installation_Environment  \\\n",
       "0       SW2       XLPE                7      1200              Underground   \n",
       "1       SW4       PILC                3       800                 Overhead   \n",
       "2                 XLPE                8       400              Underground   \n",
       "3                 XLPE                4       220              Underground   \n",
       "\n",
       "   ... Partial_Discharge_Frequency Partial_Discharge_Intensity  \\\n",
       "0  ...                           0                           0   \n",
       "1  ...                           0                           0   \n",
       "2  ...                           1                           3   \n",
       "3  ...                           0                           0   \n",
       "\n",
       "  Thermal_History_Excursions  Num_Faults   Fault_Type  Repairs_Count  \\\n",
       "0                          0           0         None              0   \n",
       "1                          1           1  Earth Fault              1   \n",
       "2                          1           2  Earth Fault              2   \n",
       "3                          0           0         None              0   \n",
       "\n",
       "     Joint_History  Corrosivity  Water_Ingress  Remarks  \n",
       "0         Original          Low             No  Healthy  \n",
       "1    Repaired once       Medium             No  Monitor  \n",
       "2  Multiple joints         High            Yes  Healthy  \n",
       "3         Original          Low             No           \n",
       "\n",
       "[4 rows x 29 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = [\n",
    "    \"Cable_ID\",\"Voltage_Level\",\"Feeder_ID\",\"Sub_Feeder_ID\",\"From_Switch\",\"To_Switch\",\n",
    "    \"Cable_Type\",\"Cable_Age_Years\",\"Length_m\",\"Installation_Environment\",\n",
    "    \"Soil_Type\",\"Humidity\",\"Proximity_to_Water\",\"Load_History_Avg_Load\",\"Load_History_Peak_Load\", \n",
    "    \"Loading_Cycles\",\"Overload_Events\",\"IR_Measurement_MOhm\",\"Tan_Delta\",\"Partial_Discharge_Frequency\",\n",
    "    \"Partial_Discharge_Intensity\",\"Thermal_History_Excursions\",\"Num_Faults\",\"Fault_Type\",\"Repairs_Count\",\n",
    "    \"Joint_History\",\"Corrosivity\",\"Water_Ingress\",\"Remarks\"\n",
    "]\n",
    "\n",
    "# Sample data for different voltage levels. You can add or import your actual data here.\n",
    "data = [\n",
    "    [\n",
    "        \"33KV-SW1-SW2\", \"33kV\", \"\", \"\", \"SW1\", \"SW2\", \"XLPE\", 7, 1200, \"Underground\",\n",
    "        \"Sandy\", \"Medium\", \"Far\", 60, 100, 150, 1, 500, 0.001, 0, 0, 0, 0, \"None\", 0, \"Original\", \"Low\", \"No\", \"Healthy\"\n",
    "    ],\n",
    "    [\n",
    "        \"22KV-SW3-SW4\", \"22kV\", \"\", \"\", \"SW3\", \"SW4\", \"PILC\", 3, 800, \"Overhead\",\n",
    "        \"Clay\", \"High\", \"Near\", 50, 90, 100, 0, 400, 0.002, 0, 0, 1, 1, \"Earth Fault\", 1, \"Repaired once\", \"Medium\", \"No\", \"Monitor\"\n",
    "    ],\n",
    "    [\n",
    "        \"FDR1-DT001\", \"11kV\", \"FDR1\", \"\", \"\", \"\", \"XLPE\", 8, 400, \"Underground\",\n",
    "        \"Loam\", \"Medium\", \"Near\", 40, 80, 120, 2, 120, 0.003, 1, 3, 1, 2, \"Earth Fault\", 2, \"Multiple joints\", \"High\", \"Yes\", \"Healthy\"\n",
    "    ],\n",
    "    [\n",
    "        \"FDR2-SUB1-DT005\", \"11kV\", \"FDR2\", \"SUB1\", \"\", \"\", \"XLPE\", 4, 220, \"Underground\",\n",
    "        \"Rocky\", \"Low\", \"Far\", 30, 50, 90, 0, 200, 0.001, 0, 0, 0, 0, \"None\", 0, \"Original\", \"Low\", \"No\", \"\"\n",
    "    ],\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Save as Excel and CSV for use\n",
    "# df.to_excel(\"/media/sagarkumar/New Volume/SAGAR/DATA_GENERATION/master_cable_data_final.xlsx\", index=False)\n",
    "df.to_csv(\"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/master_cable_data_final.csv\", index=False)\n",
    "\n",
    "df.head()  # Display the first few rows of the DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a71682e",
   "metadata": {},
   "source": [
    "FOR 22 AND 33 MASTER DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72a5aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_22_33 = pd.read_csv(\"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/monthly_SWNO_matrix_22KV_33KV.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f491c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = df_22_33['SWNO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "979d3ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_df = pd.DataFrame(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ba358c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_df.to_csv(\"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7bb30b",
   "metadata": {},
   "source": [
    "ENERGYAUDIT VS SWNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8d5f49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv...\n",
      "  - Found 343 unique, clean values.\n",
      "Processing file: /media/sagarkumar/New Volume/SAGAR/2-year-data/ENERGYAUDIT.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_227956/1560782353.py:50: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath, usecols=[actual_col_name], on_bad_lines='skip', encoding=encoding) # <--- CHANGED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Found 9954 unique, clean values.\n",
      "\n",
      "--- Comparison Report ---\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv' (Column: SWNO): 343\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/2-year-data/ENERGYAUDIT.csv' (Column: SWITCH_NO): 9954\n",
      "-------------------------\n",
      "Number of values that matched: 0\n",
      "-------------------------\n",
      "No matching values were found to save.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION: PLEASE EDIT THIS SECTION ---\n",
    "\n",
    "# File 1 Details\n",
    "file1_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv\"\n",
    "file1_column_name = \"SWNO\"\n",
    "# This file processed correctly before, so 'utf-8' is likely correct.\n",
    "file1_encoding = 'utf-8'  # <--- ADDED\n",
    "\n",
    "# File 2 Details\n",
    "file2_path = \"/media/sagarkumar/New Volume/SAGAR/2-year-data/ENERGYAUDIT.csv\"\n",
    "file2_column_name = \"SWITCH_NO\"\n",
    "# This is the file that had an error. 'latin1' is a safe choice.\n",
    "file2_encoding = 'utf-8' # <--- ADDED ('windows-1252' is also a good option)\n",
    "\n",
    "# Output File Path\n",
    "# This file will contain only the values that exist in BOTH files.\n",
    "# Changed to a simpler path. It will save in the same directory you run the script.\n",
    "output_file_path = \"matching_values.csv\" # <--- CHANGED for simplicity\n",
    "\n",
    "\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "\n",
    "def find_actual_column_name(columns, target_name):\n",
    "    \"\"\"Helper function to find a column name, ignoring case.\"\"\"\n",
    "    for col in columns:\n",
    "        if str(col).lower() == str(target_name).lower():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def get_unique_values_from_file(filepath, column_name, encoding): # <--- CHANGED (added encoding)\n",
    "    \"\"\"\n",
    "    Reads a file with a specific encoding, extracts unique values from a\n",
    "    column, cleans them, and returns them as a set.\n",
    "    \"\"\"\n",
    "    print(f\"Processing file: {filepath}...\")\n",
    "    try:\n",
    "        # Read just the header to find the correct column name (case-insensitive)\n",
    "        # Pass the encoding parameter here\n",
    "        header_df = pd.read_csv(filepath, nrows=0, on_bad_lines='skip', encoding=encoding) # <--- CHANGED\n",
    "        actual_col_name = find_actual_column_name(header_df.columns, column_name)\n",
    "\n",
    "        if not actual_col_name:\n",
    "            print(f\"  - Error: Column '{column_name}' not found. Please check the column name.\")\n",
    "            return set()\n",
    "\n",
    "        # Read the full column using the correct name and encoding\n",
    "        df = pd.read_csv(filepath, usecols=[actual_col_name], on_bad_lines='skip', encoding=encoding) # <--- CHANGED\n",
    "\n",
    "        # --- Data Cleaning ---\n",
    "        # Convert all values to string, extract digits, and then convert to numbers.\n",
    "        # This handles mixed data types (e.g., '123' vs 123) and text prefixes (e.g., 'SW-123').\n",
    "        s = pd.Series(df[actual_col_name].dropna().unique(), dtype=str)\n",
    "        s = s.str.extract('(\\d+)').iloc[:, 0]\n",
    "        s = pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "        cleaned_values = set(s.dropna().astype(int))\n",
    "\n",
    "        print(f\"  - Found {len(cleaned_values)} unique, clean values.\")\n",
    "        return cleaned_values\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  - Error: File not found. Please check the path: {filepath}\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"  - An unexpected error occurred: {e}\")\n",
    "        return set()\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# Get the unique values from both files, passing the encoding for each\n",
    "unique_values_from_file1 = get_unique_values_from_file(file1_path, file1_column_name, file1_encoding) # <--- CHANGED\n",
    "unique_values_from_file2 = get_unique_values_from_file(file2_path, file2_column_name, file2_encoding) # <--- CHANGED\n",
    "\n",
    "# --- Comparison and Reporting ---\n",
    "\n",
    "print(\"\\n--- Comparison Report ---\")\n",
    "if not unique_values_from_file1 or not unique_values_from_file2:\n",
    "    print(\"Could not perform comparison because one of the files could not be processed or contained no valid data.\")\n",
    "else:\n",
    "    # Use set intersection to find the values that exist in both sets\n",
    "    matching_values = unique_values_from_file1.intersection(unique_values_from_file2)\n",
    "\n",
    "    # Print the final report\n",
    "    print(f\"Unique values in '{file1_path}' (Column: {file1_column_name}): {len(unique_values_from_file1)}\")\n",
    "    print(f\"Unique values in '{file2_path}' (Column: {file2_column_name}): {len(unique_values_from_file2)}\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"Number of values that matched: {len(matching_values)}\")\n",
    "    print(\"-\" * 25)\n",
    "\n",
    "    # --- Save the results to the output file --- # <--- ADDED SECTION\n",
    "    if matching_values:\n",
    "        # Convert the set of matching values to a DataFrame\n",
    "        matching_df = pd.DataFrame(sorted(list(matching_values)), columns=['Matching_Switch_Numbers'])\n",
    "        # Save the DataFrame to a CSV file\n",
    "      \n",
    "        print(f\"Success! Matching values have been saved to: {output_file_path}\")\n",
    "    else:\n",
    "        print(\"No matching values were found to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54d5df0",
   "metadata": {},
   "source": [
    "FEEDERDETAILS VS SWNO BUT WRONG MATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b23f6a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv...\n",
      "  - Found 343 unique, clean values.\n",
      "Processing file: /media/sagarkumar/New Volume/SAGAR/2-year-data/FEEDERDETAILS.csv...\n",
      "  - Found 69702 unique, clean values.\n",
      "\n",
      "--- Comparison Report ---\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv' (Column: SWNO): 343\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/2-year-data/FEEDERDETAILS.csv' (Column: ID): 69702\n",
      "-------------------------\n",
      "Number of values that matched: 326\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION: PLEASE EDIT THIS SECTION ---\n",
    "\n",
    "# File 1 Details\n",
    "file1_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv\"\n",
    "file1_column_name = \"SWNO\" \n",
    "\n",
    "# File 2 Details\n",
    "file2_path = \"/media/sagarkumar/New Volume/SAGAR/2-year-data/FEEDERDETAILS.csv\"\n",
    "\n",
    "file2_column_name = \"ID\" \n",
    "\n",
    "# Output File Path\n",
    "# This file will contain only the values that exist in BOTH files.\n",
    "output_file_path = \"/path/to/your/output/matching_values.csv\"\n",
    "\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "\n",
    "def find_actual_column_name(columns, target_name):\n",
    "    \"\"\"Helper function to find a column name, ignoring case.\"\"\"\n",
    "    for col in columns:\n",
    "        if str(col).lower() == str(target_name).lower():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def get_unique_values_from_file(filepath, column_name):\n",
    "    \"\"\"\n",
    "    Reads a file, extracts unique values from a specific column,\n",
    "    cleans them, and returns them as a set.\n",
    "    \"\"\"\n",
    "    print(f\"Processing file: {filepath}...\")\n",
    "    try:\n",
    "        # Read just the header to find the correct column name (case-insensitive)\n",
    "        header_df = pd.read_csv(filepath, nrows=0, on_bad_lines='skip')\n",
    "        actual_col_name = find_actual_column_name(header_df.columns, column_name)\n",
    "\n",
    "        if not actual_col_name:\n",
    "            print(f\"  - Error: Column '{column_name}' not found. Please check the column name.\")\n",
    "            return set()\n",
    "\n",
    "        # Read the full column using the correct name\n",
    "        df = pd.read_csv(filepath, usecols=[actual_col_name], on_bad_lines='skip')\n",
    "        \n",
    "        # --- Data Cleaning ---\n",
    "        # Convert all values to string, extract digits, and then convert to numbers.\n",
    "        # This handles mixed data types (e.g., '123' vs 123) and text prefixes (e.g., 'SW-123').\n",
    "        s = pd.Series(df[actual_col_name].dropna().unique(), dtype=str)\n",
    "        s = s.str.extract('(\\d+)').iloc[:, 0]\n",
    "        s = pd.to_numeric(s, errors='coerce')\n",
    "        \n",
    "        cleaned_values = set(s.dropna().astype(int))\n",
    "        \n",
    "        print(f\"  - Found {len(cleaned_values)} unique, clean values.\")\n",
    "        return cleaned_values\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  - Error: File not found. Please check the path: {filepath}\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"  - An unexpected error occurred: {e}\")\n",
    "        return set()\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# Get the unique values from both files\n",
    "unique_values_from_file1 = get_unique_values_from_file(file1_path, file1_column_name)\n",
    "unique_values_from_file2 = get_unique_values_from_file(file2_path, file2_column_name)\n",
    "\n",
    "# --- Comparison and Reporting ---\n",
    "\n",
    "print(\"\\n--- Comparison Report ---\")\n",
    "if not unique_values_from_file1 or not unique_values_from_file2:\n",
    "    print(\"Could not perform comparison because one of the files could not be processed or contained no valid data.\")\n",
    "else:\n",
    "    # Use set intersection to find the values that exist in both sets\n",
    "    matching_values = unique_values_from_file1.intersection(unique_values_from_file2)\n",
    "    \n",
    "    # Print the final report\n",
    "    print(f\"Unique values in '{file1_path}' (Column: {file1_column_name}): {len(unique_values_from_file1)}\")\n",
    "    print(f\"Unique values in '{file2_path}' (Column: {file2_column_name}): {len(unique_values_from_file2)}\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"Number of values that matched: {len(matching_values)}\")\n",
    "    print(\"-\" * 25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d658d7",
   "metadata": {},
   "source": [
    "SWNO VS HTCABLE CLEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56b34ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv...\n",
      "  - Found 343 unique, clean values.\n",
      "Processing file: /media/sagarkumar/New Volume/SAGAR/2-year-data/HTCABLE_Clean.csv...\n",
      "  - Found 10244 unique, clean values.\n",
      "\n",
      "--- Comparison Report ---\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv' (Column: SWNO): 343\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/2-year-data/HTCABLE_Clean.csv' (Column: DESTINATION_SWITCH_ID): 10244\n",
      "-------------------------\n",
      "Number of values that matched: 258\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION: PLEASE EDIT THIS SECTION ---\n",
    "\n",
    "# File 1 Details\n",
    "file1_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv\"\n",
    "file1_column_name = \"SWNO\" \n",
    "\n",
    "# File 2 Details\n",
    "file2_path = \"/media/sagarkumar/New Volume/SAGAR/2-year-data/HTCABLE_Clean.csv\"\n",
    "\n",
    "file2_column_name = \"DESTINATION_SWITCH_ID\" \n",
    "\n",
    "# Output File Path\n",
    "# This file will contain only the values that exist in BOTH files.\n",
    "output_file_path = \"/path/to/your/output/matching_values.csv\"\n",
    "\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "\n",
    "def find_actual_column_name(columns, target_name):\n",
    "    \"\"\"Helper function to find a column name, ignoring case.\"\"\"\n",
    "    for col in columns:\n",
    "        if str(col).lower() == str(target_name).lower():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def get_unique_values_from_file(filepath, column_name):\n",
    "    \"\"\"\n",
    "    Reads a file, extracts unique values from a specific column,\n",
    "    cleans them, and returns them as a set.\n",
    "    \"\"\"\n",
    "    print(f\"Processing file: {filepath}...\")\n",
    "    try:\n",
    "        # Read just the header to find the correct column name (case-insensitive)\n",
    "        header_df = pd.read_csv(filepath, nrows=0, on_bad_lines='skip')\n",
    "        actual_col_name = find_actual_column_name(header_df.columns, column_name)\n",
    "\n",
    "        if not actual_col_name:\n",
    "            print(f\"  - Error: Column '{column_name}' not found. Please check the column name.\")\n",
    "            return set()\n",
    "\n",
    "        # Read the full column using the correct name\n",
    "        df = pd.read_csv(filepath, usecols=[actual_col_name], on_bad_lines='skip')\n",
    "        \n",
    "        # --- Data Cleaning ---\n",
    "        # Convert all values to string, extract digits, and then convert to numbers.\n",
    "        # This handles mixed data types (e.g., '123' vs 123) and text prefixes (e.g., 'SW-123').\n",
    "        s = pd.Series(df[actual_col_name].dropna().unique(), dtype=str)\n",
    "        s = s.str.extract('(\\d+)').iloc[:, 0]\n",
    "        s = pd.to_numeric(s, errors='coerce')\n",
    "        \n",
    "        cleaned_values = set(s.dropna().astype(int))\n",
    "        \n",
    "        print(f\"  - Found {len(cleaned_values)} unique, clean values.\")\n",
    "        return cleaned_values\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  - Error: File not found. Please check the path: {filepath}\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"  - An unexpected error occurred: {e}\")\n",
    "        return set()\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# Get the unique values from both files\n",
    "unique_values_from_file1 = get_unique_values_from_file(file1_path, file1_column_name)\n",
    "unique_values_from_file2 = get_unique_values_from_file(file2_path, file2_column_name)\n",
    "\n",
    "# --- Comparison and Reporting ---\n",
    "\n",
    "print(\"\\n--- Comparison Report ---\")\n",
    "if not unique_values_from_file1 or not unique_values_from_file2:\n",
    "    print(\"Could not perform comparison because one of the files could not be processed or contained no valid data.\")\n",
    "else:\n",
    "    # Use set intersection to find the values that exist in both sets\n",
    "    matching_values = unique_values_from_file1.intersection(unique_values_from_file2)\n",
    "    \n",
    "    # Print the final report\n",
    "    print(f\"Unique values in '{file1_path}' (Column: {file1_column_name}): {len(unique_values_from_file1)}\")\n",
    "    print(f\"Unique values in '{file2_path}' (Column: {file2_column_name}): {len(unique_values_from_file2)}\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"Number of values that matched: {len(matching_values)}\")\n",
    "    print(\"-\" * 25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b74ac9",
   "metadata": {},
   "source": [
    "SWNO VS NETWORKDETAILS BUT TO THE SERIAL NO MAY BE WRONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3a66ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv...\n",
      "  - Found 343 unique, clean values.\n",
      "Processing file: /media/sagarkumar/New Volume/SAGAR/2-year-data/NETWORKDETAILS.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_227956/2537703470.py:44: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath, usecols=[actual_col_name], on_bad_lines='skip')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Found 52927 unique, clean values.\n",
      "\n",
      "--- Comparison Report ---\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv' (Column: SWNO): 343\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/2-year-data/NETWORKDETAILS.csv' (Column: SERIAL_NO): 52927\n",
      "-------------------------\n",
      "Number of values that matched: 250\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION: PLEASE EDIT THIS SECTION ---\n",
    "\n",
    "# File 1 Details\n",
    "file1_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv\"\n",
    "file1_column_name = \"SWNO\" \n",
    "\n",
    "# File 2 Details\n",
    "file2_path = \"/media/sagarkumar/New Volume/SAGAR/2-year-data/NETWORKDETAILS.csv\"\n",
    "\n",
    "file2_column_name = \"SERIAL_NO\" \n",
    "\n",
    "# Output File Path\n",
    "# This file will contain only the values that exist in BOTH files.\n",
    "output_file_path = \"/path/to/your/output/matching_values.csv\"\n",
    "\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "\n",
    "def find_actual_column_name(columns, target_name):\n",
    "    \"\"\"Helper function to find a column name, ignoring case.\"\"\"\n",
    "    for col in columns:\n",
    "        if str(col).lower() == str(target_name).lower():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def get_unique_values_from_file(filepath, column_name):\n",
    "    \"\"\"\n",
    "    Reads a file, extracts unique values from a specific column,\n",
    "    cleans them, and returns them as a set.\n",
    "    \"\"\"\n",
    "    print(f\"Processing file: {filepath}...\")\n",
    "    try:\n",
    "        # Read just the header to find the correct column name (case-insensitive)\n",
    "        header_df = pd.read_csv(filepath, nrows=0, on_bad_lines='skip')\n",
    "        actual_col_name = find_actual_column_name(header_df.columns, column_name)\n",
    "\n",
    "        if not actual_col_name:\n",
    "            print(f\"  - Error: Column '{column_name}' not found. Please check the column name.\")\n",
    "            return set()\n",
    "\n",
    "        # Read the full column using the correct name\n",
    "        df = pd.read_csv(filepath, usecols=[actual_col_name], on_bad_lines='skip')\n",
    "        \n",
    "        # --- Data Cleaning ---\n",
    "        # Convert all values to string, extract digits, and then convert to numbers.\n",
    "        # This handles mixed data types (e.g., '123' vs 123) and text prefixes (e.g., 'SW-123').\n",
    "        s = pd.Series(df[actual_col_name].dropna().unique(), dtype=str)\n",
    "        s = s.str.extract('(\\d+)').iloc[:, 0]\n",
    "        s = pd.to_numeric(s, errors='coerce')\n",
    "        \n",
    "        cleaned_values = set(s.dropna().astype(int))\n",
    "        \n",
    "        print(f\"  - Found {len(cleaned_values)} unique, clean values.\")\n",
    "        return cleaned_values\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  - Error: File not found. Please check the path: {filepath}\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"  - An unexpected error occurred: {e}\")\n",
    "        return set()\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# Get the unique values from both files\n",
    "unique_values_from_file1 = get_unique_values_from_file(file1_path, file1_column_name)\n",
    "unique_values_from_file2 = get_unique_values_from_file(file2_path, file2_column_name)\n",
    "\n",
    "# --- Comparison and Reporting ---\n",
    "\n",
    "print(\"\\n--- Comparison Report ---\")\n",
    "if not unique_values_from_file1 or not unique_values_from_file2:\n",
    "    print(\"Could not perform comparison because one of the files could not be processed or contained no valid data.\")\n",
    "else:\n",
    "    # Use set intersection to find the values that exist in both sets\n",
    "    matching_values = unique_values_from_file1.intersection(unique_values_from_file2)\n",
    "    \n",
    "    # Print the final report\n",
    "    print(f\"Unique values in '{file1_path}' (Column: {file1_column_name}): {len(unique_values_from_file1)}\")\n",
    "    print(f\"Unique values in '{file2_path}' (Column: {file2_column_name}): {len(unique_values_from_file2)}\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"Number of values that matched: {len(matching_values)}\")\n",
    "    print(\"-\" * 25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1381a135",
   "metadata": {},
   "source": [
    "SWNO VS TXN_JOINT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5fc01364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv...\n",
      "  - Found 343 unique, clean values.\n",
      "Processing file: //media/sagarkumar/New Volume/SAGAR/2-year-data/TXN_JOINT.csv...\n",
      "  - Found 6924 unique, clean values.\n",
      "\n",
      "--- Comparison Report ---\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv' (Column: SWNO): 343\n",
      "Unique values in '//media/sagarkumar/New Volume/SAGAR/2-year-data/TXN_JOINT.csv' (Column: DESTINATION_SWITCH_ID): 6924\n",
      "-------------------------\n",
      "Number of values that matched: 233\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION: PLEASE EDIT THIS SECTION ---\n",
    "\n",
    "# File 1 Details\n",
    "file1_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv\"\n",
    "file1_column_name = \"SWNO\" \n",
    "\n",
    "# File 2 Details\n",
    "file2_path = \"//media/sagarkumar/New Volume/SAGAR/2-year-data/TXN_JOINT.csv\"\n",
    "\n",
    "file2_column_name = \"DESTINATION_SWITCH_ID\" \n",
    "\n",
    "# Output File Path\n",
    "# This file will contain only the values that exist in BOTH files.\n",
    "output_file_path = \"/path/to/your/output/matching_values.csv\"\n",
    "\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "\n",
    "def find_actual_column_name(columns, target_name):\n",
    "    \"\"\"Helper function to find a column name, ignoring case.\"\"\"\n",
    "    for col in columns:\n",
    "        if str(col).lower() == str(target_name).lower():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def get_unique_values_from_file(filepath, column_name):\n",
    "    \"\"\"\n",
    "    Reads a file, extracts unique values from a specific column,\n",
    "    cleans them, and returns them as a set.\n",
    "    \"\"\"\n",
    "    print(f\"Processing file: {filepath}...\")\n",
    "    try:\n",
    "        # Read just the header to find the correct column name (case-insensitive)\n",
    "        header_df = pd.read_csv(filepath, nrows=0, on_bad_lines='skip')\n",
    "        actual_col_name = find_actual_column_name(header_df.columns, column_name)\n",
    "\n",
    "        if not actual_col_name:\n",
    "            print(f\"  - Error: Column '{column_name}' not found. Please check the column name.\")\n",
    "            return set()\n",
    "\n",
    "        # Read the full column using the correct name\n",
    "        df = pd.read_csv(filepath, usecols=[actual_col_name], on_bad_lines='skip')\n",
    "        \n",
    "        # --- Data Cleaning ---\n",
    "        # Convert all values to string, extract digits, and then convert to numbers.\n",
    "        # This handles mixed data types (e.g., '123' vs 123) and text prefixes (e.g., 'SW-123').\n",
    "        s = pd.Series(df[actual_col_name].dropna().unique(), dtype=str)\n",
    "        s = s.str.extract('(\\d+)').iloc[:, 0]\n",
    "        s = pd.to_numeric(s, errors='coerce')\n",
    "        \n",
    "        cleaned_values = set(s.dropna().astype(int))\n",
    "        \n",
    "        print(f\"  - Found {len(cleaned_values)} unique, clean values.\")\n",
    "        return cleaned_values\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  - Error: File not found. Please check the path: {filepath}\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"  - An unexpected error occurred: {e}\")\n",
    "        return set()\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# Get the unique values from both files\n",
    "unique_values_from_file1 = get_unique_values_from_file(file1_path, file1_column_name)\n",
    "unique_values_from_file2 = get_unique_values_from_file(file2_path, file2_column_name)\n",
    "\n",
    "# --- Comparison and Reporting ---\n",
    "\n",
    "print(\"\\n--- Comparison Report ---\")\n",
    "if not unique_values_from_file1 or not unique_values_from_file2:\n",
    "    print(\"Could not perform comparison because one of the files could not be processed or contained no valid data.\")\n",
    "else:\n",
    "    # Use set intersection to find the values that exist in both sets\n",
    "    matching_values = unique_values_from_file1.intersection(unique_values_from_file2)\n",
    "    \n",
    "    # Print the final report\n",
    "    print(f\"Unique values in '{file1_path}' (Column: {file1_column_name}): {len(unique_values_from_file1)}\")\n",
    "    print(f\"Unique values in '{file2_path}' (Column: {file2_column_name}): {len(unique_values_from_file2)}\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"Number of values that matched: {len(matching_values)}\")\n",
    "    print(\"-\" * 25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fedb0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00af8748",
   "metadata": {},
   "source": [
    "SWNO VS FAULTDAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c210cdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv...\n",
      "  - Found 343 unique, clean values.\n",
      "Processing file: /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/FAULT DATA/processed_fault_data_FINAL.csv...\n",
      "  - Found 4141 unique, clean values.\n",
      "\n",
      "--- Comparison Report ---\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv' (Column: SWNO): 343\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/FAULT DATA/processed_fault_data_FINAL.csv' (Column: FROM_SWITCH): 4141\n",
      "-------------------------\n",
      "Number of values that matched: 0\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION: PLEASE EDIT THIS SECTION ---\n",
    "\n",
    "# File 1 Details\n",
    "file1_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv\"\n",
    "file1_column_name = \"SWNO\" \n",
    "\n",
    "# File 2 Details\n",
    "file2_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/FAULT DATA/processed_fault_data_FINAL.csv\"\n",
    "\n",
    "file2_column_name = \"FROM_SWITCH\" \n",
    "\n",
    "# Output File Path\n",
    "# This file will contain only the values that exist in BOTH files.\n",
    "output_file_path = \"/path/to/your/output/matching_values.csv\"\n",
    "\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "\n",
    "def find_actual_column_name(columns, target_name):\n",
    "    \"\"\"Helper function to find a column name, ignoring case.\"\"\"\n",
    "    for col in columns:\n",
    "        if str(col).lower() == str(target_name).lower():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def get_unique_values_from_file(filepath, column_name):\n",
    "    \"\"\"\n",
    "    Reads a file, extracts unique values from a specific column,\n",
    "    cleans them, and returns them as a set.\n",
    "    \"\"\"\n",
    "    print(f\"Processing file: {filepath}...\")\n",
    "    try:\n",
    "        # Read just the header to find the correct column name (case-insensitive)\n",
    "        header_df = pd.read_csv(filepath, nrows=0, on_bad_lines='skip')\n",
    "        actual_col_name = find_actual_column_name(header_df.columns, column_name)\n",
    "\n",
    "        if not actual_col_name:\n",
    "            print(f\"  - Error: Column '{column_name}' not found. Please check the column name.\")\n",
    "            return set()\n",
    "\n",
    "        # Read the full column using the correct name\n",
    "        df = pd.read_csv(filepath, usecols=[actual_col_name], on_bad_lines='skip')\n",
    "        \n",
    "        # --- Data Cleaning ---\n",
    "        # Convert all values to string, extract digits, and then convert to numbers.\n",
    "        # This handles mixed data types (e.g., '123' vs 123) and text prefixes (e.g., 'SW-123').\n",
    "        s = pd.Series(df[actual_col_name].dropna().unique(), dtype=str)\n",
    "        s = s.str.extract('(\\d+)').iloc[:, 0]\n",
    "        s = pd.to_numeric(s, errors='coerce')\n",
    "        \n",
    "        cleaned_values = set(s.dropna().astype(int))\n",
    "        \n",
    "        print(f\"  - Found {len(cleaned_values)} unique, clean values.\")\n",
    "        return cleaned_values\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  - Error: File not found. Please check the path: {filepath}\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"  - An unexpected error occurred: {e}\")\n",
    "        return set()\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# Get the unique values from both files\n",
    "unique_values_from_file1 = get_unique_values_from_file(file1_path, file1_column_name)\n",
    "unique_values_from_file2 = get_unique_values_from_file(file2_path, file2_column_name)\n",
    "\n",
    "# --- Comparison and Reporting ---\n",
    "\n",
    "print(\"\\n--- Comparison Report ---\")\n",
    "if not unique_values_from_file1 or not unique_values_from_file2:\n",
    "    print(\"Could not perform comparison because one of the files could not be processed or contained no valid data.\")\n",
    "else:\n",
    "    # Use set intersection to find the values that exist in both sets\n",
    "    matching_values = unique_values_from_file1.intersection(unique_values_from_file2)\n",
    "    \n",
    "    # Print the final report\n",
    "    print(f\"Unique values in '{file1_path}' (Column: {file1_column_name}): {len(unique_values_from_file1)}\")\n",
    "    print(f\"Unique values in '{file2_path}' (Column: {file2_column_name}): {len(unique_values_from_file2)}\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"Number of values that matched: {len(matching_values)}\")\n",
    "    print(\"-\" * 25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071b2422",
   "metadata": {},
   "source": [
    "SWWNO VS LOGSHEETHT CABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f09907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv...\n",
      "  - Found 343 unique, clean values.\n",
      "Processing file: /media/sagarkumar/New Volume/SAGAR/2-year-data/TXN_NMS_HTLOGSHEET.csv...\n",
      "  - Found 5145 unique, clean values.\n",
      "\n",
      "--- Comparison Report ---\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv' (Column: SWNO): 343\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/2-year-data/TXN_NMS_HTLOGSHEET.csv' (Column: SWITCH_NO): 5145\n",
      "-------------------------\n",
      "Number of values that matched: 239\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION: PLEASE EDIT THIS SECTION ---\n",
    "\n",
    "# File 1 Details\n",
    "file1_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/22_33_SWNO.csv\"\n",
    "file1_column_name = \"SWNO\" \n",
    "\n",
    "# File 2 Details\n",
    "file2_path = \"/media/sagarkumar/New Volume/SAGAR/2-year-data/TXN_NMS_HTLOGSHEET.csv\"\n",
    "\n",
    "file2_column_name = \"SWITCH_NO\" \n",
    "\n",
    "# Output File Path\n",
    "# This file will contain only the values that exist in BOTH files.\n",
    "output_file_path = \"/path/to/your/output/matching_values.csv\"\n",
    "\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "\n",
    "def find_actual_column_name(columns, target_name):\n",
    "    \"\"\"Helper function to find a column name, ignoring case.\"\"\"\n",
    "    for col in columns:\n",
    "        if str(col).lower() == str(target_name).lower():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def get_unique_values_from_file(filepath, column_name):\n",
    "    \"\"\"\n",
    "    Reads a file, extracts unique values from a specific column,\n",
    "    cleans them, and returns them as a set.\n",
    "    \"\"\"\n",
    "    print(f\"Processing file: {filepath}...\")\n",
    "    try:\n",
    "        # Read just the header to find the correct column name (case-insensitive)\n",
    "        header_df = pd.read_csv(filepath, nrows=0, on_bad_lines='skip')\n",
    "        actual_col_name = find_actual_column_name(header_df.columns, column_name)\n",
    "\n",
    "        if not actual_col_name:\n",
    "            print(f\"  - Error: Column '{column_name}' not found. Please check the column name.\")\n",
    "            return set()\n",
    "\n",
    "        # Read the full column using the correct name\n",
    "        df = pd.read_csv(filepath, usecols=[actual_col_name], on_bad_lines='skip')\n",
    "        \n",
    "        # --- Data Cleaning ---\n",
    "        # Convert all values to string, extract digits, and then convert to numbers.\n",
    "        # This handles mixed data types (e.g., '123' vs 123) and text prefixes (e.g., 'SW-123').\n",
    "        s = pd.Series(df[actual_col_name].dropna().unique(), dtype=str)\n",
    "        s = s.str.extract('(\\d+)').iloc[:, 0]\n",
    "        s = pd.to_numeric(s, errors='coerce')\n",
    "        \n",
    "        cleaned_values = set(s.dropna().astype(int))\n",
    "        \n",
    "        print(f\"  - Found {len(cleaned_values)} unique, clean values.\")\n",
    "        return cleaned_values\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  - Error: File not found. Please check the path: {filepath}\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"  - An unexpected error occurred: {e}\")\n",
    "        return set()\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# Get the unique values from both files\n",
    "unique_values_from_file1 = get_unique_values_from_file(file1_path, file1_column_name)\n",
    "unique_values_from_file2 = get_unique_values_from_file(file2_path, file2_column_name)\n",
    "\n",
    "# --- Comparison and Reporting ---\n",
    "\n",
    "print(\"\\n--- Comparison Report ---\")\n",
    "if not unique_values_from_file1 or not unique_values_from_file2:\n",
    "    print(\"Could not perform comparison because one of the files could not be processed or contained no valid data.\")\n",
    "else:\n",
    "    # Use set intersection to find the values that exist in both sets\n",
    "    matching_values = unique_values_from_file1.intersection(unique_values_from_file2)\n",
    "    \n",
    "    # Print the final report\n",
    "    print(f\"Unique values in '{file1_path}' (Column: {file1_column_name}): {len(unique_values_from_file1)}\")\n",
    "    print(f\"Unique values in '{file2_path}' (Column: {file2_column_name}): {len(unique_values_from_file2)}\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"Number of values that matched: {len(matching_values)}\")\n",
    "    print(\"-\" * 25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f2be29d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '22_33_SWNO.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m\n\u001b[1;32m     19\u001b[0m OUT_FILE    \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSWNO_MASTER_COMBINED.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 1. LOAD\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m df_swno  \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFILE_SWNO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m df_audit \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(FILE_AUDIT, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m     26\u001b[0m df_cable \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(FILE_CABLE, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '22_33_SWNO.csv'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create SWNO_MASTER_COMBINED.csv by merging\n",
    "  â€¢ 22_33_SWNO.csv          (master list; key = SWNO)\n",
    "  â€¢ ENERGYAUDIT.csv         (key = SWITCH_NO â†’ SWNO)\n",
    "  â€¢ HTCABLE.csv             (key = DESTINATION_SWITCH_ID â†’ SWNO)\n",
    "  â€¢ TXN_JOINT.csv           (key = DESTINATION_SWITCH_ID â†’ SWNO)\n",
    "All 1-to-many columns are pipe-concatenated ( \" | \" ).\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 0. FILE LOCATIONS  (edit if your files are elsewhere)\n",
    "# -----------------------------------------------------------\n",
    "FILE_SWNO   = \"22_33_SWNO.csv\"\n",
    "FILE_AUDIT  = \"ENERGYAUDIT.csv\"\n",
    "FILE_CABLE  = \"HTCABLE.csv\"\n",
    "FILE_JOINT  = \"TXN_JOINT.csv\"\n",
    "OUT_FILE    = \"SWNO_MASTER_COMBINED.csv\"\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1. LOAD\n",
    "# -----------------------------------------------------------\n",
    "df_swno  = pd.read_csv(FILE_SWNO,  dtype=str)\n",
    "df_audit = pd.read_csv(FILE_AUDIT, dtype=str)\n",
    "df_cable = pd.read_csv(FILE_CABLE, dtype=str)\n",
    "df_joint = pd.read_csv(FILE_JOINT, dtype=str)\n",
    "\n",
    "# Ensure key column is string\n",
    "df_swno[\"SWNO\"] = df_swno[\"SWNO\"].astype(str).str.strip()\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2. CLEAN  â–ºâ–ºâ–º  ENERGYAUDIT\n",
    "# -----------------------------------------------------------\n",
    "rename_audit = {\"FUNC_LOC\": \"FUNCTION_LOCATION\"}\n",
    "df_audit.rename(columns=rename_audit, inplace=True)\n",
    "\n",
    "audit_keep = [\n",
    "    \"SWITCH_NO\", \"SYSTEM_DATE\", \"MD_TIME\",\n",
    "    \"CAPACITY_KVA\", \"MD_KVA\", \"CLUSTER_TYPE\",\n",
    "    \"LOAD_FACTOR\", \"FUNCTION_LOCATION\"\n",
    "]\n",
    "df_audit = df_audit[audit_keep]\n",
    "\n",
    "# keep latest SYSTEM_DATE per SWITCH_NO\n",
    "df_audit[\"SYSTEM_DATE\"] = pd.to_datetime(df_audit[\"SYSTEM_DATE\"], errors=\"coerce\")\n",
    "audit_latest = (\n",
    "    df_audit.sort_values(\"SYSTEM_DATE\")\n",
    "            .groupby(\"SWITCH_NO\", dropna=False)\n",
    "            .tail(1)                         # newest row\n",
    "            .rename(columns=lambda c: f\"AUDIT_{c}\" if c != \"SWITCH_NO\" else c)\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3. CLEAN  â–ºâ–ºâ–º  HTCABLE\n",
    "# -----------------------------------------------------------\n",
    "rename_cable = {\n",
    "    \"CABEL_ID\":         \"CABLEID\",\n",
    "    \"CABLECOUNTUCTOR\":  \"CABLECONDUCTORMATERIAL\",\n",
    "    \"NO.OFCORES\":       \"NUMBEROFCORES\",\n",
    "    \"NEUTRAL_MATERIAL\": \"NEUTRALMATERIAL\",\n",
    "    \"coments\":          \"COMMENTS\",\n",
    "}\n",
    "df_cable.rename(columns=rename_cable, inplace=True)\n",
    "\n",
    "cable_keep = [\n",
    "    \"DESTINATION_SWITCH_ID\", \"SOURCE_SS\", \"DESTINATION_SS\",\n",
    "    \"SOURCE_SSFL\", \"DESTINATION_SSFL\", \"REMARKS\",\n",
    "    \"CABLETYPE\", \"ROUTE_SOURCE\", \"MEASUREDLENGTH\", \"OVERHEAD\",\n",
    "    \"NUMBEROFCORES\", \"CABLECONDUCTORMATERIAL\", \"ARMOURED\",\n",
    "    \"CABLEID\", \"NEUTRALMATERIAL\", \"COMMENTS\"\n",
    "]\n",
    "df_cable = df_cable[cable_keep]\n",
    "\n",
    "# aggregate rows per switch (pipe-separate duplicates)\n",
    "cable_agg = (\n",
    "    df_cable.groupby(\"DESTINATION_SWITCH_ID\", dropna=False)\n",
    "            .agg(lambda col: \" | \".join(col.dropna().unique()))\n",
    "            .reset_index()\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4. CLEAN  â–ºâ–ºâ–º  TXN_JOINT\n",
    "# -----------------------------------------------------------\n",
    "rename_joint = {\n",
    "    \"JOINTER_TYPE\":     \"JOINTTYPE\",\n",
    "    \"NUMBEROFCORES1\":   \"NUMBEROFCORES_S1\",\n",
    "    \"NUMBEROFCORES2\":   \"NUMBEROFCORES_S2\",\n",
    "    \"CABLESIZE1\":       \"CABLESIZE_1\",\n",
    "    \"CABLESIZE2\":       \"CABLESIZE_2\",\n",
    "    \"CABLETYPE1\":       \"CABLETYPE_1\",\n",
    "    \"CABLETYPE2\":       \"CABLETYPE_2\",\n",
    "}\n",
    "df_joint.rename(columns=rename_joint, inplace=True)\n",
    "\n",
    "joint_keep = [\n",
    "    \"DESTINATION_SWITCH_ID\", \"FEEDERID\", \"SOURCE_SWITCH_ID\",\n",
    "    \"JOINTTYPE\", \"SOURCE_SS\", \"DESTINATION_SS\", \"LABELTEXT\",\n",
    "    \"NATURE_OF_JOB\", \"CABLESIZE_1\", \"CABLESIZE_2\", \"HYPERLINK\",\n",
    "    \"CABLETYPE_1\", \"CABLETYPE_2\", \"VOLTAGERATING\",\n",
    "    \"NUMBEROFCORES_S1\", \"NUMBEROFCORES_S2\",\n",
    "    \"SOURCE_SSFL\", \"DESTINATION_SSFL\"\n",
    "]\n",
    "df_joint = df_joint[joint_keep]\n",
    "\n",
    "joint_agg = (\n",
    "    df_joint.groupby(\"DESTINATION_SWITCH_ID\", dropna=False)\n",
    "            .agg(lambda col: \" | \".join(col.dropna().unique()))\n",
    "            .reset_index()\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 5. MERGE  (order: SWNO master â†’ audit â†’ cable â†’ joint)\n",
    "# -----------------------------------------------------------\n",
    "master = df_swno.copy()\n",
    "\n",
    "# a ENERGYAUDIT   SWNO â†” SWITCH_NO\n",
    "master = master.merge(\n",
    "    audit_latest, how=\"left\",\n",
    "    left_on=\"SWNO\", right_on=\"SWITCH_NO\"\n",
    ").drop(columns=[\"SWITCH_NO\"])\n",
    "\n",
    "# b HTCABLE       SWNO â†” DESTINATION_SWITCH_ID\n",
    "master = master.merge(\n",
    "    cable_agg, how=\"left\",\n",
    "    left_on=\"SWNO\", right_on=\"DESTINATION_SWITCH_ID\"\n",
    ").drop(columns=[\"DESTINATION_SWITCH_ID\"])\n",
    "\n",
    "# c. TXN_JOINT     SWNO â†” DESTINATION_SWITCH_ID\n",
    "master = master.merge(\n",
    "    joint_agg, how=\"left\",\n",
    "    left_on=\"SWNO\", right_on=\"DESTINATION_SWITCH_ID\",\n",
    "    suffixes=(\"\", \"_JOINT\")       # avoid clashes if any\n",
    ").drop(columns=[\"DESTINATION_SWITCH_ID\"])\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 6. SAVE\n",
    "# -----------------------------------------------------------\n",
    "master.to_csv(OUT_FILE, index=False)\n",
    "print(f\"âœ…  {OUT_FILE} written. Rows: {len(master)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207a63e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
