{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f32a353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b76c63a6",
   "metadata": {},
   "source": [
    "CHECK THE DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26004c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total linebreak cells found: 0\n",
      "Total control-char cells found: 0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file_path = '/media/sagarkumar/New Volume/SAGAR/Book 2(Sheet1).csv'\n",
    "linebreak_count = 0\n",
    "ctrl_count = 0\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for i, row in enumerate(reader):\n",
    "        for j, cell in enumerate(row):\n",
    "            if '\\n' in cell or '\\r' in cell:\n",
    "                print(f'Linebreak found in row {i+1}, column {j+1}: {repr(cell)}')\n",
    "                linebreak_count += 1\n",
    "            # Any ASCII control char except tab (9) and newline (10, 13)\n",
    "            if any(ord(c) < 32 and ord(c) not in (9, 10, 13) for c in cell):\n",
    "                print(f'Ctrl char found in row {i+1}, column {j+1}: {repr(cell)}')\n",
    "                ctrl_count += 1\n",
    "      \n",
    "\n",
    "print(f'Total linebreak cells found: {linebreak_count}')\n",
    "print(f'Total control-char cells found: {ctrl_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b63a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    expected = None\n",
    "    for i, line in enumerate(f):\n",
    "        cols = line.count(',') + 1\n",
    "        if i == 0:\n",
    "            expected = cols\n",
    "        elif cols != expected:\n",
    "            print(f\"Row {i+1} has {cols} columns (Expected: {expected})\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89ce6331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 11058\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Raw file read, no parse (just lines)\n",
    "filename = \"/media/sagarkumar/New Volume/SAGAR/HTLOGSHEET_CLEAN_enetrytype_1_cleaned.csv\"\n",
    "with open(filename, encoding='utf-8', errors='ignore') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(\"Total lines:\", len(lines))\n",
    "\n",
    "# Line breaks within a field detection (very likely in 'FREE_REMARKS' column)\n",
    "for i, line in enumerate(lines):\n",
    "    if '\\n' in line or '\\r' in line:\n",
    "        # \\n to har line me hoga, but internal bhi ho sakta hai\n",
    "        if line.count('\\n') > 1 or line.count('\\r') > 1:\n",
    "            print(f\"Line {i+1}: Multiple linebreaks in a single line\")\n",
    "\n",
    "    # Look for possible tabs, weird ascii\n",
    "    if re.search(r\"[\\t\\x0b\\x0c\\x1b]\", line):\n",
    "        print(f\"Line {i+1}: Contains tab or control char\")\n",
    "\n",
    "# Check for unclosed quotes\n",
    "for i, line in enumerate(lines):\n",
    "    if line.count('\"') % 2 != 0:\n",
    "        print(f\"Line {i+1}: Unmatched double quote\")\n",
    "\n",
    "# Column count (by comma)\n",
    "col_counts = [l.count(',') for l in lines]\n",
    "mode_col_count = max(set(col_counts), key=col_counts.count)\n",
    "for idx, c in enumerate(col_counts):\n",
    "    if c != mode_col_count:\n",
    "        print(f\"Line {idx+1}: {c} columns (Expected: {mode_col_count})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd33002e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4673d9d",
   "metadata": {},
   "source": [
    "CLEANING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caf582b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned file written to /media/sagarkumar/New Volume/SAGAR/HTLOGSHEET_CLEAN_enetrytype_1_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "input_file_path = '/media/sagarkumar/New Volume/SAGAR/HTLOGSHEET_CLEAN_enetrytype_1.csv'\n",
    "output_file_path = '/media/sagarkumar/New Volume/SAGAR/HTLOGSHEET_CLEAN_enetrytype_1_cleaned.csv'\n",
    "\n",
    "def clean_cell(cell):\n",
    "    # Remove any ASCII control characters except tab (9), LF (10), CR (13)\n",
    "    cell = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]', '', cell)\n",
    "    # Replace linebreaks within a cell with space\n",
    "    cell = cell.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    return cell\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8', errors='replace', newline='') as infile, \\\n",
    "     open(output_file_path, 'w', encoding='utf-8', errors='replace', newline='') as outfile:\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    for row in reader:\n",
    "        new_row = [clean_cell(cell) for cell in row]\n",
    "        writer.writerow(new_row)\n",
    "\n",
    "print(f\"Cleaned file written to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4fdac6",
   "metadata": {},
   "source": [
    "PROCCESING THE DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0263a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Load file (final clean file)\n",
    "df_original = pd.read_csv('/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/TXN_NMS_HTLOGSHEET_cleaned.csv', low_memory=False)\n",
    "\n",
    "# 2. Filter ENTRY_TYPE == 1\n",
    "df_entry1 = df_original[df_original['ENTRY_TYPE'] == 1].copy().reset_index(drop=True)\n",
    "\n",
    "# 3. UPPERCASE remarks\n",
    "df_entry1['FREE_REMARKS_UPPER'] = df_entry1['FREE_REMARKS'].astype(str).str.upper()\n",
    "\n",
    "# 4. Extraction functions\n",
    "def extract_size(text):\n",
    "    m = re.search(r'X\\s*([\\d\\.\\+\\-\\s]*SQ\\.?\\s*MM)', text)\n",
    "    return m.group(1).replace(\" \", \"\") if m else \"\"\n",
    "\n",
    "def extract_insulation(text):\n",
    "    m = re.search(r'(PILC\\+XLPE|XLPE\\+PILC|PILC|XLPE)', text)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def extract_voltage(text):\n",
    "    m = re.search(r'(\\d{2,3})\\s*KV', text)\n",
    "    return f\"{m.group(1)}KV\" if m else \"\"\n",
    "\n",
    "def extract_type(text):\n",
    "    m = re.search(r'(HTCF SECTION|LT SECTION|HT SECTION)', text)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def extract_section(text):\n",
    "    m = re.search(r'BETWEEN\\s+(.+?)\\s+S/S\\s+SWITCH:([0-9]+)\\s+TO\\s+(.+?)\\s+S/S\\s+SWITCH:([0-9]+)', text)\n",
    "    if m:\n",
    "        return pd.Series([m.group(1).strip(), m.group(2), m.group(3).strip(), m.group(4)])\n",
    "    return pd.Series([\"\", \"\", \"\", \"\"])\n",
    "\n",
    "def extract_delayed_reason(text):\n",
    "    m = re.search(r'DELAYED DUE TO(.*?)(?:NOTIFICATION NO|$)', text)\n",
    "    return m.group(1).strip() if m else \"\"\n",
    "\n",
    "def extract_fault_nature(text):\n",
    "    patterns = [\n",
    "        'CABLE DAMAGED BY', 'DMS OFFLINE AT', 'FPI MALFUNCTION',\n",
    "        'FAILED TO OPEN', 'FAILED TO CLOSE', 'WENT OFFLINE',\n",
    "        'FEEDER TRIPPING', 'TRAFFIC ON', 'SUPPLY RESTORATION DELAYED'\n",
    "    ]\n",
    "    found = []\n",
    "    for pat in patterns:\n",
    "        for m in re.finditer(r'({0}.*?)(?:\\.|,|;|NOTIFICATION NO|$)'.format(re.escape(pat)), text):\n",
    "            found.append(m.group(1).strip())\n",
    "    return '; '.join(found) if found else \"\"\n",
    "\n",
    "# 5. Extraction (create new DataFrame for output)\n",
    "out = pd.DataFrame()\n",
    "out['Size'] = df_entry1['FREE_REMARKS_UPPER'].apply(extract_size)\n",
    "out['Insulation'] = df_entry1['FREE_REMARKS_UPPER'].apply(extract_insulation)\n",
    "out['Voltage'] = df_entry1['FREE_REMARKS_UPPER'].apply(extract_voltage)\n",
    "out['Type'] = df_entry1['FREE_REMARKS_UPPER'].apply(extract_type)\n",
    "\n",
    "section_cols = df_entry1['FREE_REMARKS_UPPER'].apply(extract_section)\n",
    "section_cols.columns = ['FROM', 'FROM_SWITCH', 'TO', 'TO_SWITCH']\n",
    "out = pd.concat([out, section_cols], axis=1)\n",
    "\n",
    "out['DELAYED_REASON'] = df_entry1['FREE_REMARKS_UPPER'].apply(extract_delayed_reason)\n",
    "out['FAULT_NATURE'] = df_entry1['FREE_REMARKS_UPPER'].apply(extract_fault_nature)\n",
    "\n",
    "# Time columns\n",
    "out['TIME_OUTAGE'] = df_entry1['TIME_OUTAGE'].astype(str)\n",
    "out['MAIN_REPORTED_TIME'] = pd.to_datetime(df_entry1['MAIN_REPORTED_TIME'], errors='coerce')\n",
    "out['TIME_RESTORED'] = pd.to_datetime(df_entry1['TIME_RESTORED'], errors='coerce')\n",
    "out['TIME_DIFFERENCE'] = (out['TIME_RESTORED'] - out['MAIN_REPORTED_TIME']).dt.total_seconds() / 60\n",
    "\n",
    "# 6. Save\n",
    "final_cols = [\n",
    "    'Size', 'Insulation', 'Voltage', 'Type',\n",
    "    'FROM', 'FROM_SWITCH', 'TO', 'TO_SWITCH',\n",
    "    'DELAYED_REASON', 'FAULT_NATURE',\n",
    "    'TIME_OUTAGE', 'MAIN_REPORTED_TIME', 'TIME_RESTORED', 'TIME_DIFFERENCE'\n",
    "]\n",
    "out[final_cols].to_csv('/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/processed_fault_data_FINAL.csv', index=False)\n",
    "print(\"Extracted file written.\")\n",
    "print(out[final_cols].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85942c4",
   "metadata": {},
   "source": [
    "ONLY FREE_REMARKS ANALYZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17930d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted file written.\n",
      "                        Size Insulation Voltage          Type  \\\n",
      "0                                         220KV                 \n",
      "1                                                               \n",
      "2                      SQ.MM  PILC+XLPE    11KV  HTCF SECTION   \n",
      "3            .06+70+120SQ.MM  PILC+XLPE    11KV  HTCF SECTION   \n",
      "4  0.15+185+225+300+240SQ.MM  PILC+XLPE    11KV  HTCF SECTION   \n",
      "5                   120SQ.MM       PILC    11KV  HTCF SECTION   \n",
      "6                   120SQ.MM       PILC    11KV  HTCF SECTION   \n",
      "7                   120SQ.MM       PILC    11KV  HTCF SECTION   \n",
      "8                   120SQ.MM       PILC    11KV  HTCF SECTION   \n",
      "9                   120SQ.MM       PILC    11KV  HTCF SECTION   \n",
      "\n",
      "                       FROM FROM_SWITCH                      TO TO_SWITCH  \\\n",
      "0                                                                           \n",
      "1                                                                           \n",
      "2  VISHWESHWAR NAGAR HETALI       40478                  SAMANT     05587   \n",
      "3        JAWAHAR NAGAR NO.3       13847    SONAVALA ESTATE NO.1     00714   \n",
      "4                                                                           \n",
      "5                AAREY NO.2       18066   AAREY UNIT NO.7 KIOSK     18008   \n",
      "6    AAREY SANKRAMAN STUDIO       19112         AAREY UNIT NO.2     19710   \n",
      "7                 AJIT PARK       15725      TUREL PAKHADI NO.1     06900   \n",
      "8       BABREKAR NAGAR NO.1       18669   KANDIVLI HOUSING NO.3     18671   \n",
      "9    BHAGATSINGH NAGAR NO.1       28860  BHAGATSINGH NAGAR NO.3     34127   \n",
      "\n",
      "                                      DELAYED_REASON  \\\n",
      "0                                                      \n",
      "1                                                      \n",
      "2                                                      \n",
      "3  1)FAILED TO OPEN JAWAHAR NAGAR ROAD NO.12 S/S ...   \n",
      "4  1)FAILED TO CLOSE GOREGAON SHOPPING CENTRE S/S...   \n",
      "5  1)AAREY NO.2 S/S (DMS) WENT OFFLINE AFTER GIVI...   \n",
      "6  1) NON DMS SUBSTATION ARE IN ARREY AREA. DMS U...   \n",
      "7  1)TUREL PAKHADI NO.1 S/S (DMS) WENT OFFLINE AF...   \n",
      "8       1) DMS OFFLINE AT KANDIVLI HOUSING NO.3 S/S.   \n",
      "9  1)FAILED TO OPEN BANGUR NAGAR NO.3 S/S DMS SW....   \n",
      "\n",
      "                                        FAULT_NATURE  \n",
      "0                                                     \n",
      "1                                                     \n",
      "2                                                     \n",
      "3  FAILED TO OPEN JAWAHAR NAGAR ROAD NO; FAILED T...  \n",
      "4  FAILED TO CLOSE GOREGAON SHOPPING CENTRE S/S D...  \n",
      "5                  WENT OFFLINE AFTER GIVING COMMAND  \n",
      "6                                                     \n",
      "7  FAILED TO OPEN AJIT PARK S/S DMS SW; WENT OFFL...  \n",
      "8                 DMS OFFLINE AT KANDIVLI HOUSING NO  \n",
      "9                     FAILED TO OPEN BANGUR NAGAR NO  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Load the file\n",
    "df = pd.read_csv('/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/TXN_NMS_HTLOGSHEET_FREE_REMARKS_ONLY.csv', usecols=['FREE_REMARKS'], low_memory=False)\n",
    "\n",
    "# 2. UPPERCASE remarks for consistency\n",
    "df['FREE_REMARKS_UPPER'] = df['FREE_REMARKS'].astype(str).str.upper()\n",
    "\n",
    "# 3. Extraction functions\n",
    "def extract_size(text):\n",
    "    m = re.search(r'X\\s*([\\d\\.\\+\\-\\s]*SQ\\.?\\s*MM)', text)\n",
    "    return m.group(1).replace(\" \", \"\") if m else \"\"\n",
    "\n",
    "def extract_insulation(text):\n",
    "    m = re.search(r'(PILC\\+XLPE|XLPE\\+PILC|PILC|XLPE)', text)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def extract_voltage(text):\n",
    "    m = re.search(r'(\\d{2,3})\\s*KV', text)\n",
    "    return f\"{m.group(1)}KV\" if m else \"\"\n",
    "\n",
    "def extract_type(text):\n",
    "    m = re.search(r'(HTCF SECTION|LT SECTION|HT SECTION)', text)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def extract_section(text):\n",
    "    m = re.search(r'BETWEEN\\s+(.+?)\\s+S/S\\s+SWITCH:([0-9]+)\\s+TO\\s+(.+?)\\s+S/S\\s+SWITCH:([0-9]+)', text)\n",
    "    if m:\n",
    "        return pd.Series([m.group(1).strip(), m.group(2), m.group(3).strip(), m.group(4)])\n",
    "    return pd.Series([\"\", \"\", \"\", \"\"])\n",
    "\n",
    "def extract_delayed_reason(text):\n",
    "    m = re.search(r'DELAYED DUE TO(.*?)(?:NOTIFICATION NO|$)', text)\n",
    "    return m.group(1).strip() if m else \"\"\n",
    "\n",
    "def extract_fault_nature(text):\n",
    "    patterns = [\n",
    "        'CABLE DAMAGED BY', 'DMS OFFLINE AT', 'FPI MALFUNCTION',\n",
    "        'FAILED TO OPEN', 'FAILED TO CLOSE', 'WENT OFFLINE',\n",
    "        'FEEDER TRIPPING', 'TRAFFIC ON', 'SUPPLY RESTORATION DELAYED'\n",
    "    ]\n",
    "    found = []\n",
    "    for pat in patterns:\n",
    "        for m in re.finditer(r'({0}.*?)(?:\\.|,|;|NOTIFICATION NO|$)'.format(re.escape(pat)), text):\n",
    "            found.append(m.group(1).strip())\n",
    "    return '; '.join(found) if found else \"\"\n",
    "\n",
    "# 4. Apply extraction functions\n",
    "out = pd.DataFrame()\n",
    "out['Size'] = df['FREE_REMARKS_UPPER'].apply(extract_size)\n",
    "out['Insulation'] = df['FREE_REMARKS_UPPER'].apply(extract_insulation)\n",
    "out['Voltage'] = df['FREE_REMARKS_UPPER'].apply(extract_voltage)\n",
    "out['Type'] = df['FREE_REMARKS_UPPER'].apply(extract_type)\n",
    "\n",
    "section_cols = df['FREE_REMARKS_UPPER'].apply(extract_section)\n",
    "section_cols.columns = ['FROM', 'FROM_SWITCH', 'TO', 'TO_SWITCH']\n",
    "out = pd.concat([out, section_cols], axis=1)\n",
    "\n",
    "out['DELAYED_REASON'] = df['FREE_REMARKS_UPPER'].apply(extract_delayed_reason)\n",
    "out['FAULT_NATURE'] = df['FREE_REMARKS_UPPER'].apply(extract_fault_nature)\n",
    "\n",
    "# 5. Save results\n",
    "out.to_csv('/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/processed_fault_data_ONLY_FREEREMARKS.csv', index=False)\n",
    "print(\"Extracted file written.\")\n",
    "print(out.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad937e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Parsed 24,228 records  →  /media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/processed_fault_data_ONLY_FREEREMARKS.csv\n",
      "|    | Size                      | Insulation   | Voltage   | Type         | FROM                     |   FROM_SWITCH | TO                     |   TO_SWITCH | DELAYED_REASON                                                                                                                                                                                                                                                                                                                                                                    | FAULT_NATURE   |\n",
      "|---:|:--------------------------|:-------------|:----------|:-------------|:-------------------------|--------------:|:-----------------------|------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------|\n",
      "|  0 | SQ.MM                     | PILC+XLPE    | 11KV      | HTCF SECTION | VISHWESHWAR NAGAR HETALI |         40478 | SAMANT                 |        5587 |                                                                                                                                                                                                                                                                                                                                                                                   |                |\n",
      "|  1 | .06+70+120SQ.MM           | PILC+XLPE    | 11KV      | HTCF SECTION | JAWAHAR NAGAR NO.3       |         13847 | SONAVALA ESTATE NO.1   |         714 | 1)FAILED TO OPEN JAWAHAR NAGAR ROAD NO.12 S/S DMS SW.NO.25658 FAILED TO OPERATE FROM SCADA. 2)FAILED TO OPEN JAWAHAR NAGAR ROAD NO.12 S/S DMS SW.NO.25657 FAILED TO OPERATE FROM SCADA. 3)KASTURI KUNJ S/S (DMS) WENT OFFLINE AFTER TRIPPING.                                                                                                                                     |                |\n",
      "|  2 | 0.15+185+225+300+240SQ.MM | PILC+XLPE    | 11KV      | HTCF SECTION | nan                      |           nan | nan                    |         nan | 1)FAILED TO CLOSE GOREGAON SHOPPING CENTRE S/S DMS SW.NO.04673 FAILED TO OPERATE FROM SCADA.                                                                                                                                                                                                                                                                                      |                |\n",
      "|  3 | 120SQ.MM                  | PILC         | 11KV      | HTCF SECTION | AAREY NO.2               |         18066 | AAREY UNIT NO.7 KIOSK  |       18008 | 1)AAREY NO.2 S/S (DMS) WENT OFFLINE AFTER GIVING COMMAND. 2)FPI FAULTY AT AAREY UNIT NO.7 KIOSK S/S..                                                                                                                                                                                                                                                                             |                |\n",
      "|  4 | 120SQ.MM                  | PILC         | 11KV      | HTCF SECTION | AAREY SANKRAMAN STUDIO   |         19112 | AAREY UNIT NO.2        |       19710 | 1) NON DMS SUBSTATION ARE IN ARREY AREA. DMS USED FOR ISOLATION AND RESTORATION.                                                                                                                                                                                                                                                                                                  |                |\n",
      "|  5 | 120SQ.MM                  | PILC         | 11KV      | HTCF SECTION | AJIT PARK                |         15725 | TUREL PAKHADI NO.1     |        6900 | 1)TUREL PAKHADI NO.1 S/S (DMS) WENT OFFLINE AFTER TRIPPING. 2)LIBERTY GARDEN SOUTH NO.2 S/S (DMS) WENT OFFLINE AFTER TRIPPING. 3) FEEDER TRIPPING AT PALM COURT REC-STN SW.NO.27081. 4)FAILED TO OPEN AJIT PARK S/S DMS SW.NO.15725 FAILED TO OPERATE FROM SCADA.                                                                                                                 |                |\n",
      "|  6 | 120SQ.MM                  | PILC         | 11KV      | HTCF SECTION | BABREKAR NAGAR NO.1      |         18669 | KANDIVLI HOUSING NO.3  |       18671 | 1) DMS OFFLINE AT KANDIVLI HOUSING NO.3 S/S.                                                                                                                                                                                                                                                                                                                                      |                |\n",
      "|  7 | 120SQ.MM                  | PILC         | 11KV      | HTCF SECTION | BHAGATSINGH NAGAR NO.1   |         28860 | BHAGATSINGH NAGAR NO.3 |       34127 | 1)FAILED TO OPEN BANGUR NAGAR NO.3 S/S DMS SW.NO.31181 FAILED TO OPERATE FROM SCADA. DMS OPERATED AFTER GIVING MULTIPLE COMMANDS.                                                                                                                                                                                                                                                 |                |\n",
      "|  8 | 120SQ.MM                  | PILC         | 11KV      | HTCF SECTION | DINDOSHI VASAHAT CENTRAL |         17141 | DINDOSHI VASAHAT SOUTH |       14101 |                                                                                                                                                                                                                                                                                                                                                                                   |                |\n",
      "|  9 | 120SQ.MM                  | PILC         | 11KV      | HTCF SECTION | GOKULDHAM NO.2           |          7636 | GOKULDHAM NO.1         |        6877 | 1) FPI MALFUNCTION AT GOKULDHAM NO.1 S/S SW.NO.06877 (GLOWN). 2) FPI MALFUNCTION AT GOKULDHAM NO.1 S/S SW.NO.06878 (GLOWN). 3)FAILED TO OPEN GOKULDHAM NO.1 S/S DMS SW.NO.06877 FAILED TO OPERATE FROM SCADA. 4)FAILED TO OPEN GOKULDHAM NO.1 S/S DMS SW.NO.06878 FAILED TO OPERATE FROM SCADA. 5)FAILED TO OPEN GOKULDHAM NO.1 S/S DMS SW.NO.06892 FAILED TO OPERATE FROM SCADA. |                |\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -----------------------------------------------------------\n",
    "#  Pull structured fields out of HT-cable FREE_REMARKS strings\n",
    "# -----------------------------------------------------------\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# ─── Edit these two paths ───────────────────────────────────\n",
    "SRC  = r'/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/Book1.csv'   # has FREE_REMARKS column\n",
    "DEST = r'/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/processed_fault_data_ONLY_FREEREMARKS.csv'\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1) Load and normalise ----------------------------------------------------------\n",
    "df_txt = (\n",
    "    pd.read_csv(SRC, usecols=['FREE_REMARKS'], low_memory=False)\n",
    "      .fillna('')\n",
    "      .assign(TXT=lambda d: d['FREE_REMARKS'].str.upper())\n",
    ")\n",
    "\n",
    "# 2) Pre-compile the regexes -----------------------------------------------------\n",
    "RE_SIZE        = re.compile(r'\\bX\\s*([\\d+\\-.\\s]*SQ\\.?\\s*MM)')            # 120SQ.MM  0.15+185+240SQ.MM\n",
    "RE_INSULATION  = re.compile(r'(PILC\\+XLPE|XLPE\\+PILC|PILC|XLPE)')\n",
    "RE_VOLTAGE     = re.compile(r'(\\d{2,3})\\s*KV')\n",
    "RE_TYPE        = re.compile(r'(HTCF SECTION|LT SECTION|HT SECTION)')\n",
    "RE_SECTION     = re.compile(\n",
    "    r'BETWEEN\\s+(.+?)\\s+S/S\\s+SW\\.?ITCH:?\\s*([0-9]+)\\s+TO\\s+(.+?)\\s+S/S\\s+SW\\.?ITCH:?\\s*([0-9]+)',\n",
    "    flags=re.I\n",
    ")\n",
    "RE_DELAYED     = re.compile(\n",
    "    r'(?:ISOLATION\\s*&?\\s*RESTORATION\\s+)?DELAYED (?:AS|DUE TO)\\s*(.*?)(?:NOTIFICATION NO|$)',\n",
    "    flags=re.I\n",
    ")\n",
    "\n",
    "FAULT_TOKENS = [\n",
    "    'CABLE DAMAGED BY', 'CABLE DAMAGED',     # covers slightly shorter variant\n",
    "    'DMS OFFLINE AT', 'WENT OFFLINE',\n",
    "    'FPI MALFUNCTION', 'FPI FAULTY',\n",
    "    'FAILED TO OPEN',  'FAILED TO CLOSE',\n",
    "    'FEEDER TRIPPING', 'TRAFFIC ON',\n",
    "    'SUPPLY RESTORTION DELAYED', 'SUPPLY RESTORATION DELAYED'\n",
    "]\n",
    "# create one big alternation for speed\n",
    "RE_FAULT = re.compile(\n",
    "    '(' + '|'.join(re.escape(tok) for tok in FAULT_TOKENS) + r'.*?)(?:\\.|,|;|NOTIFICATION NO|$)',\n",
    "    flags=re.I\n",
    ")\n",
    "\n",
    "# 3) Utility ---------------------------------------------------------------------\n",
    "def _first(regex, text, fmt=lambda m: m.group(1)):\n",
    "    m = regex.search(text)\n",
    "    return fmt(m) if m else ''\n",
    "\n",
    "def _faults(text: str) -> str:\n",
    "    return '; '.join(m.group(1).strip() for m in RE_FAULT.finditer(text))\n",
    "\n",
    "# 4) Column extraction -----------------------------------------------------------\n",
    "out = pd.DataFrame({\n",
    "    'Size'       : df_txt['TXT'].apply(lambda t: _first(RE_SIZE, t, lambda m: m.group(1).replace(' ', ''))),\n",
    "    'Insulation' : df_txt['TXT'].apply(lambda t: _first(RE_INSULATION, t)),\n",
    "    'Voltage'    : df_txt['TXT'].apply(lambda t: _first(RE_VOLTAGE, t, lambda m: f\"{m.group(1)}KV\")),\n",
    "    'Type'       : df_txt['TXT'].apply(lambda t: _first(RE_TYPE, t))\n",
    "})\n",
    "\n",
    "# --- FROM / TO / SWITCH columns -------------------------------------------------\n",
    "sec = df_txt['TXT'].str.extract(RE_SECTION)\n",
    "sec.columns = ['FROM', 'FROM_SWITCH', 'TO', 'TO_SWITCH']\n",
    "out = pd.concat([out, sec], axis=1)\n",
    "\n",
    "# --- delay / fault --------------------------------------------------------------\n",
    "out['DELAYED_REASON'] = df_txt['TXT'].apply(lambda t: _first(RE_DELAYED, t).strip())\n",
    "out['FAULT_NATURE']   = df_txt['TXT'].apply(_faults)\n",
    "\n",
    "# 5) Final tidy-up & write -------------------------------------------------------\n",
    "out = out[['Size','Insulation','Voltage','Type',\n",
    "           'FROM','FROM_SWITCH','TO','TO_SWITCH',\n",
    "           'DELAYED_REASON','FAULT_NATURE']]\n",
    "\n",
    "out.to_csv(DEST, index=False)\n",
    "print(f'✓ Parsed {len(out):,} records  →  {DEST}')\n",
    "print(out.head(10).to_markdown())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6e9fd",
   "metadata": {},
   "source": [
    "ANALYZE THE DESCRIPTION COLUMN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c2b243c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows containing the word \"SWITCH\": 21584\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows where the DESCRIPTION column contains the word \"SWITCH\"\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/DESCRIPTION.csv', low_memory=False)\n",
    "rows_with_switch = df['DESCRIPTION'].str.contains(r'\\bSWITCH\\b', case=False, na=False).sum()\n",
    "print(f'Number of rows containing the word \"SWITCH\": {rows_with_switch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e7390b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23939"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)  # Total number of rows in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd18e39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows containing the word \"11kV\": 19598\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rows_with_switch = df['DESCRIPTION'].str.contains(r'\\b11 KV\\b', case=False, na=False).sum()\n",
    "print(f'Number of rows containing the word \"11kV\": {rows_with_switch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab973a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows containing the word \"22kV\": 627\n",
      "Number of rows containing the word \"33kV\": 1194\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "rows_with_switch_22 = df['DESCRIPTION'].str.contains(r'\\b22 KV\\b', case=False, na=False).sum()\n",
    "row_with_33 = df['DESCRIPTION'].str.contains(r'\\b33 KV\\b', case=False, na=False).sum()\n",
    "print(f'Number of rows containing the word \"22kV\": {rows_with_switch_22}')\n",
    "print(f'Number of rows containing the word \"33kV\": {row_with_33}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16adce0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows containing \"SWITCH\", \"11kV\", \"22kV\", or \"33kV\": 21419\n"
     ]
    }
   ],
   "source": [
    "total = rows_with_switch + row_with_33 + rows_with_switch_22\n",
    "print(f'Total number of rows containing \"SWITCH\", \"11kV\", \"22kV\", or \"33kV\": {total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1133ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows containing the word \"SWITCH\": 21749\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows where the DESCRIPTION column contains the word \"SWITCH\"\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/TXN_NMS_HTLOGSHEET_FREE_REMARKS_ONLY_clean.csv', low_memory=False)\n",
    "rows_with_switch = df['FREE_REMARKS'].str.contains(r'\\bSWITCH\\b', case=False, na=False).sum()\n",
    "print(f'Number of rows containing the word \"SWITCH\": {rows_with_switch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d703585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows containing the word \"11kV\": 19777\n",
      "Number of rows containing the word \"22kV\": 680\n",
      "Number of rows containing the word \"33kV\": 1294\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rows_with_switch = df['FREE_REMARKS'].str.contains(r'\\b11 KV\\b', case=False, na=False).sum()\n",
    "print(f'Number of rows containing the word \"11kV\": {rows_with_switch}')\n",
    "\n",
    "\n",
    "rows_with_switch_22 = df['FREE_REMARKS'].str.contains(r'\\b22 KV\\b', case=False, na=False).sum()\n",
    "row_with_33 = df['FREE_REMARKS'].str.contains(r'\\b33 KV\\b', case=False, na=False).sum()\n",
    "print(f'Number of rows containing the word \"22kV\": {rows_with_switch_22}')\n",
    "print(f'Number of rows containing the word \"33kV\": {row_with_33}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fec9a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows containing \"SWITCH\", \"11kV\", \"22kV\", or \"33kV\": 21751\n"
     ]
    }
   ],
   "source": [
    "total = rows_with_switch + row_with_33 + rows_with_switch_22\n",
    "print(f'Total number of rows containing \"SWITCH\", \"11kV\", \"22kV\", or \"33kV\": {total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f089af",
   "metadata": {},
   "source": [
    "MAtching the feeder id with thE cleaned  DATA OF FAULT collmn SWITCH_NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb029aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /media/sagarkumar/New Volume/SAGAR/HTLOGSHEET_CLEAN_enetrytype_1_cleaned.csv...\n",
      "  - Found 1268 unique, clean values.\n",
      "Processing file: /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/final_two_column_with_rank_11_withoutDT.csv...\n",
      "  - Found 945 unique, clean values.\n",
      "\n",
      "--- Comparison Report ---\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/HTLOGSHEET_CLEAN_enetrytype_1_cleaned.csv' (Column: SWITCH_NO): 1268\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/final_two_column_with_rank_11_withoutDT.csv' (Column: FEEDER_ID): 945\n",
      "-------------------------\n",
      "Number of values that matched: 824\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION: PLEASE EDIT THIS SECTION ---\n",
    "\n",
    "# File 1 Details\n",
    "file1_path = \"/media/sagarkumar/New Volume/SAGAR/HTLOGSHEET_CLEAN_enetrytype_1_cleaned.csv\"\n",
    "file1_column_name = \"SWITCH_NO\"\n",
    "\n",
    "# File 2 Details\n",
    "file2_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/final_two_column_with_rank_11_withoutDT.csv\"\n",
    "file2_column_name = \"FEEDER_ID\"\n",
    "\n",
    "\n",
    "\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "\n",
    "def find_actual_column_name(columns, target_name):\n",
    "    \"\"\"Helper function to find a column name, ignoring case.\"\"\"\n",
    "    for col in columns:\n",
    "        if str(col).lower() == str(target_name).lower():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def get_unique_values_from_file(filepath, column_name):\n",
    "    \"\"\"\n",
    "    Reads a file, extracts unique values from a specific column,\n",
    "    cleans them, and returns them as a set.\n",
    "    \"\"\"\n",
    "    print(f\"Processing file: {filepath}...\")\n",
    "    try:\n",
    "        # Read just the header to find the correct column name (case-insensitive)\n",
    "        header_df = pd.read_csv(filepath, nrows=0, on_bad_lines='skip')\n",
    "        actual_col_name = find_actual_column_name(header_df.columns, column_name)\n",
    "\n",
    "        if not actual_col_name:\n",
    "            print(f\"  - Error: Column '{column_name}' not found. Please check the column name.\")\n",
    "            return set()\n",
    "\n",
    "        # Read the full column using the correct name\n",
    "        df = pd.read_csv(filepath, usecols=[actual_col_name], on_bad_lines='skip')\n",
    "        \n",
    "        # --- Data Cleaning ---\n",
    "        # Convert all values to string, extract digits, and then convert to numbers.\n",
    "        # This handles mixed data types (e.g., '123' vs 123) and text prefixes (e.g., 'SW-123').\n",
    "        s = pd.Series(df[actual_col_name].dropna().unique(), dtype=str)\n",
    "        s = s.str.extract('(\\d+)').iloc[:, 0]\n",
    "        s = pd.to_numeric(s, errors='coerce')\n",
    "        \n",
    "        cleaned_values = set(s.dropna().astype(int))\n",
    "        \n",
    "        print(f\"  - Found {len(cleaned_values)} unique, clean values.\")\n",
    "        return cleaned_values\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  - Error: File not found. Please check the path: {filepath}\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"  - An unexpected error occurred: {e}\")\n",
    "        return set()\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# Get the unique values from both files\n",
    "unique_values_from_file1 = get_unique_values_from_file(file1_path, file1_column_name)\n",
    "unique_values_from_file2 = get_unique_values_from_file(file2_path, file2_column_name)\n",
    "\n",
    "# --- Comparison and Reporting ---\n",
    "\n",
    "print(\"\\n--- Comparison Report ---\")\n",
    "if not unique_values_from_file1 or not unique_values_from_file2:\n",
    "    print(\"Could not perform comparison because one of the files could not be processed or contained no valid data.\")\n",
    "else:\n",
    "    # Use set intersection to find the values that exist in both sets\n",
    "    matching_values = unique_values_from_file1.intersection(unique_values_from_file2)\n",
    "    \n",
    "    # Print the final report\n",
    "    print(f\"Unique values in '{file1_path}' (Column: {file1_column_name}): {len(unique_values_from_file1)}\")\n",
    "    print(f\"Unique values in '{file2_path}' (Column: {file2_column_name}): {len(unique_values_from_file2)}\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"Number of values that matched: {len(matching_values)}\")\n",
    "    print(\"-\" * 25)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a19d060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching values (sorted):\n",
      "Matching values saved to: /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/order_id.csv\n"
     ]
    }
   ],
   "source": [
    "matching_values_list = sorted(matching_values)\n",
    "print(\"Matching values (sorted):\")\n",
    "output_file_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/order_id.csv\"\n",
    "matching_values_df = pd.DataFrame(matching_values_list, columns=[file1_column_name])\n",
    "matching_values_df.to_csv(output_file_path, index=False)\n",
    "print(f\"Matching values saved to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867df427",
   "metadata": {},
   "source": [
    "ORDER_ID AND SWITCH NO IN ORDER ID MATCH WITH SWITCH NO IS 89 BUT ORDER_ID MATCH THE FEDER _ID WITH 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c8359c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of values that matched: 89\n",
      "Total matching values: 99\n"
     ]
    }
   ],
   "source": [
    "match = unique_values_from_file1.intersection(matching_values_list)\n",
    "print(f\"Number of values that matched: {len(match)}\")\n",
    "\n",
    "\n",
    "print(f\"Total matching values: {len(matching_values_list)}\")\n",
    "match_list = sorted(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cc29eff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values in 'match' but not in 'matching_values_list': [26332, 28192, 29986, 30467, 30668, 32164, 32529, 32530, 34678, 39411]\n",
      "Count: 10\n"
     ]
    }
   ],
   "source": [
    "# Find values in match that are NOT in matching_values_list\n",
    "diff_match_vs_matching_values_list = sorted(set(matching_values_list) - set(match))\n",
    "print(\"Values in 'match' but not in 'matching_values_list':\", diff_match_vs_matching_values_list)\n",
    "print(f\"Count: {len(diff_match_vs_matching_values_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f32aee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('your_input_file.csv', low_memory=False)\n",
    "\n",
    "# Fallback for missing values\n",
    "df['FREE_REMARKS'] = df['FREE_REMARKS'].fillna('').astype(str)\n",
    "\n",
    "# Regex patterns\n",
    "RE_BETWEEN = re.compile(\n",
    "    r'''BETWEEN\\s+\n",
    "        (?P<src>.+?)\\s+S/S\\s+(?:SWITCH|SW\\.?\\s*NO\\.?)\\s*[:\\s]*(?P<src_sw>\\d+)\n",
    "        \\s+TO\\s+\n",
    "        (?P<dst>.+?)\\s+S/S\\s+(?:SWITCH|SW\\.?\\s*NO\\.?)\\s*[:\\s]*(?P<dst_sw>\\d+)\n",
    "    ''', flags=re.I|re.X)\n",
    "\n",
    "RE_VOLTAGE = re.compile(r'(\\d{2,3})\\s*KV', re.I)\n",
    "\n",
    "def extract_info(row):\n",
    "    txt = row['FREE_REMARKS'].upper()\n",
    "    m = RE_BETWEEN.search(txt)\n",
    "    # Default empty\n",
    "    src, src_sw, dst, dst_sw = '', '', '', ''\n",
    "    if m:\n",
    "        src, src_sw, dst, dst_sw = (m.group('src').strip(), m.group('src_sw').strip(),\n",
    "                                    m.group('dst').strip(), m.group('dst_sw').strip())\n",
    "    # Voltage\n",
    "    v_match = RE_VOLTAGE.search(txt)\n",
    "    voltage = v_match.group(1) + ' KV' if v_match else ''\n",
    "    return pd.Series({\n",
    "        'SOURCE_STATION': src,\n",
    "        'FROM_SWITCH': src_sw,\n",
    "        'DESTINATION_STATION': dst,\n",
    "        'TO_SWITCH': dst_sw,\n",
    "        'CABLE_VOLTAGE': voltage\n",
    "    })\n",
    "\n",
    "# Apply to DataFrame (fast, vectorized)\n",
    "extracted = df.apply(extract_info, axis=1)\n",
    "\n",
    "# Combine with your needed columns\n",
    "result = pd.concat([\n",
    "    df[['SWITCH_NO', 'STATION_NAME', 'TIME_OUTAGE', 'TIME_RESTORED', 'MAIN_REPORTED_TIME']],\n",
    "    extracted\n",
    "], axis=1)\n",
    "\n",
    "# Time difference in hours (assuming both columns are in standard format)\n",
    "result['TIME_OUTAGE'] = pd.to_datetime(result['TIME_OUTAGE'], errors='coerce')\n",
    "result['TIME_RESTORED'] = pd.to_datetime(result['TIME_RESTORED'], errors='coerce')\n",
    "result['MAIN_REPORTED_TIME'] = pd.to_datetime(result['MAIN_REPORTED_TIME'], errors='coerce')\n",
    "\n",
    "result['OUTAGE_DURATION_HRS'] = (\n",
    "    (result['TIME_RESTORED'] - result['MAIN_REPORTED_TIME'])\n",
    "    .dt.total_seconds() / 3600\n",
    ").round(2)\n",
    "\n",
    "# Save output\n",
    "result.to_csv('analyzed_HTcable_faults.csv', index=False)\n",
    "print(result.head(12))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
