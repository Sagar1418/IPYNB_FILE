{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f32a353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b76c63a6",
   "metadata": {},
   "source": [
    "CHECK THE DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26004c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total linebreak cells found: 0\n",
      "Total control-char cells found: 0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file_path = '/media/sagarkumar/New Volume/SAGAR/Book 2(Sheet1).csv'\n",
    "linebreak_count = 0\n",
    "ctrl_count = 0\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for i, row in enumerate(reader):\n",
    "        for j, cell in enumerate(row):\n",
    "            if '\\n' in cell or '\\r' in cell:\n",
    "                print(f'Linebreak found in row {i+1}, column {j+1}: {repr(cell)}')\n",
    "                linebreak_count += 1\n",
    "            # Any ASCII control char except tab (9) and newline (10, 13)\n",
    "            if any(ord(c) < 32 and ord(c) not in (9, 10, 13) for c in cell):\n",
    "                print(f'Ctrl char found in row {i+1}, column {j+1}: {repr(cell)}')\n",
    "                ctrl_count += 1\n",
    "      \n",
    "\n",
    "print(f'Total linebreak cells found: {linebreak_count}')\n",
    "print(f'Total control-char cells found: {ctrl_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b63a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    expected = None\n",
    "    for i, line in enumerate(f):\n",
    "        cols = line.count(',') + 1\n",
    "        if i == 0:\n",
    "            expected = cols\n",
    "        elif cols != expected:\n",
    "            print(f\"Row {i+1} has {cols} columns (Expected: {expected})\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89ce6331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 11058\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Raw file read, no parse (just lines)\n",
    "filename = \"/media/sagarkumar/New Volume/SAGAR/HTLOGSHEET_CLEAN_enetrytype_1_cleaned.csv\"\n",
    "with open(filename, encoding='utf-8', errors='ignore') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(\"Total lines:\", len(lines))\n",
    "\n",
    "# Line breaks within a field detection (very likely in 'FREE_REMARKS' column)\n",
    "for i, line in enumerate(lines):\n",
    "    if '\\n' in line or '\\r' in line:\n",
    "        # \\n to har line me hoga, but internal bhi ho sakta hai\n",
    "        if line.count('\\n') > 1 or line.count('\\r') > 1:\n",
    "            print(f\"Line {i+1}: Multiple linebreaks in a single line\")\n",
    "\n",
    "    # Look for possible tabs, weird ascii\n",
    "    if re.search(r\"[\\t\\x0b\\x0c\\x1b]\", line):\n",
    "        print(f\"Line {i+1}: Contains tab or control char\")\n",
    "\n",
    "# Check for unclosed quotes\n",
    "for i, line in enumerate(lines):\n",
    "    if line.count('\"') % 2 != 0:\n",
    "        print(f\"Line {i+1}: Unmatched double quote\")\n",
    "\n",
    "# Column count (by comma)\n",
    "col_counts = [l.count(',') for l in lines]\n",
    "mode_col_count = max(set(col_counts), key=col_counts.count)\n",
    "for idx, c in enumerate(col_counts):\n",
    "    if c != mode_col_count:\n",
    "        print(f\"Line {idx+1}: {c} columns (Expected: {mode_col_count})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd33002e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4673d9d",
   "metadata": {},
   "source": [
    "CLEANING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caf582b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned file written to /media/sagarkumar/New Volume/SAGAR/HTLOGSHEET_CLEAN_enetrytype_1_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "input_file_path = '/media/sagarkumar/New Volume/SAGAR/HTLOGSHEET_CLEAN_enetrytype_1.csv'\n",
    "output_file_path = '/media/sagarkumar/New Volume/SAGAR/HTLOGSHEET_CLEAN_enetrytype_1_cleaned.csv'\n",
    "\n",
    "def clean_cell(cell):\n",
    "    # Remove any ASCII control characters except tab (9), LF (10), CR (13)\n",
    "    cell = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]', '', cell)\n",
    "    # Replace linebreaks within a cell with space\n",
    "    cell = cell.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    return cell\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8', errors='replace', newline='') as infile, \\\n",
    "     open(output_file_path, 'w', encoding='utf-8', errors='replace', newline='') as outfile:\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    for row in reader:\n",
    "        new_row = [clean_cell(cell) for cell in row]\n",
    "        writer.writerow(new_row)\n",
    "\n",
    "print(f\"Cleaned file written to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4fdac6",
   "metadata": {},
   "source": [
    "PROCCESING THE DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0263a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Load file (final clean file)\n",
    "df_original = pd.read_csv('/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/TXN_NMS_HTLOGSHEET_cleaned.csv', low_memory=False)\n",
    "\n",
    "# 2. Filter ENTRY_TYPE == 1\n",
    "df_entry1 = df_original[df_original['ENTRY_TYPE'] == 1].copy().reset_index(drop=True)\n",
    "\n",
    "# 3. UPPERCASE remarks\n",
    "df_entry1['FREE_REMARKS_UPPER'] = df_entry1['FREE_REMARKS'].astype(str).str.upper()\n",
    "\n",
    "# 4. Extraction functions\n",
    "def extract_size(text):\n",
    "    m = re.search(r'X\\s*([\\d\\.\\+\\-\\s]*SQ\\.?\\s*MM)', text)\n",
    "    return m.group(1).replace(\" \", \"\") if m else \"\"\n",
    "\n",
    "def extract_insulation(text):\n",
    "    m = re.search(r'(PILC\\+XLPE|XLPE\\+PILC|PILC|XLPE)', text)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def extract_voltage(text):\n",
    "    m = re.search(r'(\\d{2,3})\\s*KV', text)\n",
    "    return f\"{m.group(1)}KV\" if m else \"\"\n",
    "\n",
    "def extract_type(text):\n",
    "    m = re.search(r'(HTCF SECTION|LT SECTION|HT SECTION)', text)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def extract_section(text):\n",
    "    m = re.search(r'BETWEEN\\s+(.+?)\\s+S/S\\s+SWITCH:([0-9]+)\\s+TO\\s+(.+?)\\s+S/S\\s+SWITCH:([0-9]+)', text)\n",
    "    if m:\n",
    "        return pd.Series([m.group(1).strip(), m.group(2), m.group(3).strip(), m.group(4)])\n",
    "    return pd.Series([\"\", \"\", \"\", \"\"])\n",
    "\n",
    "def extract_delayed_reason(text):\n",
    "    m = re.search(r'DELAYED DUE TO(.*?)(?:NOTIFICATION NO|$)', text)\n",
    "    return m.group(1).strip() if m else \"\"\n",
    "\n",
    "def extract_fault_nature(text):\n",
    "    patterns = [\n",
    "        'CABLE DAMAGED BY', 'DMS OFFLINE AT', 'FPI MALFUNCTION',\n",
    "        'FAILED TO OPEN', 'FAILED TO CLOSE', 'WENT OFFLINE',\n",
    "        'FEEDER TRIPPING', 'TRAFFIC ON', 'SUPPLY RESTORATION DELAYED'\n",
    "    ]\n",
    "    found = []\n",
    "    for pat in patterns:\n",
    "        for m in re.finditer(r'({0}.*?)(?:\\.|,|;|NOTIFICATION NO|$)'.format(re.escape(pat)), text):\n",
    "            found.append(m.group(1).strip())\n",
    "    return '; '.join(found) if found else \"\"\n",
    "\n",
    "# 5. Extraction (create new DataFrame for output)\n",
    "out = pd.DataFrame()\n",
    "out['Size'] = df_entry1['FREE_REMARKS_UPPER'].apply(extract_size)\n",
    "out['Insulation'] = df_entry1['FREE_REMARKS_UPPER'].apply(extract_insulation)\n",
    "out['Voltage'] = df_entry1['FREE_REMARKS_UPPER'].apply(extract_voltage)\n",
    "out['Type'] = df_entry1['FREE_REMARKS_UPPER'].apply(extract_type)\n",
    "\n",
    "section_cols = df_entry1['FREE_REMARKS_UPPER'].apply(extract_section)\n",
    "section_cols.columns = ['FROM', 'FROM_SWITCH', 'TO', 'TO_SWITCH']\n",
    "out = pd.concat([out, section_cols], axis=1)\n",
    "\n",
    "out['DELAYED_REASON'] = df_entry1['FREE_REMARKS_UPPER'].apply(extract_delayed_reason)\n",
    "out['FAULT_NATURE'] = df_entry1['FREE_REMARKS_UPPER'].apply(extract_fault_nature)\n",
    "\n",
    "# Time columns\n",
    "out['TIME_OUTAGE'] = df_entry1['TIME_OUTAGE'].astype(str)\n",
    "out['MAIN_REPORTED_TIME'] = pd.to_datetime(df_entry1['MAIN_REPORTED_TIME'], errors='coerce')\n",
    "out['TIME_RESTORED'] = pd.to_datetime(df_entry1['TIME_RESTORED'], errors='coerce')\n",
    "out['TIME_DIFFERENCE'] = (out['TIME_RESTORED'] - out['MAIN_REPORTED_TIME']).dt.total_seconds() / 60\n",
    "\n",
    "# 6. Save\n",
    "final_cols = [\n",
    "    'Size', 'Insulation', 'Voltage', 'Type',\n",
    "    'FROM', 'FROM_SWITCH', 'TO', 'TO_SWITCH',\n",
    "    'DELAYED_REASON', 'FAULT_NATURE',\n",
    "    'TIME_OUTAGE', 'MAIN_REPORTED_TIME', 'TIME_RESTORED', 'TIME_DIFFERENCE'\n",
    "]\n",
    "out[final_cols].to_csv('/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/processed_fault_data_FINAL.csv', index=False)\n",
    "print(\"Extracted file written.\")\n",
    "print(out[final_cols].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85942c4",
   "metadata": {},
   "source": [
    "ONLY FREE_REMARKS ANALYZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17930d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted file written.\n",
      "                        Size Insulation Voltage          Type  \\\n",
      "0                                         220KV                 \n",
      "1                                                               \n",
      "2                      SQ.MM  PILC+XLPE    11KV  HTCF SECTION   \n",
      "3            .06+70+120SQ.MM  PILC+XLPE    11KV  HTCF SECTION   \n",
      "4  0.15+185+225+300+240SQ.MM  PILC+XLPE    11KV  HTCF SECTION   \n",
      "5                   120SQ.MM       PILC    11KV  HTCF SECTION   \n",
      "6                   120SQ.MM       PILC    11KV  HTCF SECTION   \n",
      "7                   120SQ.MM       PILC    11KV  HTCF SECTION   \n",
      "8                   120SQ.MM       PILC    11KV  HTCF SECTION   \n",
      "9                   120SQ.MM       PILC    11KV  HTCF SECTION   \n",
      "\n",
      "                       FROM FROM_SWITCH                      TO TO_SWITCH  \\\n",
      "0                                                                           \n",
      "1                                                                           \n",
      "2  VISHWESHWAR NAGAR HETALI       40478                  SAMANT     05587   \n",
      "3        JAWAHAR NAGAR NO.3       13847    SONAVALA ESTATE NO.1     00714   \n",
      "4                                                                           \n",
      "5                AAREY NO.2       18066   AAREY UNIT NO.7 KIOSK     18008   \n",
      "6    AAREY SANKRAMAN STUDIO       19112         AAREY UNIT NO.2     19710   \n",
      "7                 AJIT PARK       15725      TUREL PAKHADI NO.1     06900   \n",
      "8       BABREKAR NAGAR NO.1       18669   KANDIVLI HOUSING NO.3     18671   \n",
      "9    BHAGATSINGH NAGAR NO.1       28860  BHAGATSINGH NAGAR NO.3     34127   \n",
      "\n",
      "                                      DELAYED_REASON  \\\n",
      "0                                                      \n",
      "1                                                      \n",
      "2                                                      \n",
      "3  1)FAILED TO OPEN JAWAHAR NAGAR ROAD NO.12 S/S ...   \n",
      "4  1)FAILED TO CLOSE GOREGAON SHOPPING CENTRE S/S...   \n",
      "5  1)AAREY NO.2 S/S (DMS) WENT OFFLINE AFTER GIVI...   \n",
      "6  1) NON DMS SUBSTATION ARE IN ARREY AREA. DMS U...   \n",
      "7  1)TUREL PAKHADI NO.1 S/S (DMS) WENT OFFLINE AF...   \n",
      "8       1) DMS OFFLINE AT KANDIVLI HOUSING NO.3 S/S.   \n",
      "9  1)FAILED TO OPEN BANGUR NAGAR NO.3 S/S DMS SW....   \n",
      "\n",
      "                                        FAULT_NATURE  \n",
      "0                                                     \n",
      "1                                                     \n",
      "2                                                     \n",
      "3  FAILED TO OPEN JAWAHAR NAGAR ROAD NO; FAILED T...  \n",
      "4  FAILED TO CLOSE GOREGAON SHOPPING CENTRE S/S D...  \n",
      "5                  WENT OFFLINE AFTER GIVING COMMAND  \n",
      "6                                                     \n",
      "7  FAILED TO OPEN AJIT PARK S/S DMS SW; WENT OFFL...  \n",
      "8                 DMS OFFLINE AT KANDIVLI HOUSING NO  \n",
      "9                     FAILED TO OPEN BANGUR NAGAR NO  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Load the file\n",
    "df = pd.read_csv('/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/TXN_NMS_HTLOGSHEET_FREE_REMARKS_ONLY.csv', usecols=['FREE_REMARKS'], low_memory=False)\n",
    "\n",
    "# 2. UPPERCASE remarks for consistency\n",
    "df['FREE_REMARKS_UPPER'] = df['FREE_REMARKS'].astype(str).str.upper()\n",
    "\n",
    "# 3. Extraction functions\n",
    "def extract_size(text):\n",
    "    m = re.search(r'X\\s*([\\d\\.\\+\\-\\s]*SQ\\.?\\s*MM)', text)\n",
    "    return m.group(1).replace(\" \", \"\") if m else \"\"\n",
    "\n",
    "def extract_insulation(text):\n",
    "    m = re.search(r'(PILC\\+XLPE|XLPE\\+PILC|PILC|XLPE)', text)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def extract_voltage(text):\n",
    "    m = re.search(r'(\\d{2,3})\\s*KV', text)\n",
    "    return f\"{m.group(1)}KV\" if m else \"\"\n",
    "\n",
    "def extract_type(text):\n",
    "    m = re.search(r'(HTCF SECTION|LT SECTION|HT SECTION)', text)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def extract_section(text):\n",
    "    m = re.search(r'BETWEEN\\s+(.+?)\\s+S/S\\s+SWITCH:([0-9]+)\\s+TO\\s+(.+?)\\s+S/S\\s+SWITCH:([0-9]+)', text)\n",
    "    if m:\n",
    "        return pd.Series([m.group(1).strip(), m.group(2), m.group(3).strip(), m.group(4)])\n",
    "    return pd.Series([\"\", \"\", \"\", \"\"])\n",
    "\n",
    "def extract_delayed_reason(text):\n",
    "    m = re.search(r'DELAYED DUE TO(.*?)(?:NOTIFICATION NO|$)', text)\n",
    "    return m.group(1).strip() if m else \"\"\n",
    "\n",
    "def extract_fault_nature(text):\n",
    "    patterns = [\n",
    "        'CABLE DAMAGED BY', 'DMS OFFLINE AT', 'FPI MALFUNCTION',\n",
    "        'FAILED TO OPEN', 'FAILED TO CLOSE', 'WENT OFFLINE',\n",
    "        'FEEDER TRIPPING', 'TRAFFIC ON', 'SUPPLY RESTORATION DELAYED'\n",
    "    ]\n",
    "    found = []\n",
    "    for pat in patterns:\n",
    "        for m in re.finditer(r'({0}.*?)(?:\\.|,|;|NOTIFICATION NO|$)'.format(re.escape(pat)), text):\n",
    "            found.append(m.group(1).strip())\n",
    "    return '; '.join(found) if found else \"\"\n",
    "\n",
    "# 4. Apply extraction functions\n",
    "out = pd.DataFrame()\n",
    "out['Size'] = df['FREE_REMARKS_UPPER'].apply(extract_size)\n",
    "out['Insulation'] = df['FREE_REMARKS_UPPER'].apply(extract_insulation)\n",
    "out['Voltage'] = df['FREE_REMARKS_UPPER'].apply(extract_voltage)\n",
    "out['Type'] = df['FREE_REMARKS_UPPER'].apply(extract_type)\n",
    "\n",
    "section_cols = df['FREE_REMARKS_UPPER'].apply(extract_section)\n",
    "section_cols.columns = ['FROM', 'FROM_SWITCH', 'TO', 'TO_SWITCH']\n",
    "out = pd.concat([out, section_cols], axis=1)\n",
    "\n",
    "out['DELAYED_REASON'] = df['FREE_REMARKS_UPPER'].apply(extract_delayed_reason)\n",
    "out['FAULT_NATURE'] = df['FREE_REMARKS_UPPER'].apply(extract_fault_nature)\n",
    "\n",
    "# 5. Save results\n",
    "out.to_csv('/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/processed_fault_data_ONLY_FREEREMARKS.csv', index=False)\n",
    "print(\"Extracted file written.\")\n",
    "print(out.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad937e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Parsed 24,228 records  →  /media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/processed_fault_data_ONLY_FREEREMARKS.csv\n",
      "|    | Size                      | Insulation   | Voltage   | Type         | FROM                     |   FROM_SWITCH | TO                     |   TO_SWITCH | DELAYED_REASON                                                                                                                                                                                                                                                                                                                                                                    | FAULT_NATURE   |\n",
      "|---:|:--------------------------|:-------------|:----------|:-------------|:-------------------------|--------------:|:-----------------------|------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------|\n",
      "|  0 | SQ.MM                     | PILC+XLPE    | 11KV      | HTCF SECTION | VISHWESHWAR NAGAR HETALI |         40478 | SAMANT                 |        5587 |                                                                                                                                                                                                                                                                                                                                                                                   |                |\n",
      "|  1 | .06+70+120SQ.MM           | PILC+XLPE    | 11KV      | HTCF SECTION | JAWAHAR NAGAR NO.3       |         13847 | SONAVALA ESTATE NO.1   |         714 | 1)FAILED TO OPEN JAWAHAR NAGAR ROAD NO.12 S/S DMS SW.NO.25658 FAILED TO OPERATE FROM SCADA. 2)FAILED TO OPEN JAWAHAR NAGAR ROAD NO.12 S/S DMS SW.NO.25657 FAILED TO OPERATE FROM SCADA. 3)KASTURI KUNJ S/S (DMS) WENT OFFLINE AFTER TRIPPING.                                                                                                                                     |                |\n",
      "|  2 | 0.15+185+225+300+240SQ.MM | PILC+XLPE    | 11KV      | HTCF SECTION | nan                      |           nan | nan                    |         nan | 1)FAILED TO CLOSE GOREGAON SHOPPING CENTRE S/S DMS SW.NO.04673 FAILED TO OPERATE FROM SCADA.                                                                                                                                                                                                                                                                                      |                |\n",
      "|  3 | 120SQ.MM                  | PILC         | 11KV      | HTCF SECTION | AAREY NO.2               |         18066 | AAREY UNIT NO.7 KIOSK  |       18008 | 1)AAREY NO.2 S/S (DMS) WENT OFFLINE AFTER GIVING COMMAND. 2)FPI FAULTY AT AAREY UNIT NO.7 KIOSK S/S..                                                                                                                                                                                                                                                                             |                |\n",
      "|  4 | 120SQ.MM                  | PILC         | 11KV      | HTCF SECTION | AAREY SANKRAMAN STUDIO   |         19112 | AAREY UNIT NO.2        |       19710 | 1) NON DMS SUBSTATION ARE IN ARREY AREA. DMS USED FOR ISOLATION AND RESTORATION.                                                                                                                                                                                                                                                                                                  |                |\n",
      "|  5 | 120SQ.MM                  | PILC         | 11KV      | HTCF SECTION | AJIT PARK                |         15725 | TUREL PAKHADI NO.1     |        6900 | 1)TUREL PAKHADI NO.1 S/S (DMS) WENT OFFLINE AFTER TRIPPING. 2)LIBERTY GARDEN SOUTH NO.2 S/S (DMS) WENT OFFLINE AFTER TRIPPING. 3) FEEDER TRIPPING AT PALM COURT REC-STN SW.NO.27081. 4)FAILED TO OPEN AJIT PARK S/S DMS SW.NO.15725 FAILED TO OPERATE FROM SCADA.                                                                                                                 |                |\n",
      "|  6 | 120SQ.MM                  | PILC         | 11KV      | HTCF SECTION | BABREKAR NAGAR NO.1      |         18669 | KANDIVLI HOUSING NO.3  |       18671 | 1) DMS OFFLINE AT KANDIVLI HOUSING NO.3 S/S.                                                                                                                                                                                                                                                                                                                                      |                |\n",
      "|  7 | 120SQ.MM                  | PILC         | 11KV      | HTCF SECTION | BHAGATSINGH NAGAR NO.1   |         28860 | BHAGATSINGH NAGAR NO.3 |       34127 | 1)FAILED TO OPEN BANGUR NAGAR NO.3 S/S DMS SW.NO.31181 FAILED TO OPERATE FROM SCADA. DMS OPERATED AFTER GIVING MULTIPLE COMMANDS.                                                                                                                                                                                                                                                 |                |\n",
      "|  8 | 120SQ.MM                  | PILC         | 11KV      | HTCF SECTION | DINDOSHI VASAHAT CENTRAL |         17141 | DINDOSHI VASAHAT SOUTH |       14101 |                                                                                                                                                                                                                                                                                                                                                                                   |                |\n",
      "|  9 | 120SQ.MM                  | PILC         | 11KV      | HTCF SECTION | GOKULDHAM NO.2           |          7636 | GOKULDHAM NO.1         |        6877 | 1) FPI MALFUNCTION AT GOKULDHAM NO.1 S/S SW.NO.06877 (GLOWN). 2) FPI MALFUNCTION AT GOKULDHAM NO.1 S/S SW.NO.06878 (GLOWN). 3)FAILED TO OPEN GOKULDHAM NO.1 S/S DMS SW.NO.06877 FAILED TO OPERATE FROM SCADA. 4)FAILED TO OPEN GOKULDHAM NO.1 S/S DMS SW.NO.06878 FAILED TO OPERATE FROM SCADA. 5)FAILED TO OPEN GOKULDHAM NO.1 S/S DMS SW.NO.06892 FAILED TO OPERATE FROM SCADA. |                |\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -----------------------------------------------------------\n",
    "#  Pull structured fields out of HT-cable FREE_REMARKS strings\n",
    "# -----------------------------------------------------------\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# ─── Edit these two paths ───────────────────────────────────\n",
    "SRC  = r'/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/Book1.csv'   # has FREE_REMARKS column\n",
    "DEST = r'/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/processed_fault_data_ONLY_FREEREMARKS.csv'\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1) Load and normalise ----------------------------------------------------------\n",
    "df_txt = (\n",
    "    pd.read_csv(SRC, usecols=['FREE_REMARKS'], low_memory=False)\n",
    "      .fillna('')\n",
    "      .assign(TXT=lambda d: d['FREE_REMARKS'].str.upper())\n",
    ")\n",
    "\n",
    "# 2) Pre-compile the regexes -----------------------------------------------------\n",
    "RE_SIZE        = re.compile(r'\\bX\\s*([\\d+\\-.\\s]*SQ\\.?\\s*MM)')            # 120SQ.MM  0.15+185+240SQ.MM\n",
    "RE_INSULATION  = re.compile(r'(PILC\\+XLPE|XLPE\\+PILC|PILC|XLPE)')\n",
    "RE_VOLTAGE     = re.compile(r'(\\d{2,3})\\s*KV')\n",
    "RE_TYPE        = re.compile(r'(HTCF SECTION|LT SECTION|HT SECTION)')\n",
    "RE_SECTION     = re.compile(\n",
    "    r'BETWEEN\\s+(.+?)\\s+S/S\\s+SW\\.?ITCH:?\\s*([0-9]+)\\s+TO\\s+(.+?)\\s+S/S\\s+SW\\.?ITCH:?\\s*([0-9]+)',\n",
    "    flags=re.I\n",
    ")\n",
    "RE_DELAYED     = re.compile(\n",
    "    r'(?:ISOLATION\\s*&?\\s*RESTORATION\\s+)?DELAYED (?:AS|DUE TO)\\s*(.*?)(?:NOTIFICATION NO|$)',\n",
    "    flags=re.I\n",
    ")\n",
    "\n",
    "FAULT_TOKENS = [\n",
    "    'CABLE DAMAGED BY', 'CABLE DAMAGED',     # covers slightly shorter variant\n",
    "    'DMS OFFLINE AT', 'WENT OFFLINE',\n",
    "    'FPI MALFUNCTION', 'FPI FAULTY',\n",
    "    'FAILED TO OPEN',  'FAILED TO CLOSE',\n",
    "    'FEEDER TRIPPING', 'TRAFFIC ON',\n",
    "    'SUPPLY RESTORTION DELAYED', 'SUPPLY RESTORATION DELAYED'\n",
    "]\n",
    "# create one big alternation for speed\n",
    "RE_FAULT = re.compile(\n",
    "    '(' + '|'.join(re.escape(tok) for tok in FAULT_TOKENS) + r'.*?)(?:\\.|,|;|NOTIFICATION NO|$)',\n",
    "    flags=re.I\n",
    ")\n",
    "\n",
    "# 3) Utility ---------------------------------------------------------------------\n",
    "def _first(regex, text, fmt=lambda m: m.group(1)):\n",
    "    m = regex.search(text)\n",
    "    return fmt(m) if m else ''\n",
    "\n",
    "def _faults(text: str) -> str:\n",
    "    return '; '.join(m.group(1).strip() for m in RE_FAULT.finditer(text))\n",
    "\n",
    "# 4) Column extraction -----------------------------------------------------------\n",
    "out = pd.DataFrame({\n",
    "    'Size'       : df_txt['TXT'].apply(lambda t: _first(RE_SIZE, t, lambda m: m.group(1).replace(' ', ''))),\n",
    "    'Insulation' : df_txt['TXT'].apply(lambda t: _first(RE_INSULATION, t)),\n",
    "    'Voltage'    : df_txt['TXT'].apply(lambda t: _first(RE_VOLTAGE, t, lambda m: f\"{m.group(1)}KV\")),\n",
    "    'Type'       : df_txt['TXT'].apply(lambda t: _first(RE_TYPE, t))\n",
    "})\n",
    "\n",
    "# --- FROM / TO / SWITCH columns -------------------------------------------------\n",
    "sec = df_txt['TXT'].str.extract(RE_SECTION)\n",
    "sec.columns = ['FROM', 'FROM_SWITCH', 'TO', 'TO_SWITCH']\n",
    "out = pd.concat([out, sec], axis=1)\n",
    "\n",
    "# --- delay / fault --------------------------------------------------------------\n",
    "out['DELAYED_REASON'] = df_txt['TXT'].apply(lambda t: _first(RE_DELAYED, t).strip())\n",
    "out['FAULT_NATURE']   = df_txt['TXT'].apply(_faults)\n",
    "\n",
    "# 5) Final tidy-up & write -------------------------------------------------------\n",
    "out = out[['Size','Insulation','Voltage','Type',\n",
    "           'FROM','FROM_SWITCH','TO','TO_SWITCH',\n",
    "           'DELAYED_REASON','FAULT_NATURE']]\n",
    "\n",
    "out.to_csv(DEST, index=False)\n",
    "print(f'✓ Parsed {len(out):,} records  →  {DEST}')\n",
    "print(out.head(10).to_markdown())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6e9fd",
   "metadata": {},
   "source": [
    "ANALYZE THE DESCRIPTION COLUMN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c2b243c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows containing the word \"SWITCH\": 21584\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows where the DESCRIPTION column contains the word \"SWITCH\"\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/DESCRIPTION.csv', low_memory=False)\n",
    "rows_with_switch = df['DESCRIPTION'].str.contains(r'\\bSWITCH\\b', case=False, na=False).sum()\n",
    "print(f'Number of rows containing the word \"SWITCH\": {rows_with_switch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e7390b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23939"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)  # Total number of rows in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd18e39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows containing the word \"11kV\": 19598\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rows_with_switch = df['DESCRIPTION'].str.contains(r'\\b11 KV\\b', case=False, na=False).sum()\n",
    "print(f'Number of rows containing the word \"11kV\": {rows_with_switch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab973a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows containing the word \"22kV\": 627\n",
      "Number of rows containing the word \"33kV\": 1194\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "rows_with_switch_22 = df['DESCRIPTION'].str.contains(r'\\b22 KV\\b', case=False, na=False).sum()\n",
    "row_with_33 = df['DESCRIPTION'].str.contains(r'\\b33 KV\\b', case=False, na=False).sum()\n",
    "print(f'Number of rows containing the word \"22kV\": {rows_with_switch_22}')\n",
    "print(f'Number of rows containing the word \"33kV\": {row_with_33}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16adce0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows containing \"SWITCH\", \"11kV\", \"22kV\", or \"33kV\": 21419\n"
     ]
    }
   ],
   "source": [
    "total = rows_with_switch + row_with_33 + rows_with_switch_22\n",
    "print(f'Total number of rows containing \"SWITCH\", \"11kV\", \"22kV\", or \"33kV\": {total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1133ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows containing the word \"SWITCH\": 21749\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows where the DESCRIPTION column contains the word \"SWITCH\"\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/TXN_NMS_HTLOGSHEET_FREE_REMARKS_ONLY_clean.csv', low_memory=False)\n",
    "rows_with_switch = df['FREE_REMARKS'].str.contains(r'\\bSWITCH\\b', case=False, na=False).sum()\n",
    "print(f'Number of rows containing the word \"SWITCH\": {rows_with_switch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d703585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows containing the word \"11kV\": 19777\n",
      "Number of rows containing the word \"22kV\": 680\n",
      "Number of rows containing the word \"33kV\": 1294\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rows_with_switch = df['FREE_REMARKS'].str.contains(r'\\b11 KV\\b', case=False, na=False).sum()\n",
    "print(f'Number of rows containing the word \"11kV\": {rows_with_switch}')\n",
    "\n",
    "\n",
    "rows_with_switch_22 = df['FREE_REMARKS'].str.contains(r'\\b22 KV\\b', case=False, na=False).sum()\n",
    "row_with_33 = df['FREE_REMARKS'].str.contains(r'\\b33 KV\\b', case=False, na=False).sum()\n",
    "print(f'Number of rows containing the word \"22kV\": {rows_with_switch_22}')\n",
    "print(f'Number of rows containing the word \"33kV\": {row_with_33}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fec9a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows containing \"SWITCH\", \"11kV\", \"22kV\", or \"33kV\": 21751\n"
     ]
    }
   ],
   "source": [
    "total = rows_with_switch + row_with_33 + rows_with_switch_22\n",
    "print(f'Total number of rows containing \"SWITCH\", \"11kV\", \"22kV\", or \"33kV\": {total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f089af",
   "metadata": {},
   "source": [
    "MAtching the feeder id with thE cleaned  DATA OF FAULT collmn SWITCH_NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb029aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /media/sagarkumar/New Volume/SAGAR/HTLOGSHEET_CLEAN_enetrytype_1_cleaned.csv...\n",
      "  - Found 1268 unique, clean values.\n",
      "Processing file: /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/final_two_column_with_rank_11_withoutDT.csv...\n",
      "  - Found 945 unique, clean values.\n",
      "\n",
      "--- Comparison Report ---\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/HTLOGSHEET_CLEAN_enetrytype_1_cleaned.csv' (Column: SWITCH_NO): 1268\n",
      "Unique values in '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/final_two_column_with_rank_11_withoutDT.csv' (Column: FEEDER_ID): 945\n",
      "-------------------------\n",
      "Number of values that matched: 824\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION: PLEASE EDIT THIS SECTION ---\n",
    "\n",
    "# File 1 Details\n",
    "file1_path = \"/media/sagarkumar/New Volume/SAGAR/HTLOGSHEET_CLEAN_enetrytype_1_cleaned.csv\"\n",
    "file1_column_name = \"SWITCH_NO\"\n",
    "\n",
    "# File 2 Details\n",
    "file2_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/final_two_column_with_rank_11_withoutDT.csv\"\n",
    "file2_column_name = \"FEEDER_ID\"\n",
    "\n",
    "\n",
    "\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "\n",
    "def find_actual_column_name(columns, target_name):\n",
    "    \"\"\"Helper function to find a column name, ignoring case.\"\"\"\n",
    "    for col in columns:\n",
    "        if str(col).lower() == str(target_name).lower():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def get_unique_values_from_file(filepath, column_name):\n",
    "    \"\"\"\n",
    "    Reads a file, extracts unique values from a specific column,\n",
    "    cleans them, and returns them as a set.\n",
    "    \"\"\"\n",
    "    print(f\"Processing file: {filepath}...\")\n",
    "    try:\n",
    "        # Read just the header to find the correct column name (case-insensitive)\n",
    "        header_df = pd.read_csv(filepath, nrows=0, on_bad_lines='skip')\n",
    "        actual_col_name = find_actual_column_name(header_df.columns, column_name)\n",
    "\n",
    "        if not actual_col_name:\n",
    "            print(f\"  - Error: Column '{column_name}' not found. Please check the column name.\")\n",
    "            return set()\n",
    "\n",
    "        # Read the full column using the correct name\n",
    "        df = pd.read_csv(filepath, usecols=[actual_col_name], on_bad_lines='skip')\n",
    "        \n",
    "        # --- Data Cleaning ---\n",
    "        # Convert all values to string, extract digits, and then convert to numbers.\n",
    "        # This handles mixed data types (e.g., '123' vs 123) and text prefixes (e.g., 'SW-123').\n",
    "        s = pd.Series(df[actual_col_name].dropna().unique(), dtype=str)\n",
    "        s = s.str.extract('(\\d+)').iloc[:, 0]\n",
    "        s = pd.to_numeric(s, errors='coerce')\n",
    "        \n",
    "        cleaned_values = set(s.dropna().astype(int))\n",
    "        \n",
    "        print(f\"  - Found {len(cleaned_values)} unique, clean values.\")\n",
    "        return cleaned_values\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  - Error: File not found. Please check the path: {filepath}\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"  - An unexpected error occurred: {e}\")\n",
    "        return set()\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# Get the unique values from both files\n",
    "unique_values_from_file1 = get_unique_values_from_file(file1_path, file1_column_name)\n",
    "unique_values_from_file2 = get_unique_values_from_file(file2_path, file2_column_name)\n",
    "\n",
    "# --- Comparison and Reporting ---\n",
    "\n",
    "print(\"\\n--- Comparison Report ---\")\n",
    "if not unique_values_from_file1 or not unique_values_from_file2:\n",
    "    print(\"Could not perform comparison because one of the files could not be processed or contained no valid data.\")\n",
    "else:\n",
    "    # Use set intersection to find the values that exist in both sets\n",
    "    matching_values = unique_values_from_file1.intersection(unique_values_from_file2)\n",
    "    \n",
    "    # Print the final report\n",
    "    print(f\"Unique values in '{file1_path}' (Column: {file1_column_name}): {len(unique_values_from_file1)}\")\n",
    "    print(f\"Unique values in '{file2_path}' (Column: {file2_column_name}): {len(unique_values_from_file2)}\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"Number of values that matched: {len(matching_values)}\")\n",
    "    print(\"-\" * 25)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a19d060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching values (sorted):\n",
      "Matching values saved to: /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/order_id.csv\n"
     ]
    }
   ],
   "source": [
    "matching_values_list = sorted(matching_values)\n",
    "print(\"Matching values (sorted):\")\n",
    "output_file_path = \"/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/order_id.csv\"\n",
    "matching_values_df = pd.DataFrame(matching_values_list, columns=[file1_column_name])\n",
    "matching_values_df.to_csv(output_file_path, index=False)\n",
    "print(f\"Matching values saved to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867df427",
   "metadata": {},
   "source": [
    "ORDER_ID AND SWITCH NO IN ORDER ID MATCH WITH SWITCH NO IS 89 BUT ORDER_ID MATCH THE FEDER _ID WITH 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c8359c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of values that matched: 89\n",
      "Total matching values: 99\n"
     ]
    }
   ],
   "source": [
    "match = unique_values_from_file1.intersection(matching_values_list)\n",
    "print(f\"Number of values that matched: {len(match)}\")\n",
    "\n",
    "\n",
    "print(f\"Total matching values: {len(matching_values_list)}\")\n",
    "match_list = sorted(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cc29eff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values in 'match' but not in 'matching_values_list': [26332, 28192, 29986, 30467, 30668, 32164, 32529, 32530, 34678, 39411]\n",
      "Count: 10\n"
     ]
    }
   ],
   "source": [
    "# Find values in match that are NOT in matching_values_list\n",
    "diff_match_vs_matching_values_list = sorted(set(matching_values_list) - set(match))\n",
    "print(\"Values in 'match' but not in 'matching_values_list':\", diff_match_vs_matching_values_list)\n",
    "print(f\"Count: {len(diff_match_vs_matching_values_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62d79f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern1 matched: ('VISHWESHWAR NAGAR HETALI', '40478', 'SAMANT', '05587')\n",
      "Pattern1 matched: ('JAWAHAR NAGAR NO.3', '13847', 'SONAVALA ESTATE NO.1', '00714')\n",
      "No match: 1) 3C X 0.15+185+225+300+240 SQ. MM PILC+XLPE 11 KV HTCF SECTION BETWEEN GOREGAON REC-STN SWITCH:01523 TO SIDDHARTHA NAGAR NO.1 S/S SWITCH:02310. DMS USED FOR ISOLATION AND RESTORATION. ISOLATION & RESTORATION DELAYED DUE TO 1)FAILED TO CLOSE GOREGAON SHOPPING CENTRE S/S DMS SW.NO.04673 FAILED TO OPERATE FROM SCADA. NOTIFICATION NO:- 002001589689\n",
      "Pattern1 matched: ('AAREY NO.2', '18066', 'AAREY UNIT NO.7 KIOSK', '18008')\n",
      "Pattern1 matched: ('AAREY SANKRAMAN STUDIO', '19112', 'AAREY UNIT NO.2', '19710')\n",
      "Pattern1 matched: ('AJIT PARK', '15725', 'TUREL PAKHADI NO.1', '06900')\n",
      "Pattern1 matched: ('BABREKAR NAGAR NO.1', '18669', 'KANDIVLI HOUSING NO.3', '18671')\n",
      "Pattern1 matched: ('BHAGATSINGH NAGAR NO.1', '28860', 'BHAGATSINGH NAGAR NO.3', '34127')\n",
      "Pattern1 matched: ('DINDOSHI VASAHAT CENTRAL', '17141', 'DINDOSHI VASAHAT SOUTH', '14101')\n",
      "Pattern1 matched: ('GOKULDHAM NO.2', '07636', 'GOKULDHAM NO.1', '06877')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Test on a small subset first\n",
    "df = pd.read_csv('HTLOGSHEET_CLEAN_enetrytype_1_cleaned.csv', low_memory=False)\n",
    "\n",
    "def extract_station_switch(remark):\n",
    "    remark = str(remark)\n",
    "    # Pattern 1: S/S SWITCH:\n",
    "    pattern1 = (\n",
    "        r'BETWEEN\\s+(.+?)\\s+S/S\\s+SWITCH[:\\s]*([A-Z0-9]+)\\s+TO\\s+(.+?)\\s+S/S\\s+SWITCH[:\\s]*([A-Z0-9]+)'\n",
    "    )\n",
    "    m1 = re.search(pattern1, remark, re.IGNORECASE)\n",
    "    if m1:\n",
    "        # For debug: print matched groups\n",
    "        print(\"Pattern1 matched:\", m1.groups())\n",
    "        return pd.Series([m1.group(1).strip(), m1.group(2).strip(), m1.group(3).strip(), m1.group(4).strip()])\n",
    "    # Pattern 2: REC-STN SWITCH:\n",
    "    pattern2 = (\n",
    "        r'BETWEEN\\s+(.+?)\\s+REC[-\\s]?STN\\s+SWITCH[:\\s]*([A-Z0-9]+)\\s+TO\\s+(.+?)\\s+REC[-\\s]?STN\\s+SWITCH[:\\s]*([A-Z0-9]+)'\n",
    "    )\n",
    "    m2 = re.search(pattern2, remark, re.IGNORECASE)\n",
    "    if m2:\n",
    "        print(\"Pattern2 matched:\", m2.groups())\n",
    "        return pd.Series([m2.group(1).strip(), m2.group(2).strip(), m2.group(3).strip(), m2.group(4).strip()])\n",
    "    # Pattern 3: S/S SW.NO.\n",
    "    pattern3 = (\n",
    "        r'BETWEEN\\s+(.+?)\\s+S/S\\s+SW\\.NO\\.[:\\s]*([A-Z0-9]+)\\s+TO\\s+(.+?)\\s+S/S\\s+SW\\.NO\\.[:\\s]*([A-Z0-9]+)'\n",
    "    )\n",
    "    m3 = re.search(pattern3, remark, re.IGNORECASE)\n",
    "    if m3:\n",
    "        print(\"Pattern3 matched:\", m3.groups())\n",
    "        return pd.Series([m3.group(1).strip(), m3.group(2).strip(), m3.group(3).strip(), m3.group(4).strip()])\n",
    "    # If nothing matches, return blanks (never the full remark!)\n",
    "    print(\"No match:\", remark)\n",
    "    return pd.Series(['', '', '', ''])\n",
    "\n",
    "# Try on first 10 rows (for debug)\n",
    "test_df = df.head(10).copy()\n",
    "test_df[['SOURCE_SS', 'FROM_SWITCH', 'DESTINATION_SS', 'TO_SWITCH']] = test_df['FREE_REMARKS'].apply(extract_station_switch)\n",
    "\n",
    "# See what gets printed for your rows!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe4c0ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO MATCH] REMARK: 1) 3C X 300 SQ. MM PILC 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK05_10 TO TILAK NAGAR REC-STN SWITCH:33054. NOTIFICATION NO:- 002001590522\n",
      "[NO MATCH] REMARK: 1) 3C X 300 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK02_03 TO VIKHROLI REC-STN SWITCH:01171. NOTIFICATION NO:- 002001610983\n",
      "[NO MATCH] REMARK: 1) 3C X 300+400 SQ. MM PILC+XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK07_04 TO TAGORE NGR. REC-STN SWITCH:01103. NOTIFICATION NO:- 002001644852\n",
      "[NO MATCH] REMARK: 1) 3C X 400 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK06_01 TO VIKHROLI REC-STN SWITCH:01135. CABLE DAMAGED BY MCGM BRIGDE DEPT ON ANDHERI GHATKOPAR LINK RD OPP. GHAT BUS DEPT NEAR GODREJ COMPOUND WALL NEAR LAXMI NGR NALA. NOTIFICATION NO:- 002001646319\n",
      "[NO MATCH] REMARK: 1) 3C X 400 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK07_04 TO SHIVAJI NAGAR REC-STN SWITCH:22001. NOTIFICATION NO:- 002001591073\n",
      "[NO MATCH] REMARK: 1) 3C X 400 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK08_14 TO CHHEDA NAGAR REC-STN SWITCH:33069. DMS USED FOR RESTORATION. NOTIFICATION NO:- 002001581605\n",
      "[NO MATCH] REMARK: 1) 3C X 400+300 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK07_04 TO TAGORE NGR. REC-STN SWITCH:01103. NOTIFICATION NO:- 002001602527\n",
      "[NO MATCH] REMARK: nan\n",
      "[NO MATCH] REMARK: 1) 3C X 400 SQ. MM XLPE 33 KV HTCF SECTION BETWEEN TO OSHIWARA RECEIVING STATION S/S SWITCH:33294. DMS USED FOR ISOLATION AND RESTORATION. NOTIFICATION NO:- 002002044113\n",
      "[NO MATCH] REMARK: CABLE DAMAGE BY MCGM OPPO JAY LAXMI CSS GATE 1) 3C X 300+400 SQ. MM XLPE 11 KV HTCF SECTION BETWEEN CHEMBUR TULASI SWITCH : 43996 TO JAILAXMI SWITCH : 30127. DMS USED FOR ISOLATION AND RESTORATION. ISOLATION & RESTORATION DELAYED DUE TO 1)FAILED TO OPEN - MULTIPLE COMMAND JAILAXMI S/S DMS SW.NO.30127 FAILED TO OPERATE FROM SCADA. 2)FAILED TO OPEN - MULTIPLE COMMAND JAILAXMI S/S DMS SW.NO.30126 FAILED TO OPERATE FROM SCADA. 3)FAILED TO CLOSE - MULTIPLE COMMAND BHAGYAWAN S/S DMS SW.NO.07963 FAILED TO OPERATE FROM SCADA. 4)FAILED TO OPEN - MULTIPLE COMMAND BHAGYAWAN S/S DMS SW.NO.07963 FAILED TO OPERATE FROM SCADA. DI:JAILAXMI CI:1316 CHL:416.73.\n",
      "[NO MATCH] REMARK: 1) 3C X 240+300 SQ. MM PILC+XLPE 11 KV HTCF SECTION BETWEEN EAST-WEST APT.S/S SWITCH:19107 TO KANDIVALI SAIKRIPA S/S SWITCH:34068. DMS USED FOR ISOLATION AND RESTORATION. ISOLATION & RESTORATION DELAYED DUE TO 1)RNA ROYAL PARK REC-STN WENT OFFLINE AFTER TRIPPING. CABLE DAMAGED BY MBMC OPP KANDIVALI SAI KRUPA S/S.. NOTIFICATION NO:- 002002035352\n",
      "[NO MATCH] REMARK: nan\n",
      "[NO MATCH] REMARK: nan\n",
      "[NO MATCH] REMARK: 1) 3C X SQ. MM PILC+XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK_04 TO SHIVAJI NAGAR REC-STN SWITCH:22001. NO INTERRUPTION TO CONSUMER AS LOAD WAS ALREADY TRANSFERRED. NOTIFICATION NO:- 002001810894\n",
      "[NO MATCH] REMARK: 1) 3C X 300 SQ. MM PILC 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK05_10 TO TILAK NAGAR REC-STN SWITCH:33054. NOTIFICATION NO:- 002001780481\n",
      "[NO MATCH] REMARK: 1) 3C X 300 SQ. MM PILC 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK05_10 TO TILAK NAGAR REC-STN SWITCH:33054. NOTIFICATION NO:- 002001793892\n",
      "[NO MATCH] REMARK: 1) 3C X 300 SQ. MM PILC 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK05_10 TO TILAK NAGAR REC-STN SWITCH:33054. NOTIFICATION NO:- 002001836718\n",
      "[NO MATCH] REMARK: 1) 3C X 300 SQ. MM PILC 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK07_04 TO TAGORE NGR. REC-STN SWITCH:01103. DMS USED FOR RESTORATION. NOTIFICATION NO:- 1\n",
      "[NO MATCH] REMARK: 1) 3C X 300 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK05_10 TO TILAK NAGAR REC-STN SWITCH:33054. ISOLATION & RESTORATION DELAYED DUE TO 1)FAILED TO CLOSE TILAK NAGAR REC-STN SW.NO.31980 FAILED TO OPERATE FROM SCADA. 2)FAILED TO OPEN TILAK NAGAR REC-STN SW.NO.33054 FAILED TO OPERATE FROM SCADA. NOTIFICATION NO:- 002001843818\n",
      "[NO MATCH] REMARK: 1) 3C X 300 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK05_10 TO TILAK NAGAR REC-STN SWITCH:33054. NOTIFICATION NO:- 002001768953\n",
      "[NO MATCH] REMARK: 1) 3C X 300 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK05_10 TO TILAK NAGAR REC-STN SWITCH:33054. NOTIFICATION NO:- 002001777443\n",
      "[NO MATCH] REMARK: NOTIFICATION NO:- 1\n",
      "[NO MATCH] REMARK: nan\n",
      "[NO MATCH] REMARK: 1) 3C X 300 SQ. MM PILC 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK05_10 TO TILAK NAGAR REC-STN SWITCH:33054. NOTIFICATION NO:- 002001747250\n",
      "[NO MATCH] REMARK: 1) 3C X 300+240 SQ. MM XLPE 11 KV HTCF SECTION BETWEEN RNA ROYAL PARK REC-STN SWITCH:34734 TO EAST-WEST APT.S/S SWITCH:19109. ISOLATION & RESTORATION DELAYED DUE TO 1)PRANIK GARDEN S/S (DMS) WENT OFFLINE 2) FPI MALFUNCTION AT PUSHP MEET CSS 29944 (GLOWN) 3)FPI MALFUNCTION AT EAST WEST APPT CSS 19107 (GLOWN). SUSPECTED CABLE BURNT ON M G ROAD NEAR SANGHAVI INDUSTRIES CSS GATE. NOTIFICATION NO:- 002001726803\n",
      "[NO MATCH] REMARK: 1) 3C X 300+400 SQ. MM PILC+XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK05_10 TO TILAK NAGAR REC-STN SWITCH:33054. NOTIFICATION NO:- 002001743098\n",
      "[NO MATCH] REMARK: 1) 3C X 300+400 SQ. MM PILC+XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK07_04 TO TAGORE NGR. REC-STN SWITCH:01103. NOTIFICATION NO:- 002001691049\n",
      "[NO MATCH] REMARK: 1) 3C X 400 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK07_04 TO SHIVAJI NAGAR REC-STN SWITCH:22001. NOTIFICATION NO:- 002001684424\n",
      "[NO MATCH] REMARK: 1) 3C X 400 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK09_12 TO TAGORE NGR. REC-STN SWITCH:33100. NOTIFICATION NO:- 002001729801\n",
      "[NO MATCH] REMARK: 1) 3C X 400+300 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK07_04 TO TAGORE NGR. REC-STN SWITCH:01103. NO SUPPLY INTERRUPTION AS LOAD WAS ALREADY TRANSFERRED UNDER PLANNED OUTAGE.. NOTIFICATION NO:- 0\n",
      "[NO MATCH] REMARK: TRIPPED DUE TO R-PHASE JUMPER BURNT AT TOWER 76A MONOPOLE.. RESTORATION DELAYED DUE TO 1)FAILED TO CLOSE VIRVANI IND.NO.1 S/S DMS SW.NO.05168 FAILED TO OPERATE FROM SCADA. 2)FAILED TO CLOSE GANDHI NAGAR NO.4 S/S DMS SW.NO.11378 FAILED TO OPERATE FROM SCADA. 3) DMS OFFLINE AT OBEROI MALL S/S. RESTORATION DELAYED DUE TO ONGOING OUTAGE OF 33KV AEML GOREGAON FEEDER 14 ( DINDOSHI 20 MVA-T2+ OMKAR 20 MVA-T2+NIRLON 20 MVA-T3+ NESCO 20MVA-T1).\n",
      "[NO MATCH] REMARK: NOTIFICATION NO:- 111\n",
      "[NO MATCH] REMARK: 1) 3C X 300 SQ. MM PILC 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK02_03 TO VIKHROLI REC-STN SWITCH:01171. NOTIFICATION NO:- 002001492867\n",
      "[NO MATCH] REMARK: 1) 3C X 400 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK02_03 TO NATWAR PAREKH REC-STN SWITCH:33214. NOTIFICATION NO:- 002001536786\n",
      "[NO MATCH] REMARK: 1) 3C X 400 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK02_03 TO NATWAR PAREKH REC-STN SWITCH:33214. NOTIFICATION NO:- 002001511987\n",
      "[NO MATCH] REMARK: 1) 3C X 300 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK05_10 TO TILAK NAGAR REC-STN SWITCH:33054. DMS USED FOR RESTORATION. NOTIFICATION NO:- 002001531722\n",
      "[NO MATCH] REMARK: 1) 3C X 400 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK06_01 TO VIKHROLI REC-STN SWITCH:01135. NO SUPPLY INTRUPTION AS TATA PALNNED OUTAGE GOING ON.. NOTIFICATION NO:- 002001515278\n",
      "[NO MATCH] REMARK: 1) 3C X 300+400 SQ. MM PILC+XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK07_04 TO TAGORE NGR. REC-STN SWITCH:01103. ISOLATION & RESTORATION DELAYED DUE TO 1)FAILED TO CLOSE TAGORE NGR. REC-STN SW.NO.10562 FAILED TO OPERATE FROM SCADA. NOTIFICATION NO:- 002001502406\n",
      "[NO MATCH] REMARK: 1) 3C X 400 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK09_12 TO HINGWALA LANE REC-STN SWITCH:33163. DMS USED FOR ISOLATION AND RESTORATION. NOTIFICATION NO:- 002001576917\n",
      "[NO MATCH] REMARK: NO FAULT AFTER TESTING. NOTIFICATION NO:- 002001284566\n",
      "[NO MATCH] REMARK: nan\n",
      "[NO MATCH] REMARK: 1) 3C X SQ. MM PILC+XLPE 11 KV HTCF SECTION BETWEEN 38664 S/S SWITCH:SHYAM UPAVAN TO 38789 S/S SWITCH:LION PENCIL NEW. DMS USED FOR ISOLATION AND RESTORATION. ISOLATION & RESTORATION DELAYED DUE TO 1) FPI MALFUNCTION AT LION PENCIL NEW S/S SW.NO.38789 (GLOWN). NOTIFICATION NO:- .\n",
      "[NO MATCH] REMARK: 1) 3C X 240+300 SQ. MM PILC+XLPE 11 KV HTCF SECTION BETWEEN EAST-WEST APT.S/S SWITCH:19107 TO KANDIVALI SAIKRIPA S/S SWITCH:34068. DMS USED FOR ISOLATION AND RESTORATION. NOTIFICATION NO:- 002001224377\n",
      "[NO MATCH] REMARK: 1) 3C X SQ. MM PILC+XLPE 33 KV HTCF SECTION BETWEEN TATA POWAI REC-STN SWITCH:T33POWAI OG6 TO VIKHROLI REC-STN SWITCH:33011. NOTIFICATION NO:- 002001304637\n",
      "[NO MATCH] REMARK: 1) 3C X SQ. MM PILC+XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK02_03 TO NATWAR PAREKH REC-STN SWITCH:33214. RESTORATION DELAY DUE TO NATWAR PAREKH R/S OFFLINE. NOTIFICATION NO:- 002001214065\n",
      "[NO MATCH] REMARK: 1) 3C X 0.3 SQ. MM PILC 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK05_10 TO TILAK NAGAR REC-STN SWITCH:33054. RESTORATION DELAYED DUE TO MULTIPLE 22 KV CABLE FAULT IN SAME R/S. NOTIFICATION NO:- 002001224065\n",
      "[NO MATCH] REMARK: 1) 3C X 400 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK07_04 TO SHIVAJI NAGAR REC-STN SWITCH:22001. NOTIFICATION NO:- 00\n",
      "[NO MATCH] REMARK: 1) 3C X 400 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK08_14 TO KALPATARU LBS R/S S/S SWITCH:33216. NOTIFICATION NO:- 002001283468\n",
      "[NO MATCH] REMARK: 1) 3C X 3C X 400 SQ. MM PILC 22 KV HTCF SECTION BETWEEN TATA S VIKROLI REC-STN SWITCH:T22VIK11_13 TO KALPTARU LBS REC-STN SWITCH:33217. NO SUPPLY INTERRUPTION TO ANY CONSUMER. NOTIFICATION NO:- 002001263342\n",
      "[NO MATCH] REMARK: 1) 3C X 400 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA`S VIKHROLI REC-STN SWITCH:T22VIK02_03 TO NATWAR PAREKH REC-STN SWITCH:33214. CABLE DAMAGED BY MCGM OPP. JAY MOTORS ON G.M. LINK RD. NOTIFICATION NO:- 002001293830\n",
      "[NO MATCH] REMARK: nan\n",
      "[NO MATCH] REMARK: 3C X 400 SQ. MM XLPE 33 KV HTCF SECTION BETWEEN 220 KV R INFRA VERSOVA REC-STN SWITCH:220VSV14 TO MALAD R/S 20 MVA-1 (STANDBY). NO SUPPLY INTERUPTION AS THIS IS A STANDBY FEEDER.\n",
      "[NO MATCH] REMARK: DMS USED FOR ISOLATION AND RESTORATION.\n",
      "[NO MATCH] REMARK: NOTIFICATION NO:- 200089987667\n",
      "[NO MATCH] REMARK: 1) 3C X 400 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA`S VIKHROLI REC-STN SWITCH:T22VIK07_04 TO SHIVAJI NAGAR REC-STN SWITCH:22001. CABLE DAMAGED BY MCGM NEAR SHIVAJI NAGAR DUMPING GROUND. NOTIFICATION NO:- 002001199381\n",
      "[NO MATCH] REMARK: 1) 3C X 400 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA`S VIKROLI REC-STN SWITCH:T22VIK09_12 TO TAGORE NGR. REC-STN SWITCH:33100. CABLE DAMAGED BY BMC BEHIND VIDYA DARSHAN S/S ON EASTERN EXPRESS HIGHWAY. NOTIFICATION NO:- 002001205347\n",
      "[NO MATCH] REMARK: 1) 3C X SQ. MM PILC+XLPE 33 KV HTCF SECTION BETWEEN 220KV BORIVALI REC-STN SWITCH:220BOR15. NOTIFICATION NO:- 0\n",
      "[NO MATCH] REMARK: 1) 3C X 225+240+300 SQ. MM PILC+XLPE 11 KV HTCF SECTION BETWEEN NEHRU NGR PUMP HOUSE S/S SWITCH:12359 TO NEHRU NGR SKY VIEW SWITCH- 42617 . DMS USED FOR ISOLATION AND RESTORATION.ANGOLIMALA CSS 100 % SUPPLY RESTORED ON DG SET AT 19:50 HRS NOTIFICATION NO:- 002001875329 Rotational loadshedding carried out due to loading constraint on 11kV network.”\n",
      "[NO MATCH] REMARK: 1) 3C X 300+400 SQ. MM PILC+XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK07_04 TO TAGORE NGR. REC-STN SWITCH:01103. NOTIFICATION NO:- 002001861730\n",
      "[NO MATCH] REMARK: 1) 3C X 400 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK11_13 TO HINGWALA LANE REC-STN SWITCH:33162. CABLE DAMAGED BY MCGM ON EASTERN EXPRESS HIGHWAY NEAR SAI NIVARA OPPOSITE GHATKOPAR DEPOT. NOTIFICATION NO:- 002001855216\n",
      "[NO MATCH] REMARK: 3C X 150+300 SQ. MM XLPE 11 KV HTCF SECTION BETWEEN KANNAMAWAR NGR 4 SWITCH : 29029 TO KANNAMAWAR NGR 5 SWITCH :42860 CABLE DAMAGED BY MHADA WATER DEPARTMENT NEAR KANNAMWAR NGR POLICE STATION NEAR BLDG NO 70. DMS USED FOR ISOLATION AND RESTORATION. ISOLATION & RESTORATION DELAYED DUE TO 1)FAILED TO CLOSE - MULTIPLE COMMAND TAGORE NGR. REC-STN SW.NO.10570 FAILED TO OPERATE FROM SCADA & LOCALLY ELECTRICALLY. 2) FPI MALFUNCTION AT KANNAMAWAR NGR 4 S/S SW.NO.29029 (NOT GLOWN). 3) HEAVY TRAFFIC ON ADI SHANKARACHARRYA MARG.\n",
      "[NO MATCH] REMARK: TRIPPED DUE TO FAULT ON AIRPORT LEG..\n",
      "[NO MATCH] REMARK: 1) 3C X 300 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK11_13 TO KALPATARU LBS R/S S/S SWITCH:33217. CABLE DAMAGED BY MCGM ON LBS ROAD. NOTIFICATION NO:- 002001363195\n",
      "[NO MATCH] REMARK: 1) 3C X 400 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK11_13 TO KALPTARU REC-STN SWITCH:33217. NOTIFICATION NO:- 002001338493\n",
      "[NO MATCH] REMARK: NO INTERRUPTION AS THERE WAS NO LOAD ON SECTION. 3C X 400 SQ. MM XLPE 33 KV HTCF SECTION BETWEEN TATA`S DHARAVI REC-STN SWITCH:H09 OG7 TO PALI REC-STN SWITCH:33235. NO SUPPLY INTERRUPTION AS ONLY SECTION WAS CHARGED.\n",
      "[NO MATCH] REMARK: 1) 3C X 400 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA`S VIKHROLI REC-STN SWITCH:T22VIK07_04 TO SHIVAJI NAGAR REC-STN SWITCH:22001. NOTIFICATION NO:- 002001333025\n",
      "[NO MATCH] REMARK: 1) 3C X 400 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA`S VIKHROLI REC-STN SWITCH:T22VIK07_04 TO SHIVAJI NAGAR REC-STN SWITCH:22001. NOTIFICATION NO:- 002001460526\n",
      "[NO MATCH] REMARK: 1) 3C X 300 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA`S VIKHROLI REC-STN SWITCH:T22VIK07_04 TO TAGORE NGR. REC-STN SWITCH:01103. CABLE DAMAGED BY BMC NEAR TAGORE NGR R/S INFRONT OF TAGORE NGR S/S. NOTIFICATION NO:- 002001395455\n",
      "[NO MATCH] REMARK: 1) 3C X 300 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA`S VIKHROLI REC-STN SWITCH:T22VIK07_04 TO TAGORE NGR. REC-STN SWITCH:01103. NOTIFICATION NO:-2001377419\n",
      "[NO MATCH] REMARK: 1) 3C X 400 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA`S VIKHROLI REC-STN SWITCH:T22VIK08_14 TO CHHEDA NAGAR REC-STN SWITCH:33069. NOTIFICATION NO:- 002001374836\n",
      "[NO MATCH] REMARK: 1) 3C X 400 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA`S VIKHROLI REC-STN SWITCH:T22VIK07_04 TO SHIVAJI NAGAR REC-STN SWITCH:22001. NOTIFICATION NO:- 002001649494\n",
      "[NO MATCH] REMARK: 1) 3C X 300 SQ. MM XLPE 22 KV HTCF SECTION BETWEEN TATA S VIKHROLI REC-STN SWITCH:T22VIK05_10 TO TILAK NAGAR REC-STN SWITCH:33054. NOTIFICATION NO:- 002001602362\n",
      "[NO MATCH] REMARK: nan\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_station_switch_diag(remark):\n",
    "    remark = str(remark)\n",
    "    hybrid_pattern = (\n",
    "        r'BETWEEN\\s+(.+?)\\s+(?:S/S|REC[-\\s]?STN)\\s+SWITCH[:\\s]*([A-Z0-9]+)\\s+TO\\s+(.+?)\\s+(?:S/S|REC[-\\s]?STN)\\s+SWITCH[:\\s]*([A-Z0-9]+)'\n",
    "    )\n",
    "    m = re.search(hybrid_pattern, remark, re.IGNORECASE)\n",
    "    if m:\n",
    "        src_ss = re.sub(r'(?:S/S|REC[-\\s]?STN)\\s*$', '', m.group(1).strip(), flags=re.IGNORECASE)\n",
    "        from_sw = m.group(2).strip()\n",
    "        dst_ss = re.sub(r'(?:S/S|REC[-\\s]?STN)\\s*$', '', m.group(3).strip(), flags=re.IGNORECASE)\n",
    "        to_sw = m.group(4).strip()\n",
    "        # print(f\"[MATCH] SOURCE_SS: '{src_ss}' | FROM_SWITCH: '{from_sw}' | DESTINATION_SS: '{dst_ss}' | TO_SWITCH: '{to_sw}'\")\n",
    "        return pd.Series([src_ss, from_sw, dst_ss, to_sw])\n",
    "    else:\n",
    "        print(f\"[NO MATCH] REMARK: {remark}\")\n",
    "        return pd.Series(['', '', '', ''])\n",
    "\n",
    "# Usage with your DataFrame\n",
    "df = pd.read_csv('HTLOGSHEET_CLEAN_enetrytype_1_cleaned.csv', low_memory=False)\n",
    "df[['SOURCE_SS', 'FROM_SWITCH', 'DESTINATION_SS', 'TO_SWITCH']] = df['FREE_REMARKS'].apply(extract_station_switch_diag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "133b9939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NO MATCH] REMARK: nan\n",
      "[NO MATCH] REMARK: 1) 3C X 400 SQ. MM XLPE 33 KV HTCF SECTION BETWEEN TO OSHIWARA RECEIVING STATION S/S SWITCH:33294. DMS USED FOR ISOLATION AND RESTORATION. NOTIFICATION NO:- 002002044113\n",
      "[NO MATCH] REMARK: nan\n",
      "[NO MATCH] REMARK: nan\n",
      "[NO MATCH] REMARK: NOTIFICATION NO:- 1\n",
      "[NO MATCH] REMARK: nan\n",
      "[NO MATCH] REMARK: TRIPPED DUE TO R-PHASE JUMPER BURNT AT TOWER 76A MONOPOLE.. RESTORATION DELAYED DUE TO 1)FAILED TO CLOSE VIRVANI IND.NO.1 S/S DMS SW.NO.05168 FAILED TO OPERATE FROM SCADA. 2)FAILED TO CLOSE GANDHI NAGAR NO.4 S/S DMS SW.NO.11378 FAILED TO OPERATE FROM SCADA. 3) DMS OFFLINE AT OBEROI MALL S/S. RESTORATION DELAYED DUE TO ONGOING OUTAGE OF 33KV AEML GOREGAON FEEDER 14 ( DINDOSHI 20 MVA-T2+ OMKAR 20 MVA-T2+NIRLON 20 MVA-T3+ NESCO 20MVA-T1).\n",
      "[NO MATCH] REMARK: NOTIFICATION NO:- 111\n",
      "[NO MATCH] REMARK: NO FAULT AFTER TESTING. NOTIFICATION NO:- 002001284566\n",
      "[NO MATCH] REMARK: nan\n",
      "[NO MATCH] REMARK: 1) 3C X SQ. MM PILC+XLPE 11 KV HTCF SECTION BETWEEN 38664 S/S SWITCH:SHYAM UPAVAN TO 38789 S/S SWITCH:LION PENCIL NEW. DMS USED FOR ISOLATION AND RESTORATION. ISOLATION & RESTORATION DELAYED DUE TO 1) FPI MALFUNCTION AT LION PENCIL NEW S/S SW.NO.38789 (GLOWN). NOTIFICATION NO:- .\n",
      "[NO MATCH] REMARK: 1) 3C X SQ. MM PILC+XLPE 33 KV HTCF SECTION BETWEEN TATA POWAI REC-STN SWITCH:T33POWAI OG6 TO VIKHROLI REC-STN SWITCH:33011. NOTIFICATION NO:- 002001304637\n",
      "[NO MATCH] REMARK: nan\n",
      "[NO MATCH] REMARK: 3C X 400 SQ. MM XLPE 33 KV HTCF SECTION BETWEEN 220 KV R INFRA VERSOVA REC-STN SWITCH:220VSV14 TO MALAD R/S 20 MVA-1 (STANDBY). NO SUPPLY INTERUPTION AS THIS IS A STANDBY FEEDER.\n",
      "[NO MATCH] REMARK: DMS USED FOR ISOLATION AND RESTORATION.\n",
      "[NO MATCH] REMARK: NOTIFICATION NO:- 200089987667\n",
      "[NO MATCH] REMARK: 1) 3C X SQ. MM PILC+XLPE 33 KV HTCF SECTION BETWEEN 220KV BORIVALI REC-STN SWITCH:220BOR15. NOTIFICATION NO:- 0\n",
      "[NO MATCH] REMARK: 1) 3C X 225+240+300 SQ. MM PILC+XLPE 11 KV HTCF SECTION BETWEEN NEHRU NGR PUMP HOUSE S/S SWITCH:12359 TO NEHRU NGR SKY VIEW SWITCH- 42617 . DMS USED FOR ISOLATION AND RESTORATION.ANGOLIMALA CSS 100 % SUPPLY RESTORED ON DG SET AT 19:50 HRS NOTIFICATION NO:- 002001875329 Rotational loadshedding carried out due to loading constraint on 11kV network.”\n",
      "[NO MATCH] REMARK: TRIPPED DUE TO FAULT ON AIRPORT LEG..\n",
      "[NO MATCH] REMARK: NO INTERRUPTION AS THERE WAS NO LOAD ON SECTION. 3C X 400 SQ. MM XLPE 33 KV HTCF SECTION BETWEEN TATA`S DHARAVI REC-STN SWITCH:H09 OG7 TO PALI REC-STN SWITCH:33235. NO SUPPLY INTERRUPTION AS ONLY SECTION WAS CHARGED.\n",
      "[NO MATCH] REMARK: nan\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_station_switch_diag(remark):\n",
    "    remark = str(remark)\n",
    "    pattern = re.compile(\n",
    "        r'BETWEEN\\s+(.+?)(?:\\s+(?:S/S|REC[\\s-]?STN|R/S|S\\.S\\.))?\\s+SWITCH[:\\s]*([A-Z0-9_]+)\\s+TO\\s+(.+?)(?:\\s+(?:S/S|REC[\\s-]?STN|R/S|S\\.S\\.))?\\s+SWITCH[:\\s]*([A-Z0-9_]+)',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    m = pattern.search(remark)\n",
    "    if m:\n",
    "        src_ss = m.group(1).strip()\n",
    "        from_sw = m.group(2).strip()\n",
    "        dst_ss = m.group(3).strip()\n",
    "        to_sw = m.group(4).strip()\n",
    "        # print(f\"[MATCH] SOURCE_SS: '{src_ss}' | FROM_SWITCH: '{from_sw}' | DESTINATION_SS: '{dst_ss}' | TO_SWITCH: '{to_sw}'\")\n",
    "        return pd.Series([src_ss, from_sw, dst_ss, to_sw])\n",
    "    else:\n",
    "        print(f\"[NO MATCH] REMARK: {remark}\")\n",
    "        return pd.Series(['', '', '', ''])\n",
    "\n",
    "# Usage with your DataFrame:\n",
    "df = pd.read_csv('HTLOGSHEET_CLEAN_enetrytype_1_cleaned.csv', low_memory=False)\n",
    "df[['SOURCE_SS', 'FROM_SWITCH', 'DESTINATION_SS', 'TO_SWITCH']] = df['FREE_REMARKS'].apply(extract_station_switch_diag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620d1500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f8b19c3",
   "metadata": {},
   "source": [
    "Pattern with maximum match and analye the free_remark and extract the switch_ no and station\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9a35b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Output saved to: /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/HT_fault_cable_info_processed.csv\n",
      "No-match rows saved to: /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/HT_fault_cable_info_no_match.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Load your data\n",
    "input_file = 'HTLOGSHEET_CLEAN_enetrytype_1_cleaned.csv'\n",
    "output_file = '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/HT_fault_cable_info_processed.csv'\n",
    "no_match_file = '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/HT_fault_cable_info_no_match.csv'\n",
    "\n",
    "df = pd.read_csv(input_file, low_memory=False)\n",
    "\n",
    "# Extraction function\n",
    "def extract_remark_info(text):\n",
    "    if pd.isnull(text):\n",
    "        return pd.Series([np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan])\n",
    "    # STD_CABLE_SIZE\n",
    "    cable_match = re.search(r'(\\d{1,4}\\s*(?:\\+|\\s*x\\s*)?\\s*\\d{1,4}(?:\\+\\d{1,4})*?)\\s*SQ\\.*\\s*MM', text.upper())\n",
    "    cable_size = cable_match.group(1).replace(' ', '') if cable_match else np.nan\n",
    "    # VOLTAGE\n",
    "    voltage_match = re.search(r'(\\d{2,3}\\s*KV)', text.upper())\n",
    "    voltage = voltage_match.group(1).replace(' ', '') if voltage_match else np.nan\n",
    "    # CABLE_TYPE extraction\n",
    "    cable_type_match = re.search(r'(PILC\\+XLPE|XLPE\\+PILC|PILC|XLPE)', text.upper())\n",
    "    cable_type = cable_type_match.group(1) if cable_type_match else np.nan\n",
    "    # Main pattern\n",
    "    pattern = re.compile(\n",
    "        r'BETWEEN\\s+(.*?)\\s*(?:S/S)?\\s*SWITCH[:\\-\\s]*([A-Z0-9_]+).*?TO\\s+(.*?)\\s*(?:S/S)?\\s*SWITCH[:\\-\\s]*([A-Z0-9_]+)',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        src, from_sw, dst, to_sw = match.groups()\n",
    "        return pd.Series([src.strip(), from_sw.strip(), dst.strip(), to_sw.strip(), voltage, cable_size, cable_type])\n",
    "    # Try alternate pattern\n",
    "    pattern_alt = re.compile(\n",
    "        r'BETWEEN\\s+(.*?)\\s*(?:S/S)?\\s*SWITCH[\\:\\-\\s]*([A-Z0-9_]+)\\s*TO\\s+(.*?)\\s*(?:S/S)?\\s*SWITCH[\\:\\-\\s]*([A-Z0-9_]+)',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    match_alt = pattern_alt.search(text)\n",
    "    if match_alt:\n",
    "        src, from_sw, dst, to_sw = match_alt.groups()\n",
    "        return pd.Series([src.strip(), from_sw.strip(), dst.strip(), to_sw.strip(), voltage, cable_size, cable_type])\n",
    "    # No match\n",
    "    return pd.Series([np.nan, np.nan, np.nan, np.nan, voltage, cable_size, cable_type])\n",
    "\n",
    "\n",
    "# Apply extraction\n",
    "df[['SOURCE_SS', 'FROM_SWITCH', 'DESTINATION_SS', 'TO_SWITCH', 'VOLTAGE', 'STD_CABLE_SIZE', 'CABLE_TYPE']] = (\n",
    "    df['FREE_REMARKS'].apply(extract_remark_info)\n",
    ")\n",
    "\n",
    "\n",
    "# Calculate TIME_DIFFERENCE_HOURS\n",
    "df['TIME_OUTAGE'] = pd.to_datetime(df['TIME_OUTAGE'], errors='coerce')\n",
    "df['TIME_RESTORED'] = pd.to_datetime(df['TIME_RESTORED'], errors='coerce')\n",
    "df['TIME_DIFFERENCE_HOURS'] = (df['TIME_RESTORED'] - df['TIME_OUTAGE']).dt.total_seconds() / 3600\n",
    "\n",
    "# Columns for output\n",
    "final_cols = [\n",
    "    'SWITCH_NO', 'STATION_NAME', 'STD_CABLE_SIZE', 'CABLE_TYPE', 'TIME_OUTAGE',\n",
    "    'SOURCE_SS', 'FROM_SWITCH', 'DESTINATION_SS', 'TO_SWITCH',\n",
    "    'VOLTAGE', 'TIME_DIFFERENCE_HOURS'\n",
    "]\n",
    "\n",
    "out_df = df[final_cols]\n",
    "out_df.to_csv(output_file, index=False)\n",
    "print(\"Done! Output saved to:\", output_file)\n",
    "\n",
    "# --- Extract NO-MATCH CASES ---\n",
    "\n",
    "# Rows where nothing was matched for station or switch info\n",
    "no_match_mask = (\n",
    "    df['SOURCE_SS'].isnull() &\n",
    "    df['FROM_SWITCH'].isnull() &\n",
    "    df['DESTINATION_SS'].isnull() &\n",
    "    df['TO_SWITCH'].isnull()\n",
    ")\n",
    "no_match_df = df.loc[no_match_mask, ['FREE_REMARKS', 'VOLTAGE', 'STD_CABLE_SIZE']]\n",
    "no_match_df.to_csv(no_match_file, index=False)\n",
    "print(\"No-match rows saved to:\", no_match_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58406419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e58c8cc2",
   "metadata": {},
   "source": [
    "Fault nature using the basic nlp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dc70d2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Output saved to: /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/HT_fault_cable_info_processed.csv\n",
      "No-match rows saved to: /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/HT_fault_cable_info_no_match.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Load your data\n",
    "input_file = 'HTLOGSHEET_CLEAN_enetrytype_1_cleaned.csv'\n",
    "output_file = '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/HT_fault_cable_info_processed.csv'\n",
    "no_match_file = '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/HT_fault_cable_info_no_match.csv'\n",
    "\n",
    "df = pd.read_csv(input_file, low_memory=False)\n",
    "\n",
    "def extract_remark_info(text):\n",
    "    if pd.isnull(text):\n",
    "        return pd.Series([np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan])\n",
    "    cable_match = re.search(r'(\\d{1,4}\\s*(?:\\+|\\s*x\\s*)?\\s*\\d{1,4}(?:\\+\\d{1,4})*?)\\s*SQ\\.*\\s*MM', text.upper())\n",
    "    cable_size = cable_match.group(1).replace(' ', '') if cable_match else np.nan\n",
    "    voltage_match = re.search(r'(\\d{2,3}\\s*KV)', text.upper())\n",
    "    voltage = voltage_match.group(1).replace(' ', '') if voltage_match else np.nan\n",
    "    cable_type_match = re.search(r'(PILC\\+XLPE|XLPE\\+PILC|PILC|XLPE)', text.upper())\n",
    "    cable_type = cable_type_match.group(1) if cable_type_match else np.nan\n",
    "    pattern = re.compile(\n",
    "        r'BETWEEN\\s+(.*?)\\s*(?:S/S)?\\s*SWITCH[:\\-\\s]*([A-Z0-9_]+).*?TO\\s+(.*?)\\s*(?:S/S)?\\s*SWITCH[:\\-\\s]*([A-Z0-9_]+)',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        src, from_sw, dst, to_sw = match.groups()\n",
    "        return pd.Series([src.strip(), from_sw.strip(), dst.strip(), to_sw.strip(), voltage, cable_size, cable_type])\n",
    "    pattern_alt = re.compile(\n",
    "        r'BETWEEN\\s+(.*?)\\s*(?:S/S)?\\s*SWITCH[\\:\\-\\s]*([A-Z0-9_]+)\\s*TO\\s+(.*?)\\s*(?:S/S)?\\s*SWITCH[\\:\\-\\s]*([A-Z0-9_]+)',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    match_alt = pattern_alt.search(text)\n",
    "    if match_alt:\n",
    "        src, from_sw, dst, to_sw = match_alt.groups()\n",
    "        return pd.Series([src.strip(), from_sw.strip(), dst.strip(), to_sw.strip(), voltage, cable_size, cable_type])\n",
    "    return pd.Series([np.nan, np.nan, np.nan, np.nan, voltage, cable_size, cable_type])\n",
    "\n",
    "df[['SOURCE_SS', 'FROM_SWITCH', 'DESTINATION_SS', 'TO_SWITCH', 'VOLTAGE', 'STD_CABLE_SIZE', 'CABLE_TYPE']] = (\n",
    "    df['FREE_REMARKS'].apply(extract_remark_info)\n",
    ")\n",
    "\n",
    "df['TIME_OUTAGE'] = pd.to_datetime(df['TIME_OUTAGE'], errors='coerce')\n",
    "df['TIME_RESTORED'] = pd.to_datetime(df['TIME_RESTORED'], errors='coerce')\n",
    "df['TIME_DIFFERENCE_HOURS'] = (df['TIME_RESTORED'] - df['TIME_OUTAGE']).dt.total_seconds() / 3600\n",
    "\n",
    "# --- NEW: Extract all delay points as separate rows ---\n",
    "\n",
    "def extract_delay_points(text):\n",
    "    # Find \"DELAYED DUE TO\" and split points 1), 2), ...\n",
    "    m = re.search(r'DELAYED DUE TO(.*?)(?:NOTIFICATION NO|$)', str(text), re.IGNORECASE)\n",
    "    if m:\n",
    "        points = re.split(r'\\d+\\)', m.group(1))\n",
    "        return [p.strip('.; \\n') for p in points if p.strip()]\n",
    "    return []\n",
    "\n",
    "def extract_affected_station_switch(point):\n",
    "    # Try full pattern with station and switch\n",
    "    m = re.search(r'([A-Z0-9 .\\-]+?)\\s+S/S(?:.*?)SW\\.?NO\\.?([0-9]+)', point)\n",
    "    if m:\n",
    "        return m.group(1).strip(), m.group(2)\n",
    "    # Fallback: pattern with only station (no switch)\n",
    "    m2 = re.search(r'([A-Z0-9 .\\-]+?)\\s+S/S', point)\n",
    "    if m2:\n",
    "        return m2.group(1).strip(), np.nan\n",
    "    # Fallback: pattern like \"DMS OFFLINE AT <STATION>\"\n",
    "    m3 = re.search(r'OFFLINE AT ([A-Z0-9 .\\-]+?)\\s+S/S', point)\n",
    "    if m3:\n",
    "        return m3.group(1).strip(), np.nan\n",
    "    return np.nan, np.nan\n",
    "\n",
    "\n",
    "def categorize_reason(point):\n",
    "    point_l = point.lower()\n",
    "    cats = []\n",
    "    if \"failed to operate from scada\" in point_l or \"scada failure\" in point_l or \"scada offline\" in point_l or \"dms offline\" in point_l:\n",
    "        cats.append(\"SCADA FAILURE\")\n",
    "    if \"failed to open\" in point_l or \"failed to close\" in point_l or \"switch failure\" in point_l:\n",
    "        cats.append(\"SWITCH FAILURE\")\n",
    "    if \"tripping\" in point_l or \"went offline after tripping\" in point_l:\n",
    "        cats.append(\"TRIPPING\")\n",
    "    if \"fpi malfunction\" in point_l or \"fpi faulty\" in point_l:\n",
    "        cats.append(\"FPI FAULT\")\n",
    "    if \"cable damaged\" in point_l or \"cable fault\" in point_l:\n",
    "        cats.append(\"CABLE FAULT\")\n",
    "    if \"access problem\" in point_l or \"gate opening issue\" in point_l:\n",
    "        cats.append(\"ACCESS ISSUE\")\n",
    "    if \"non dms substation\" in point_l:\n",
    "        cats.append(\"NON DMS AREA\")\n",
    "    if not cats and \"isolation\" in point_l:\n",
    "        cats.append(\"NORMAL ISOLATION\")\n",
    "    return \"+\".join(cats) if cats else \"UNKNOWN\"\n",
    "\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    base_info = {\n",
    "        'SWITCH_NO': row.get('SWITCH_NO'),\n",
    "        'STATION_NAME': row.get('STATION_NAME'),\n",
    "        'STD_CABLE_SIZE': row.get('STD_CABLE_SIZE'),\n",
    "        'CABLE_TYPE': row.get('CABLE_TYPE'),\n",
    "        'TIME_OUTAGE': row.get('TIME_OUTAGE'),\n",
    "        'SOURCE_SS': row.get('SOURCE_SS'),\n",
    "        'FROM_SWITCH': row.get('FROM_SWITCH'),\n",
    "        'DESTINATION_SS': row.get('DESTINATION_SS'),\n",
    "        'TO_SWITCH': row.get('TO_SWITCH'),\n",
    "        'VOLTAGE': row.get('VOLTAGE'),\n",
    "        'TIME_DIFFERENCE_HOURS': row.get('TIME_DIFFERENCE_HOURS'),\n",
    "    }\n",
    "    points = extract_delay_points(row['FREE_REMARKS'])\n",
    "    if points:\n",
    "        for p in points:\n",
    "            affected_station, affected_switch = extract_affected_station_switch(p)\n",
    "            category = categorize_reason(p)\n",
    "            this_row = base_info.copy()\n",
    "            this_row['AFFECTED_STATION'] = affected_station\n",
    "            this_row['AFFECTED_SWITCH'] = affected_switch\n",
    "            this_row['REASON_CATEGORY'] = category\n",
    "            this_row['REASON_TEXT'] = p\n",
    "            all_rows.append(this_row)\n",
    "    else:\n",
    "        # No delay points, keep at least one row\n",
    "        this_row = base_info.copy()\n",
    "        this_row['AFFECTED_STATION'] = np.nan\n",
    "        this_row['AFFECTED_SWITCH'] = np.nan\n",
    "        this_row['REASON_CATEGORY'] = np.nan\n",
    "        this_row['REASON_TEXT'] = np.nan\n",
    "        all_rows.append(this_row)\n",
    "\n",
    "# Output DataFrame\n",
    "result_df = pd.DataFrame(all_rows)\n",
    "result_df.to_csv(output_file, index=False)\n",
    "print(\"Done! Output saved to:\", output_file)\n",
    "\n",
    "# --- Extract NO-MATCH CASES ---\n",
    "\n",
    "no_match_mask = (\n",
    "    df['SOURCE_SS'].isnull() &\n",
    "    df['FROM_SWITCH'].isnull() &\n",
    "    df['DESTINATION_SS'].isnull() &\n",
    "    df['TO_SWITCH'].isnull()\n",
    ")\n",
    "no_match_df = df.loc[no_match_mask, ['FREE_REMARKS', 'VOLTAGE', 'STD_CABLE_SIZE']]\n",
    "no_match_df.to_csv(no_match_file, index=False)\n",
    "print(\"No-match rows saved to:\", no_match_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3db04d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing or unmatched pattern: 14034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                                    NaN\n",
       "3      KASTURI KUNJ S/S (DMS) WENT OFFLINE AFTER TRIP...\n",
       "5      AAREY NO.2 S/S (DMS) WENT OFFLINE AFTER GIVING...\n",
       "6                FPI FAULTY AT AAREY UNIT NO.7 KIOSK S/S\n",
       "7      NON DMS SUBSTATION ARE IN ARREY AREA. DMS USED...\n",
       "                             ...                        \n",
       "155                                                  NaN\n",
       "156           DMS OFFLINE AT GANESH NAGAR MHADA NO.3 S/S\n",
       "157                        DMS OFFLINE AT VASTU PARK S/S\n",
       "159     VASTU PARK S/S (DMS) WENT OFFLINE AFTER TRIPPING\n",
       "160                      DMS OFFLINE AT VISHAL NAGAR S/S\n",
       "Name: REASON_TEXT, Length: 100, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This will show you all rows where at least one field did not get filled (NaN or empty)\n",
    "missing_pattern_rows = result_df[\n",
    "    result_df[['AFFECTED_STATION', 'AFFECTED_SWITCH', 'REASON_CATEGORY', 'REASON_TEXT']]\n",
    "    .isnull().any(axis=1)\n",
    "    | (result_df['AFFECTED_STATION'].astype(str).str.strip() == \"\")\n",
    "    | (result_df['REASON_CATEGORY'].astype(str).str.strip() == \"\")\n",
    "    | (result_df['REASON_TEXT'].astype(str).str.strip() == \"\")\n",
    "]\n",
    "\n",
    "print(f\"Rows with missing or unmatched pattern: {len(missing_pattern_rows)}\")\n",
    "display(missing_pattern_rows['REASON_TEXT'].head(100))  # See the first 10, for inspection\n",
    "\n",
    "# Optionally, save them for inspection\n",
    "# missing_pattern_rows.to_csv(\"HT_fault_cable_info_missing_pattern.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "78ca8d2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Apply matching\u001b[39;00m\n\u001b[1;32m     37\u001b[0m result_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAFFECTED_STATION\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m result_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mREASON_TEXT\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: match_station_names(x, station_patterns))\n\u001b[0;32m---> 38\u001b[0m result_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAFFECTED_SWITCH\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mresult_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mREASON_TEXT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatch_switch_numbers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswitch_patterns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m result_df\u001b[38;5;241m.\u001b[39mto_csv(output_file, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdated AFFECTED_STATION and AFFECTED_SWITCH with master list matches and saved to:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_file)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:4935\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4800\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4801\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4802\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4807\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4808\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4809\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4810\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4811\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4926\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4927\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4928\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4930\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4931\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4935\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:1422\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:1502\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1502\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1507\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/base.py:925\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mpandas/_libs/lib.pyx:2999\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[53], line 38\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Apply matching\u001b[39;00m\n\u001b[1;32m     37\u001b[0m result_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAFFECTED_STATION\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m result_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mREASON_TEXT\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: match_station_names(x, station_patterns))\n\u001b[0;32m---> 38\u001b[0m result_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAFFECTED_SWITCH\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m result_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mREASON_TEXT\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mmatch_switch_numbers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswitch_patterns\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     40\u001b[0m result_df\u001b[38;5;241m.\u001b[39mto_csv(output_file, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdated AFFECTED_STATION and AFFECTED_SWITCH with master list matches and saved to:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_file)\n",
      "Cell \u001b[0;32mIn[53], line 33\u001b[0m, in \u001b[0;36mmatch_switch_numbers\u001b[0;34m(text, switch_patterns)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[1;32m     32\u001b[0m text_upper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(text)\u001b[38;5;241m.\u001b[39mupper()\n\u001b[0;32m---> 33\u001b[0m found \u001b[38;5;241m=\u001b[39m [sw \u001b[38;5;28;01mfor\u001b[39;00m sw, pattern \u001b[38;5;129;01min\u001b[39;00m switch_patterns \u001b[38;5;28;01mif\u001b[39;00m pattern\u001b[38;5;241m.\u001b[39msearch(text_upper)]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(found) \u001b[38;5;28;01mif\u001b[39;00m found \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n",
      "Cell \u001b[0;32mIn[53], line 33\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[1;32m     32\u001b[0m text_upper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(text)\u001b[38;5;241m.\u001b[39mupper()\n\u001b[0;32m---> 33\u001b[0m found \u001b[38;5;241m=\u001b[39m [sw \u001b[38;5;28;01mfor\u001b[39;00m sw, pattern \u001b[38;5;129;01min\u001b[39;00m switch_patterns \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mpattern\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_upper\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(found) \u001b[38;5;28;01mif\u001b[39;00m found \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "station_master_file = '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/ss_unique.csv'\n",
    "switch_master_file = '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/switch.csv'\n",
    "output_file = '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/HT_fault_cable_info_processed2.csv'\n",
    "\n",
    "# Load data\n",
    "result_df = pd.read_csv(output_file)\n",
    "station_master = pd.read_csv(station_master_file)\n",
    "switch_master = pd.read_csv(switch_master_file)\n",
    "\n",
    "all_station_names = station_master['SOURCE_SS'].dropna().str.upper().str.strip().tolist()\n",
    "# Assume switch numbers are strings (so leading zeros are preserved)\n",
    "all_switch_numbers = switch_master['0'].dropna().astype(str).str.strip().tolist()\n",
    "\n",
    "# Precompile regex patterns\n",
    "station_patterns = [(name, re.compile(r'\\b' + re.escape(name) + r'\\b')) for name in all_station_names]\n",
    "switch_patterns = [(sw, re.compile(r'\\b' + re.escape(sw) + r'\\b')) for sw in all_switch_numbers]\n",
    "\n",
    "def match_station_names(text, station_patterns):\n",
    "    if pd.isnull(text):\n",
    "        return np.nan\n",
    "    text_upper = str(text).upper()\n",
    "    found = [name for name, pattern in station_patterns if pattern.search(text_upper)]\n",
    "    return '; '.join(found) if found else np.nan\n",
    "\n",
    "def match_switch_numbers(text, switch_patterns):\n",
    "    if pd.isnull(text):\n",
    "        return np.nan\n",
    "    text_upper = str(text).upper()\n",
    "    found = [sw for sw, pattern in switch_patterns if pattern.search(text_upper)]\n",
    "    return '; '.join(found) if found else np.nan\n",
    "\n",
    "# Apply matching\n",
    "result_df['AFFECTED_STATION'] = result_df['REASON_TEXT'].apply(lambda x: match_station_names(x, station_patterns))\n",
    "result_df['AFFECTED_SWITCH'] = result_df['REASON_TEXT'].apply(lambda x: match_switch_numbers(x, switch_patterns))\n",
    "\n",
    "result_df.to_csv(output_file, index=False)\n",
    "print(\"Updated AFFECTED_STATION and AFFECTED_SWITCH with master list matches and saved to:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "00fe3981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Output saved to: /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/HT_fault_cable_info_processed2.csv\n",
      "No-match rows saved to: /media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/HT_fault_cable_info_no_match2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "input_file = 'HTLOGSHEET_CLEAN_enetrytype_1_cleaned.csv'\n",
    "output_file = '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/HT_fault_cable_info_processed2.csv'\n",
    "no_match_file = '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/HT_fault_cable_info_no_match2.csv'\n",
    "station_master_file = '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/ss_unique.csv'\n",
    "switch_master_file = '/media/sagarkumar/New Volume/SAGAR/IPYNB_FILE/DATA_GENERATION/switch.csv'\n",
    "\n",
    "# Load master lists\n",
    "station_master = pd.read_csv(station_master_file)\n",
    "switch_master = pd.read_csv(switch_master_file)\n",
    "all_station_names = station_master['SOURCE_SS'].dropna().str.upper().str.strip().tolist()\n",
    "all_switch_numbers = switch_master['0'].dropna().astype(str).str.strip().tolist()\n",
    "\n",
    "# Helper fallback matchers\n",
    "def match_station_names(text, all_station_names):\n",
    "    if pd.isnull(text): return np.nan\n",
    "    text_upper = str(text).upper()\n",
    "    found = [name for name in all_station_names if name in text_upper]\n",
    "    return '; '.join(found) if found else np.nan\n",
    "\n",
    "def match_switch_numbers(text, all_switch_numbers):\n",
    "    if pd.isnull(text):\n",
    "        return np.nan\n",
    "    text_upper = str(text).upper()\n",
    "    found = [sw for sw in all_switch_numbers if sw in text_upper]\n",
    "    # Convert to int, get max, return as string\n",
    "    found_digits = [int(sw) for sw in found if sw.isdigit()]\n",
    "    if found_digits:\n",
    "        return str(max(found_digits))\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "df = pd.read_csv(input_file, low_memory=False)\n",
    "\n",
    "def extract_remark_info(text):\n",
    "    if pd.isnull(text):\n",
    "        return pd.Series([np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan])\n",
    "    cable_match = re.search(r'(\\d{1,4}\\s*(?:\\+|\\s*x\\s*)?\\s*\\d{1,4}(?:\\+\\d{1,4})*?)\\s*SQ\\.*\\s*MM', text.upper())\n",
    "    cable_size = cable_match.group(1).replace(' ', '') if cable_match else np.nan\n",
    "    voltage_match = re.search(r'(\\d{2,3}\\s*KV)', text.upper())\n",
    "    voltage = voltage_match.group(1).replace(' ', '') if voltage_match else np.nan\n",
    "    cable_type_match = re.search(r'(PILC\\+XLPE|XLPE\\+PILC|PILC|XLPE)', text.upper())\n",
    "    cable_type = cable_type_match.group(1) if cable_type_match else np.nan\n",
    "    pattern = re.compile(\n",
    "        r'BETWEEN\\s+(.*?)\\s*(?:S/S)?\\s*SWITCH[:\\-\\s]*([A-Z0-9_]+).*?TO\\s+(.*?)\\s*(?:S/S)?\\s*SWITCH[:\\-\\s]*([A-Z0-9_]+)',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        src, from_sw, dst, to_sw = match.groups()\n",
    "        return pd.Series([src.strip(), from_sw.strip(), dst.strip(), to_sw.strip(), voltage, cable_size, cable_type])\n",
    "    pattern_alt = re.compile(\n",
    "        r'BETWEEN\\s+(.*?)\\s*(?:S/S)?\\s*SWITCH[\\:\\-\\s]*([A-Z0-9_]+)\\s*TO\\s+(.*?)\\s*(?:S/S)?\\s*SWITCH[\\:\\-\\s]*([A-Z0-9_]+)',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    match_alt = pattern_alt.search(text)\n",
    "    if match_alt:\n",
    "        src, from_sw, dst, to_sw = match_alt.groups()\n",
    "        return pd.Series([src.strip(), from_sw.strip(), dst.strip(), to_sw.strip(), voltage, cable_size, cable_type])\n",
    "    return pd.Series([np.nan, np.nan, np.nan, np.nan, voltage, cable_size, cable_type])\n",
    "\n",
    "df[['SOURCE_SS', 'FROM_SWITCH', 'DESTINATION_SS', 'TO_SWITCH', 'VOLTAGE', 'STD_CABLE_SIZE', 'CABLE_TYPE']] = (\n",
    "    df['FREE_REMARKS'].apply(extract_remark_info)\n",
    ")\n",
    "\n",
    "df['TIME_OUTAGE'] = pd.to_datetime(df['TIME_OUTAGE'], errors='coerce')\n",
    "df['TIME_RESTORED'] = pd.to_datetime(df['TIME_RESTORED'], errors='coerce')\n",
    "df['TIME_DIFFERENCE_HOURS'] = (df['TIME_RESTORED'] - df['TIME_OUTAGE']).dt.total_seconds() / 3600\n",
    "\n",
    "def extract_delay_points(text):\n",
    "    m = re.search(r'DELAYED DUE TO(.*?)(?:NOTIFICATION NO|$)', str(text), re.IGNORECASE)\n",
    "    if m:\n",
    "        points = re.split(r'\\d+\\)', m.group(1))\n",
    "        return [p.strip('.; \\n') for p in points if p.strip()]\n",
    "    return []\n",
    "\n",
    "def extract_affected_station_switch(point):\n",
    "    m = re.search(r'([A-Z0-9 .\\-]+?)\\s+S/S(?:.*?)SW\\.?NO\\.?([0-9]+)', point)\n",
    "    if m:\n",
    "        return m.group(1).strip(), m.group(2)\n",
    "    m2 = re.search(r'([A-Z0-9 .\\-]+?)\\s+S/S', point)\n",
    "    if m2:\n",
    "        return m2.group(1).strip(), np.nan\n",
    "    m3 = re.search(r'OFFLINE AT ([A-Z0-9 .\\-]+?)\\s+S/S', point)\n",
    "    if m3:\n",
    "        return m3.group(1).strip(), np.nan\n",
    "    return np.nan, np.nan\n",
    "\n",
    "def categorize_reason(point):\n",
    "    point_l = point.lower()\n",
    "    cats = []\n",
    "    if \"failed to operate from scada\" in point_l or \"scada failure\" in point_l or \"scada offline\" in point_l or \"dms offline\" in point_l:\n",
    "        cats.append(\"SCADA FAILURE\")\n",
    "    if \"failed to open\" in point_l or \"failed to close\" in point_l or \"switch failure\" in point_l:\n",
    "        cats.append(\"SWITCH FAILURE\")\n",
    "    if \"tripping\" in point_l or \"went offline after tripping\" in point_l:\n",
    "        cats.append(\"TRIPPING\")\n",
    "    if \"fpi malfunction\" in point_l or \"fpi faulty\" in point_l:\n",
    "        cats.append(\"FPI FAULT\")\n",
    "    if \"cable damaged\" in point_l or \"cable fault\" in point_l:\n",
    "        cats.append(\"CABLE FAULT\")\n",
    "    if \"access problem\" in point_l or \"gate opening issue\" in point_l:\n",
    "        cats.append(\"ACCESS ISSUE\")\n",
    "    if \"non dms substation\" in point_l:\n",
    "        cats.append(\"NON DMS AREA\")\n",
    "    if not cats and \"isolation\" in point_l:\n",
    "        cats.append(\"NORMAL ISOLATION\")\n",
    "    return \"+\".join(cats) if cats else \"UNKNOWN\"\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    base_info = {\n",
    "        'SWITCH_NO': row.get('SWITCH_NO'),\n",
    "        'STATION_NAME': row.get('STATION_NAME'),\n",
    "        'STD_CABLE_SIZE': row.get('STD_CABLE_SIZE'),\n",
    "        'CABLE_TYPE': row.get('CABLE_TYPE'),\n",
    "        'TIME_OUTAGE': row.get('TIME_OUTAGE'),\n",
    "        'SOURCE_SS': row.get('SOURCE_SS'),\n",
    "        'FROM_SWITCH': row.get('FROM_SWITCH'),\n",
    "        'DESTINATION_SS': row.get('DESTINATION_SS'),\n",
    "        'TO_SWITCH': row.get('TO_SWITCH'),\n",
    "        'VOLTAGE': row.get('VOLTAGE'),\n",
    "        'TIME_DIFFERENCE_HOURS': row.get('TIME_DIFFERENCE_HOURS'),\n",
    "    }\n",
    "    points = extract_delay_points(row['FREE_REMARKS'])\n",
    "    if points:\n",
    "        for p in points:\n",
    "            affected_station, affected_switch = extract_affected_station_switch(p)\n",
    "            # Fallback: if not matched, try master list\n",
    "            if pd.isnull(affected_station) or affected_station == '':\n",
    "                affected_station = match_station_names(p, all_station_names)\n",
    "            if pd.isnull(affected_switch) or affected_switch == '':\n",
    "                affected_switch = match_switch_numbers(p, all_switch_numbers)\n",
    "            category = categorize_reason(p)\n",
    "            this_row = base_info.copy()\n",
    "            this_row['AFFECTED_STATION'] = affected_station\n",
    "            this_row['AFFECTED_SWITCH'] = affected_switch\n",
    "            this_row['REASON_CATEGORY'] = category\n",
    "            this_row['REASON_TEXT'] = p\n",
    "            all_rows.append(this_row)\n",
    "    else:\n",
    "        this_row = base_info.copy()\n",
    "        this_row['AFFECTED_STATION'] = np.nan\n",
    "        this_row['AFFECTED_SWITCH'] = np.nan\n",
    "        this_row['REASON_CATEGORY'] = np.nan\n",
    "        this_row['REASON_TEXT'] = np.nan\n",
    "        all_rows.append(this_row)\n",
    "\n",
    "result_df = pd.DataFrame(all_rows)\n",
    "result_df.to_csv(output_file, index=False)\n",
    "print(\"Done! Output saved to:\", output_file)\n",
    "\n",
    "# --- Extract NO-MATCH CASES ---\n",
    "\n",
    "no_match_mask = (\n",
    "    df['SOURCE_SS'].isnull() &\n",
    "    df['FROM_SWITCH'].isnull() &\n",
    "    df['DESTINATION_SS'].isnull() &\n",
    "    df['TO_SWITCH'].isnull()\n",
    ")\n",
    "no_match_df = df.loc[no_match_mask, ['FREE_REMARKS', 'VOLTAGE', 'STD_CABLE_SIZE']]\n",
    "no_match_df.to_csv(no_match_file, index=False)\n",
    "print(\"No-match rows saved to:\", no_match_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c553c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
