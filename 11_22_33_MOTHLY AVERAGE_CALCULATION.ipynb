{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6db17a53",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "525f7770",
   "metadata": {},
   "source": [
    "USE EDA ON THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "193870f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 763 files to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_869678/2818614198.py:38: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['SYSTIME'] = pd.to_datetime(df['SYSTIME'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique VOLTAGE values and counts:\n",
      "  11KV: 252537820\n",
      "  33KV: 168273218\n",
      "  22KV: 28334840\n",
      "  14: 22442\n",
      "\n",
      "Date range: 2024-01-01 00:00:00+00:00 to 2025-04-18 23:45:00+00:00\n",
      "\n",
      "Record counts per YEAR-MONTH (first 10):\n",
      "  2025-03: 30544286\n",
      "  2024-03: 46339573\n",
      "  2025-02: 26543330\n",
      "  2024-04: 42862642\n",
      "  2024-08: 3060444\n",
      "  2024-10: 27462412\n",
      "  2024-06: 37997242\n",
      "  2024-05: 43640867\n",
      "  2025-04: 2901799\n",
      "  2024-07: 5279796\n",
      "\n",
      "Record counts per VOLTAGE and MONTH (first 10):\n",
      "  11KV, month 03: 42827054\n",
      "  33KV, month 03: 28965273\n",
      "  22KV, month 03: 5088472\n",
      "  14, month 03: 3060\n",
      "  11KV, month 02: 38034644\n",
      "  33KV, month 02: 26914674\n",
      "  22KV, month 02: 4643755\n",
      "  11KV, month 04: 19486669\n",
      "  22KV, month 04: 3528327\n",
      "  33KV, month 04: 22746585\n",
      "\n",
      "Months missing for each VOLTAGE (among all files):\n",
      "11KV: no missing months\n",
      "33KV: no missing months\n",
      "22KV: no missing months\n",
      "14: missing months [9, 10, 11, 12]\n",
      "\n",
      "Unique PARA values count: 6\n",
      "Some PARA values:\n",
      "   I\n",
      "   Temp\n",
      "   MVA\n",
      "   PF\n",
      "   KW\n",
      "   V\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "folders = [\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/200/200',\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/200-400/200-400',\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/400-600/400-600',\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/600-759/600-759',\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/SCADA_JAN_24_TO_APR_25'\n",
    "]\n",
    "\n",
    "def process_file(file):\n",
    "    voltage_counts = {}\n",
    "    min_date, max_date = None, None\n",
    "    year_month_counts = {}\n",
    "    voltage_month_counts = {}\n",
    "    unique_para = set()   # <--- Added for PARA column\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            file,\n",
    "            usecols=['SYSTIME', 'VOLTAGE', 'PARA'],  # <--- Added PARA column\n",
    "            dtype={'VOLTAGE': 'category', 'SYSTIME': str, 'PARA': str},\n",
    "            low_memory=True,\n",
    "            memory_map=True\n",
    "        )\n",
    "        # Unique PARA values\n",
    "        unique_para.update(df['PARA'].dropna().unique())\n",
    "        # Normalize VOLTAGE\n",
    "        df['VOLTAGE'] = df['VOLTAGE'].str.upper().str.strip()\n",
    "        df['VOLTAGE'] = df['VOLTAGE'].apply(\n",
    "            lambda v: f\"{v}KV\" if (pd.notna(v) and v in ['11','22','33']) else v\n",
    "        )\n",
    "        df = df.dropna(subset=['VOLTAGE'])\n",
    "        # Parse SYSTIME and drop missing\n",
    "        df['SYSTIME'] = pd.to_datetime(df['SYSTIME'], errors='coerce')\n",
    "        df = df.dropna(subset=['SYSTIME'])\n",
    "        file_min, file_max = None, None\n",
    "        if not df.empty:\n",
    "            file_min, file_max = df['SYSTIME'].min(), df['SYSTIME'].max()\n",
    "        # Count voltages\n",
    "        for v, count in df['VOLTAGE'].value_counts().items():\n",
    "            voltage_counts[v] = count\n",
    "        # Year/month\n",
    "        df['YEAR'] = df['SYSTIME'].dt.year.astype('int16')\n",
    "        df['MONTH'] = df['SYSTIME'].dt.month.astype('int8')\n",
    "        for y, m in zip(df['YEAR'], df['MONTH']):\n",
    "            year_month_counts[(y, m)] = year_month_counts.get((y, m), 0) + 1\n",
    "        # Voltage/month combo\n",
    "        for v, m in zip(df['VOLTAGE'], df['MONTH']):\n",
    "            voltage_month_counts[(v, m)] = voltage_month_counts.get((v, m), 0) + 1\n",
    "        del df\n",
    "        gc.collect()\n",
    "        return voltage_counts, file_min, file_max, year_month_counts, voltage_month_counts, unique_para\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "        return {}, None, None, {}, {}, set()\n",
    "\n",
    "# Gather all files\n",
    "all_files = []\n",
    "for folder in folders:\n",
    "    all_files.extend(glob.glob(os.path.join(folder, '*.csv')))\n",
    "print(f\"Found {len(all_files)} files to process.\")\n",
    "\n",
    "results = []\n",
    "with ProcessPoolExecutor(max_workers=10) as executor:\n",
    "    for result in executor.map(process_file, all_files):\n",
    "        results.append(result)\n",
    "\n",
    "# Aggregate all results\n",
    "voltage_counts = {}\n",
    "min_date, max_date = None, None\n",
    "year_month_counts = {}\n",
    "voltage_month_counts = {}\n",
    "all_para_values = set()   # <--- Store all PARA unique values\n",
    "\n",
    "for vcounts, file_min, file_max, ymc, vmc, unique_para in results:\n",
    "    for v, c in vcounts.items():\n",
    "        voltage_counts[v] = voltage_counts.get(v, 0) + c\n",
    "    if file_min is not None:\n",
    "        min_date = file_min if min_date is None else min(file_min, min_date)\n",
    "    if file_max is not None:\n",
    "        max_date = file_max if max_date is None else max(file_max, max_date)\n",
    "    for k, c in ymc.items():\n",
    "        year_month_counts[k] = year_month_counts.get(k, 0) + c\n",
    "    for k, c in vmc.items():\n",
    "        voltage_month_counts[k] = voltage_month_counts.get(k, 0) + c\n",
    "    all_para_values.update(unique_para)   # <--- Add unique PARA values\n",
    "\n",
    "# Print EDA results\n",
    "print(\"\\nUnique VOLTAGE values and counts:\")\n",
    "for v, c in voltage_counts.items():\n",
    "    print(f\"  {v}: {c}\")\n",
    "\n",
    "print(f\"\\nDate range: {min_date} to {max_date}\")\n",
    "\n",
    "print(\"\\nRecord counts per YEAR-MONTH (first 10):\")\n",
    "for (y, m), c in list(year_month_counts.items())[:10]:\n",
    "    print(f\"  {y}-{m:02}: {c}\")\n",
    "\n",
    "print(\"\\nRecord counts per VOLTAGE and MONTH (first 10):\")\n",
    "for (v, m), c in list(voltage_month_counts.items())[:10]:\n",
    "    print(f\"  {v}, month {m:02}: {c}\")\n",
    "\n",
    "print(\"\\nMonths missing for each VOLTAGE (among all files):\")\n",
    "all_months = set(range(1, 13))\n",
    "voltages = list(voltage_counts.keys())\n",
    "for v in voltages:\n",
    "    months_present = set([m for (volt, m) in voltage_month_counts if volt == v])\n",
    "    missing = sorted(all_months - months_present)\n",
    "    if missing:\n",
    "        print(f\"{v}: missing months {missing}\")\n",
    "    else:\n",
    "        print(f\"{v}: no missing months\")\n",
    "\n",
    "# PARA column unique values and count\n",
    "print(f\"\\nUnique PARA values count: {len(all_para_values)}\")\n",
    "print(\"Some PARA values:\")\n",
    "for val in list(all_para_values)[:20]:\n",
    "    print(\"  \", val)\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0cbf245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 763 files to process.\n",
      "\n",
      "Overall detected SYSTIME formats:\n",
      "  %Y-%m-%d %H:%M:%S: 449382883\n",
      "\n",
      "Total SYSTIME entries: 449382883\n",
      "Unknown SYSTIME formats: 0 (0.00%)\n",
      "\n",
      "Sample unknown SYSTIME values:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "folders = [\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/200/200',\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/200-400/200-400',\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/400-600/400-600',\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/600-759/600-759',\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/SCADA_JAN_24_TO_APR_25'\n",
    "]\n",
    "\n",
    "def guess_format(dt_str):\n",
    "    if re.match(r\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\", dt_str):\n",
    "        return \"%Y-%m-%d %H:%M:%S\"\n",
    "    if re.match(r\"\\d{4}/\\d{2}/\\d{2} \\d{2}:\\d{2}:\\d{2}\", dt_str):\n",
    "        return \"%Y/%m/%d %H:%M:%S\"\n",
    "    if re.match(r\"\\d{2}-\\d{2}-\\d{4} \\d{2}:\\d{2}:\\d{2}\", dt_str):\n",
    "        return \"%d-%m-%Y %H:%M:%S\"\n",
    "    if re.match(r\"\\d{2}/\\d{2}/\\d{4} \\d{2}:\\d{2}:\\d{2}\", dt_str):\n",
    "        return \"%d/%m/%Y %H:%M:%S\"\n",
    "    if re.match(r\"\\d{4}-\\d{2}-\\d{2}\", dt_str):\n",
    "        return \"%Y-%m-%d\"\n",
    "    if re.match(r\"\\d{2}/\\d{2}/\\d{4}\", dt_str):\n",
    "        return \"%d/%m/%Y\"\n",
    "    if re.match(r\"\\d{2}-\\d{2}-\\d{4}\", dt_str):\n",
    "        return \"%d-%m-%Y\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def process_file(file):\n",
    "    try:\n",
    "        df = pd.read_csv(file, usecols=['SYSTIME'], dtype=str, low_memory=True, memory_map=True)\n",
    "        systimes = df['SYSTIME'].dropna()\n",
    "        format_counter = Counter()\n",
    "        total = 0\n",
    "        unknown = 0\n",
    "        unknowns = set()\n",
    "        for val in systimes:\n",
    "            fmt = guess_format(val)\n",
    "            format_counter[fmt] += 1\n",
    "            total += 1\n",
    "            if fmt == \"unknown\":\n",
    "                unknown += 1\n",
    "                unknowns.add(val)\n",
    "        return format_counter, total, unknown, list(unknowns)[:20]\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "        return Counter(), 0, 0, []\n",
    "\n",
    "# Gather all files\n",
    "all_files = []\n",
    "for folder in folders:\n",
    "    all_files.extend(glob.glob(os.path.join(folder, '*.csv')))\n",
    "print(f\"Found {len(all_files)} files to process.\")\n",
    "\n",
    "overall_format_counter = Counter()\n",
    "total_systime = 0\n",
    "total_unknown = 0\n",
    "all_unknowns = set()\n",
    "\n",
    "# Multiprocessing!\n",
    "with ProcessPoolExecutor(max_workers=10) as executor:\n",
    "    for fmt_counter, total, unknown, unknown_list in executor.map(process_file, all_files):\n",
    "        overall_format_counter.update(fmt_counter)\n",
    "        total_systime += total\n",
    "        total_unknown += unknown\n",
    "        all_unknowns.update(unknown_list)\n",
    "\n",
    "print(\"\\nOverall detected SYSTIME formats:\")\n",
    "for fmt, count in overall_format_counter.most_common():\n",
    "    print(f\"  {fmt}: {count}\")\n",
    "\n",
    "print(f\"\\nTotal SYSTIME entries: {total_systime}\")\n",
    "print(f\"Unknown SYSTIME formats: {total_unknown} ({100*total_unknown/total_systime:.2f}%)\")\n",
    "\n",
    "print(\"\\nSample unknown SYSTIME values:\")\n",
    "for v in list(all_unknowns)[:20]:\n",
    "    print(\" \", v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c210e4af",
   "metadata": {},
   "source": [
    "FOR THE 11KV VOLTAGE VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47c880f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 763 files to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_491048/2889965688.py:46: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['MONTH'] = pd.to_datetime(df['SYSTIME'], errors='coerce').dt.month.astype('Int8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved matrix to: /media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/monthly_SWNO_matrix_11KV.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import gc\n",
    "\n",
    "folders = [\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/200/200',\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/200-400/200-400',\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/400-600/400-600',\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/600-759/600-759',\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/SCADA_JAN_24_TO_APR_25'\n",
    "]\n",
    "output_csv = '/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/monthly_SWNO_matrix_11KV.csv'\n",
    "\n",
    "def process_file(file):\n",
    "    try:\n",
    "        dtype_dict = {\n",
    "            'SYSTIME': str,\n",
    "            'SWNO': str,\n",
    "            'VOLTAGE': 'category',\n",
    "            'PARA': 'category',\n",
    "            'VALUE': 'float32'\n",
    "        }\n",
    "        df = pd.read_csv(\n",
    "            file,\n",
    "            usecols=['SYSTIME', 'SWNO', 'VOLTAGE', 'PARA', 'VALUE'],\n",
    "            dtype=dtype_dict,\n",
    "            low_memory=True,\n",
    "            memory_map=True\n",
    "        )\n",
    "        # Clean columns\n",
    "        df['VOLTAGE'] = df['VOLTAGE'].str.upper().str.strip()\n",
    "        df['PARA'] = df['PARA'].str.upper().str.strip()\n",
    "        df['SWNO'] = df['SWNO'].astype(str).str.strip()\n",
    "\n",
    "        # Map all voltage '11' and '11KV' to '11KV'\n",
    "        df.loc[df['VOLTAGE'].isin(['11', '11KV']), 'VOLTAGE'] = '11KV'\n",
    "\n",
    "        # Filter\n",
    "        df = df[(df['VOLTAGE'] == '11KV') & (df['PARA'] == 'I')]\n",
    "        if df.empty:\n",
    "            return None\n",
    "        df['VALUE'] = pd.to_numeric(df['VALUE'], errors='coerce')\n",
    "        df['MONTH'] = pd.to_datetime(df['SYSTIME'], errors='coerce').dt.month.astype('Int8')\n",
    "        df = df.dropna(subset=['MONTH', 'SWNO', 'VALUE'])\n",
    "        grouped = df.groupby(['SWNO', 'MONTH'], observed=True)['VALUE'].mean().reset_index()\n",
    "        del df\n",
    "        gc.collect()\n",
    "        return grouped\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Gather all files\n",
    "all_files = []\n",
    "for folder in folders:\n",
    "    all_files.extend(glob.glob(os.path.join(folder, '*.csv')))\n",
    "print(f\"Found {len(all_files)} files to process.\")\n",
    "\n",
    "results = []\n",
    "with ProcessPoolExecutor(max_workers=10) as executor:\n",
    "    for grouped in executor.map(process_file, all_files):\n",
    "        if grouped is not None and not grouped.empty:\n",
    "            results.append(grouped)\n",
    "            del grouped\n",
    "            gc.collect()\n",
    "\n",
    "if results:\n",
    "    final_df = pd.concat(results, ignore_index=True)\n",
    "    del results\n",
    "    gc.collect()\n",
    "    # Group by SWNO and MONTH again (all files together)\n",
    "    grouped = final_df.groupby(['SWNO', 'MONTH'], observed=True)['VALUE'].mean().reset_index()\n",
    "    del final_df\n",
    "    gc.collect()\n",
    "    # Pivot to get SWNO x 12 months\n",
    "    pivot = grouped.pivot(index='SWNO', columns='MONTH', values='VALUE')\n",
    "    # Ensure all 12 months are present as columns\n",
    "    for i in range(1, 13):\n",
    "        if i not in pivot.columns:\n",
    "            pivot[i] = np.nan\n",
    "    pivot = pivot[[i for i in range(1, 13)]]\n",
    "    pivot.columns = [f'Month_{i:02}' for i in range(1, 13)]\n",
    "    pivot.index.name = 'SWNO'\n",
    "    pivot.to_csv(output_csv, float_format='%.3f')\n",
    "    print(f\"Saved matrix to: {output_csv}\")\n",
    "    del pivot\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"No data processed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30171158",
   "metadata": {},
   "source": [
    "FOR 22 AND 33 KV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c1729c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 763 files to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_142632/2063539051.py:50: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['MONTH'] = pd.to_datetime(df['SYSTIME'], errors='coerce').dt.month.astype('Int8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved matrix to: monthly_SWNO_matrix_22KV_33KV.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import gc\n",
    "\n",
    "folders = [\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/200/200',\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/200-400/200-400',\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/400-600/400-600',\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/600-759/600-759',\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/SCADA_JAN_24_TO_APR_25'\n",
    "]\n",
    "output_csv = '/media/sagark24/New Volume/MERGE CDIS/DATA_GENERATION/monthly_SWNO_matrix_22KV_33KV.csv'\n",
    "\n",
    "def normalize_voltage(v):\n",
    "    v = str(v).upper().replace(\" \", \"\")\n",
    "    if v in ['22', '22KV']:\n",
    "        return '22KV'\n",
    "    if v in ['33', '33KV']:\n",
    "        return '33KV'\n",
    "    return v\n",
    "\n",
    "def process_file(file):\n",
    "    try:\n",
    "        dtype_dict = {\n",
    "            'SYSTIME': str,\n",
    "            'SWNO': str,\n",
    "            'VOLTAGE': 'category',\n",
    "            'PARA': 'category',\n",
    "            'VALUE': 'float32'\n",
    "        }\n",
    "        df = pd.read_csv(\n",
    "            file,\n",
    "            usecols=['SYSTIME', 'SWNO', 'VOLTAGE', 'PARA', 'VALUE'],\n",
    "            dtype=dtype_dict,\n",
    "            low_memory=True,\n",
    "            memory_map=True\n",
    "        )\n",
    "        # Clean and normalize\n",
    "        df['VOLTAGE'] = df['VOLTAGE'].map(normalize_voltage)\n",
    "        df['PARA'] = df['PARA'].str.upper().str.strip()\n",
    "        df['SWNO'] = df['SWNO'].astype(str).str.strip()\n",
    "        # Only for 22KV and 33KV and PARA I\n",
    "        df = df[df['VOLTAGE'].isin(['22KV', '33KV']) & (df['PARA'] == 'I')]\n",
    "        if df.empty:\n",
    "            return None\n",
    "        df['VALUE'] = pd.to_numeric(df['VALUE'], errors='coerce')\n",
    "        df['MONTH'] = pd.to_datetime(df['SYSTIME'], errors='coerce').dt.month.astype('Int8')\n",
    "        df = df.dropna(subset=['MONTH', 'SWNO', 'VALUE', 'VOLTAGE'])\n",
    "        grouped = df.groupby(['SWNO', 'VOLTAGE', 'MONTH'], observed=True)['VALUE'].mean().reset_index()\n",
    "        del df\n",
    "        gc.collect()\n",
    "        return grouped\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Gather all files\n",
    "all_files = []\n",
    "for folder in folders:\n",
    "    all_files.extend(glob.glob(os.path.join(folder, '*.csv')))\n",
    "print(f\"Found {len(all_files)} files to process.\")\n",
    "\n",
    "results = []\n",
    "with ProcessPoolExecutor(max_workers=10) as executor:\n",
    "    for grouped in executor.map(process_file, all_files):\n",
    "        if grouped is not None and not grouped.empty:\n",
    "            results.append(grouped)\n",
    "            del grouped\n",
    "            gc.collect()\n",
    "\n",
    "if results:\n",
    "    final_df = pd.concat(results, ignore_index=True)\n",
    "    del results\n",
    "    gc.collect()\n",
    "    grouped = final_df.groupby(['SWNO', 'VOLTAGE', 'MONTH'], observed=True)['VALUE'].mean().reset_index()\n",
    "    del final_df\n",
    "    gc.collect()\n",
    "    pivot = grouped.pivot(index=['SWNO', 'VOLTAGE'], columns='MONTH', values='VALUE')\n",
    "    # Ensure all months are present\n",
    "    for i in range(1, 13):\n",
    "        if i not in pivot.columns:\n",
    "            pivot[i] = np.nan\n",
    "    pivot = pivot[[i for i in range(1, 13)]]\n",
    "    pivot.columns = [f'Month_{i:02}' for i in range(1, 13)]\n",
    "    pivot.reset_index(inplace=True)\n",
    "    pivot.to_csv(output_csv, float_format='%.3f', index=False)\n",
    "    print(f\"Saved matrix to: {output_csv}\")\n",
    "    del pivot\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"No data processed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67443487",
   "metadata": {},
   "source": [
    "EVERY DAY DATA AVERAGE FROM SCADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bd2915a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 763 files to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_869678/144188203.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['SYSTIME'] = pd.to_datetime(df['SYSTIME'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1876 SWNO × 360 days for year 2024 to: /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/daily_SWNO_matrix_11KV_YEAR2024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n",
      "/tmp/ipykernel_869678/144188203.py:88: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivot[d] = np.nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1745 SWNO × 360 days for year 2025 to: /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/daily_SWNO_matrix_11KV_YEAR2025.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import gc\n",
    "\n",
    "folders = [\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/200/200',\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/200-400/200-400',\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/400-600/400-600',\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/600-759/600-759',\n",
    "    '/media/sagark24/New Volume/MERGE CDIS/2-Year-data/SCADA_JAN_24_TO_APR_25'\n",
    "]\n",
    "output_csv_template = '/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/daily_SWNO_matrix_11KV_YEAR{}.csv'\n",
    "\n",
    "def process_file(file):\n",
    "    try:\n",
    "        dtype_dict = {\n",
    "            'SYSTIME': str,\n",
    "            'VOLTAGE': str,\n",
    "            'PARA': str,\n",
    "            'VALUE': 'float32',\n",
    "            # SWNO as string, if present in file\n",
    "            'SWNO': str,\n",
    "        }\n",
    "        usecols = ['SYSTIME', 'SWNO', 'VOLTAGE', 'PARA', 'VALUE']\n",
    "        df = pd.read_csv(file, usecols=usecols, dtype=dtype_dict, low_memory=True, memory_map=True)\n",
    "        df['VOLTAGE'] = df['VOLTAGE'].str.upper().str.strip()\n",
    "        df['PARA'] = df['PARA'].str.upper().str.strip()\n",
    "        df['SWNO'] = df['SWNO'].astype(str).str.strip()\n",
    "\n",
    "        # Standardize voltage\n",
    "        df.loc[df['VOLTAGE'].isin(['11', '11KV']), 'VOLTAGE'] = '11KV'\n",
    "        df = df[(df['VOLTAGE'] == '11KV') & (df['PARA'] == 'I')]\n",
    "        if df.empty:\n",
    "            return None\n",
    "\n",
    "        # Convert to datetime, extract YEAR and DAYOFYEAR (1–366)\n",
    "        df['SYSTIME'] = pd.to_datetime(df['SYSTIME'], errors='coerce')\n",
    "        df = df.dropna(subset=['SYSTIME', 'SWNO', 'VALUE'])\n",
    "        df['YEAR'] = df['SYSTIME'].dt.year.astype('int32')\n",
    "        df['DAY'] = df['SYSTIME'].dt.dayofyear.astype('int16')\n",
    "        # Ignore day 361+ (for non-leap years), only keep day 1–360\n",
    "        df = df[(df['DAY'] >= 1) & (df['DAY'] <= 360)]\n",
    "\n",
    "        # Make value positive\n",
    "        df['VALUE'] = df['VALUE'].abs()\n",
    "\n",
    "        grouped = df.groupby(['YEAR', 'SWNO', 'DAY'], observed=True)['VALUE'].mean().reset_index()\n",
    "        del df\n",
    "        gc.collect()\n",
    "        return grouped\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Gather all files\n",
    "all_files = []\n",
    "for folder in folders:\n",
    "    all_files.extend(glob.glob(os.path.join(folder, '*.csv')))\n",
    "print(f\"Found {len(all_files)} files to process.\")\n",
    "\n",
    "results = []\n",
    "with ProcessPoolExecutor(max_workers=10) as executor:\n",
    "    for grouped in executor.map(process_file, all_files):\n",
    "        if grouped is not None and not grouped.empty:\n",
    "            results.append(grouped)\n",
    "            del grouped\n",
    "            gc.collect()\n",
    "\n",
    "if results:\n",
    "    final_df = pd.concat(results, ignore_index=True)\n",
    "    del results\n",
    "    gc.collect()\n",
    "\n",
    "    # Group again to handle duplicates across files: for each YEAR, SWNO, DAY, take mean\n",
    "    grouped = final_df.groupby(['YEAR', 'SWNO', 'DAY'], observed=True)['VALUE'].mean().reset_index()\n",
    "    del final_df\n",
    "    gc.collect()\n",
    "\n",
    "    # Now for each YEAR, pivot to SWNO x 360 days\n",
    "    for year, group in grouped.groupby('YEAR'):\n",
    "        pivot = group.pivot(index='SWNO', columns='DAY', values='VALUE')\n",
    "        # Ensure all 360 days are present as columns\n",
    "        for d in range(1, 361):\n",
    "            if d not in pivot.columns:\n",
    "                pivot[d] = np.nan\n",
    "        pivot = pivot[[d for d in range(1, 361)]]\n",
    "        pivot.columns = [f'Day_{d:03d}' for d in range(1, 361)]\n",
    "        pivot.index.name = 'SWNO'\n",
    "        output_csv = output_csv_template.format(year)\n",
    "        pivot.to_csv(output_csv, float_format='%.3f')\n",
    "        print(f\"Saved {pivot.shape[0]} SWNO × {pivot.shape[1]} days for year {year} to: {output_csv}\")\n",
    "        del pivot\n",
    "        gc.collect()\n",
    "else:\n",
    "    print(\"No data processed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56625d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
