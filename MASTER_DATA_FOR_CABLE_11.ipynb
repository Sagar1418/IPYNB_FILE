{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b6553be",
   "metadata": {},
   "source": [
    "BASIC DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a07b5518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 45,487 rows → /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/11_KV_FINAL_HEALTH/AFINAL.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEEDER_ID</th>\n",
       "      <th>FROM_SWITCH</th>\n",
       "      <th>TO_SWITCH</th>\n",
       "      <th>SOURCE_LOCATION</th>\n",
       "      <th>DESTINATION_LOCATION</th>\n",
       "      <th>RANK</th>\n",
       "      <th>SOURCE_SS</th>\n",
       "      <th>DESTINATION_SS</th>\n",
       "      <th>FEEDERID_FULL</th>\n",
       "      <th>DATECREATED</th>\n",
       "      <th>COMMENTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15454</td>\n",
       "      <td>15454</td>\n",
       "      <td>38196</td>\n",
       "      <td>1S-MH-MU-ZST-RSTN-24TH</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-1238</td>\n",
       "      <td>0</td>\n",
       "      <td>24TH ROAD REC-STN</td>\n",
       "      <td>GANGA JAMUNA SANGAM</td>\n",
       "      <td>24THRD_11KV_15454</td>\n",
       "      <td>2009-04-13 00:00:00+00:00</td>\n",
       "      <td>24TH ROAD REC-STN TO GANGA JAMUNA CHS (FROM SW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15454</td>\n",
       "      <td>15454</td>\n",
       "      <td>38196</td>\n",
       "      <td>1S-MH-MU-ZST-RSTN-24TH</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-1238</td>\n",
       "      <td>0</td>\n",
       "      <td>24TH ROAD REC-STN</td>\n",
       "      <td>GANGA JAMUNA SANGAM</td>\n",
       "      <td>24THRD_11KV_15454</td>\n",
       "      <td>2016-07-12 00:00:00+00:00</td>\n",
       "      <td>24TH ROAD REC-STN TO GANGA JAMUNA CHS (JT.NO.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15454</td>\n",
       "      <td>15454</td>\n",
       "      <td>38196</td>\n",
       "      <td>1S-MH-MU-ZST-RSTN-24TH</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-1238</td>\n",
       "      <td>0</td>\n",
       "      <td>24TH ROAD REC-STN</td>\n",
       "      <td>GANGA JAMUNA SANGAM</td>\n",
       "      <td>24THRD_11KV_15454</td>\n",
       "      <td>2009-04-13 00:00:00+00:00</td>\n",
       "      <td>24TH ROAD REC-STN TO GANGA JAMUNA CHS (JT.NO.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15454</td>\n",
       "      <td>15454</td>\n",
       "      <td>38196</td>\n",
       "      <td>1S-MH-MU-ZST-RSTN-24TH</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-1238</td>\n",
       "      <td>0</td>\n",
       "      <td>24TH ROAD REC-STN</td>\n",
       "      <td>GANGA JAMUNA SANGAM</td>\n",
       "      <td>24THRD_11KV_15454</td>\n",
       "      <td>2016-09-12 00:00:00+00:00</td>\n",
       "      <td>24TH ROAD REC-STN TO GANGA JAMUNA CHS (JT.NO.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15454</td>\n",
       "      <td>38195</td>\n",
       "      <td>34116</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-1238</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0894</td>\n",
       "      <td>1</td>\n",
       "      <td>GANGA JAMUNA SANGAM</td>\n",
       "      <td>FORTUNE ENCLAVE</td>\n",
       "      <td>24THRD_11KV_15454</td>\n",
       "      <td>2016-09-12 00:00:00+00:00</td>\n",
       "      <td>GANGA JAMUNA CHS TO FORTUNE ENCLAVE (FROM SWNO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FEEDER_ID FROM_SWITCH TO_SWITCH         SOURCE_LOCATION  \\\n",
       "0     15454       15454     38196  1S-MH-MU-ZST-RSTN-24TH   \n",
       "1     15454       15454     38196  1S-MH-MU-ZST-RSTN-24TH   \n",
       "2     15454       15454     38196  1S-MH-MU-ZST-RSTN-24TH   \n",
       "3     15454       15454     38196  1S-MH-MU-ZST-RSTN-24TH   \n",
       "4     15454       38195     34116  1S-MH-MU-ZST-CL02-1238   \n",
       "\n",
       "     DESTINATION_LOCATION  RANK            SOURCE_SS       DESTINATION_SS  \\\n",
       "0  1S-MH-MU-ZST-CL02-1238     0    24TH ROAD REC-STN  GANGA JAMUNA SANGAM   \n",
       "1  1S-MH-MU-ZST-CL02-1238     0    24TH ROAD REC-STN  GANGA JAMUNA SANGAM   \n",
       "2  1S-MH-MU-ZST-CL02-1238     0    24TH ROAD REC-STN  GANGA JAMUNA SANGAM   \n",
       "3  1S-MH-MU-ZST-CL02-1238     0    24TH ROAD REC-STN  GANGA JAMUNA SANGAM   \n",
       "4  1S-MH-MU-ZST-CL02-0894     1  GANGA JAMUNA SANGAM      FORTUNE ENCLAVE   \n",
       "\n",
       "       FEEDERID_FULL                DATECREATED  \\\n",
       "0  24THRD_11KV_15454  2009-04-13 00:00:00+00:00   \n",
       "1  24THRD_11KV_15454  2016-07-12 00:00:00+00:00   \n",
       "2  24THRD_11KV_15454  2009-04-13 00:00:00+00:00   \n",
       "3  24THRD_11KV_15454  2016-09-12 00:00:00+00:00   \n",
       "4  24THRD_11KV_15454  2016-09-12 00:00:00+00:00   \n",
       "\n",
       "                                            COMMENTS  \n",
       "0  24TH ROAD REC-STN TO GANGA JAMUNA CHS (FROM SW...  \n",
       "1  24TH ROAD REC-STN TO GANGA JAMUNA CHS (JT.NO.2...  \n",
       "2  24TH ROAD REC-STN TO GANGA JAMUNA CHS (JT.NO.2...  \n",
       "3  24TH ROAD REC-STN TO GANGA JAMUNA CHS (JT.NO.2...  \n",
       "4  GANGA JAMUNA CHS TO FORTUNE ENCLAVE (FROM SWNO...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feeder_trace_latest_audit_with_rank.py  ✧  July 2025\n",
    "\"\"\"\n",
    "Workflow (HT-cable only)\n",
    "========================\n",
    "1. Load HTCABLE.csv, drop unused columns, remove fully-identical rows.\n",
    "2. Trace every feeder edge-by-edge, annotate with RANK (distance from feeder start).\n",
    "3. Add SOURCE_SS / DESTINATION_SS and split FROM_TO → FROM_SWITCH & TO_SWITCH.\n",
    "4. Export to Excel – nothing from the energy-audit file is touched.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List, Set, Optional\n",
    "\n",
    "# ── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "INPUT_HT    = \"/media/sagark24/New Volume/MERGE CDIS/2-Year-data/CLEANED_DATA/ht_cleaned.csv\"                     # adjust path if needed\n",
    "OUTPUT_PATH = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/11_KV_FINAL_HEALTH/AFINAL.csv\"\n",
    "\n",
    "FEEDER_ID_COL  = \"FEEDERID\"\n",
    "SRC_SWITCH_COL = \"SOURCE_SWITCH_ID\"\n",
    "DST_SWITCH_COL = \"DESTINATION_SWITCH_ID\"\n",
    "SRC_LOC_COL    = \"SOURCE_SSFL\"\n",
    "DST_LOC_COL    = \"DESTINATION_SSFL\"\n",
    "FEEDER_ID_COL2  = \"FEEDERID\"\n",
    "DATE = \"DATECREATED\"\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def _feeder_token(val: str | int | float | None) -> Optional[str]:\n",
    "    if not isinstance(val, str):\n",
    "        val = str(val) if val is not None else \"\"\n",
    "    p = val.split(\"_\")\n",
    "    return p[2] if len(p) >= 3 and (p[1] == '11kV' or p[1]=='11Kv' or p[1]=='11KV') else None\n",
    "\n",
    "def _feeder_token2(val: str | int | float | None) -> Optional[str]:\n",
    "    if not isinstance(val, str):\n",
    "        val = str(val) if val is not None else \"\"\n",
    "    p = val.split(\"_\")\n",
    "    return val if len(p) >= 3 and (p[1] == '11kV' or p[1]=='11Kv' or p[1]=='11KV') else None\n",
    "\n",
    "\n",
    "# 1️  LOAD & CLEAN HT-CABLE ---------------------------------------------------\n",
    "ht = (pd.read_csv(Path(INPUT_HT).expanduser(), low_memory=False)\n",
    "        \n",
    "        .drop_duplicates())\n",
    "ht[\"FEEDERID_FULL\"] = ht[FEEDER_ID_COL2].apply(_feeder_token2).dropna()\n",
    "ht[\"FEEDER_ID\"] = ht[FEEDER_ID_COL].apply(_feeder_token)\n",
    "for col in [SRC_SWITCH_COL, DST_SWITCH_COL, SRC_LOC_COL, DST_LOC_COL]:\n",
    "    ht[col] = ht[col].astype(str)\n",
    "\n",
    "# Ensure SOURCE_SS / DESTINATION_SS exist\n",
    "if {\"SOURCE_SS\", \"DESTINATION_SS\"}.issubset(ht.columns):\n",
    "    pass\n",
    "else:\n",
    "    ht[\"SOURCE_SS\"] = ht[SRC_LOC_COL]\n",
    "    ht[\"DESTINATION_SS\"] = ht[DST_LOC_COL]\n",
    "\n",
    "edge_cols = [SRC_SWITCH_COL, DST_SWITCH_COL, SRC_LOC_COL, DST_LOC_COL]\n",
    "source_idx: Dict[Tuple[str, str], pd.DataFrame] = {\n",
    "    (k[0], k[1]): g[edge_cols]\n",
    "    for k, g in ht.groupby([SRC_LOC_COL, \"FEEDER_ID\"], sort=False)\n",
    "}\n",
    "\n",
    "# 2️  FEEDER TRACER -----------------------------------------------------------\n",
    "def trace_feeder(fid: str) -> List[dict]:\n",
    "    rows: List[dict] = []\n",
    "    visited: Set[Tuple[str, str]] = set()\n",
    "\n",
    "    start = ht[(ht[SRC_SWITCH_COL] == fid) & (ht[\"FEEDER_ID\"] == fid)][edge_cols]\n",
    "    queue = [(row, 0) for row in start.to_records(index=False).tolist()]\n",
    "\n",
    "    while queue:\n",
    "        (from_sw, to_sw, src_loc, dst_loc), rank = queue.pop(0)\n",
    "        if (from_sw, to_sw) in visited:\n",
    "            continue\n",
    "        visited.add((from_sw, to_sw))\n",
    "\n",
    "        rows.append({\n",
    "            \"FEEDER_ID\": fid,\n",
    "            \"FROM_SWITCH\": from_sw,\n",
    "            \"TO_SWITCH\": to_sw,\n",
    "         \n",
    "            \"SOURCE_LOCATION\": src_loc,\n",
    "            \"DESTINATION_LOCATION\": dst_loc,\n",
    "            \"RANK\": rank\n",
    "        })\n",
    "\n",
    "        nxt = source_idx.get((dst_loc, fid))\n",
    "        if nxt is not None and not nxt.empty:\n",
    "            queue.extend([(row, rank + 1)\n",
    "                          for row in nxt.to_records(index=False).tolist()])\n",
    "    return rows\n",
    "\n",
    "# 3️  TRACE ALL FEEDERS -------------------------------------------------------\n",
    "trace_df = pd.DataFrame([row\n",
    "                         for fid in ht[\"FEEDER_ID\"].dropna().unique()\n",
    "                         for row in trace_feeder(str(fid))])\n",
    "\n",
    "# 4️  ADD SS COLUMNS ----------------------------------------------------------\n",
    "trace_df = (trace_df.merge(ht[[SRC_LOC_COL, DST_LOC_COL,\n",
    "                               \"SOURCE_SS\", \"DESTINATION_SS\" ,\"FEEDERID_FULL\", \"DATECREATED\" ,\"COMMENTS\"]]\n",
    "                           .rename(columns={SRC_LOC_COL: \"SOURCE_LOCATION\",\n",
    "                                            DST_LOC_COL: \"DESTINATION_LOCATION\"})\n",
    "                           .drop_duplicates(),\n",
    "                           how=\"left\",\n",
    "                           on=[\"SOURCE_LOCATION\", \"DESTINATION_LOCATION\"]))\n",
    "\n",
    "# 5️  EXPORT ------------------------------------------------------------------\n",
    "out_cols = [\"FEEDER_ID\",\"FEEDERID_FULL\",\n",
    "            \"FROM_SWITCH\", \"TO_SWITCH\", \n",
    "            \"SOURCE_SS\", \"DESTINATION_SS\",\n",
    "            \"SOURCE_LOCATION\", \"DESTINATION_LOCATION\",\n",
    "            \"RANK\",\"DATECREATED\", \"COMMENTS\"]\n",
    "\n",
    "trace_df.to_csv(OUTPUT_PATH, index=False, columns=out_cols)\n",
    "print(f\"\\nSaved {len(trace_df):,} rows → {OUTPUT_PATH}\")\n",
    "\n",
    "# Preview for interactive sessions\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(trace_df.head())\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901c7081",
   "metadata": {},
   "source": [
    "next file generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "813e11af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  All data   /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/11_KV_FINAL_HEALTH/AFINAL_full.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ── PATHS ────────────────────────────────────────────────────────────────────\n",
    "AFINAL_PATH        = Path(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/11_KV_FINAL_HEALTH/AFINAL.csv\")\n",
    "ENERGY_AUDIT_PATH  = Path(\"/media/sagark24/New Volume/MERGE CDIS/2-Year-data/CLEANED_DATA/energyaudit_cleaned.csv\")\n",
    "FEEDERDETAIL_PATH  = Path(\"/media/sagark24/New Volume/MERGE CDIS/2-Year-data/CLEANED_DATA/feederdetails_cleaned.csv\")\n",
    "OUTPUT_CSV_PATH    = Path(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/11_KV_FINAL_HEALTH/AFINAL_full.csv\")\n",
    "\n",
    "# ── 1) LOAD AFINAL -----------------------------------------------------------\n",
    "df = pd.read_csv(AFINAL_PATH, low_memory=False)\n",
    "df.columns = [c.upper() for c in df.columns]\n",
    "\n",
    "# ── 2) SWITCH-LEVEL STATS (ENERGYAUDIT) -------------------------------------\n",
    "audit = (pd.read_csv(\n",
    "            ENERGY_AUDIT_PATH,\n",
    "            usecols=[\"SWITCH_NO\", \"LOAD_FACTOR\", \"Y_INST_VOLTAGE\", \"CLUSTER_TYPE\"],\n",
    "            low_memory=False\n",
    "         ).rename(str.upper, axis=1))\n",
    "\n",
    "audit[\"SWITCH_NO\"]      = audit[\"SWITCH_NO\"].astype(str).str.strip()\n",
    "audit[\"LOAD_FACTOR\"]    = pd.to_numeric(audit[\"LOAD_FACTOR\"],    errors=\"coerce\")\n",
    "audit[\"Y_INST_VOLTAGE\"] = pd.to_numeric(audit[\"Y_INST_VOLTAGE\"], errors=\"coerce\")\n",
    "\n",
    "def first_non_null(x):\n",
    "    y = x.dropna()\n",
    "    return y.iloc[0] if len(y) else None\n",
    "\n",
    "switch_stats = (audit.dropna(subset=[\"SWITCH_NO\"])\n",
    "                     .groupby(\"SWITCH_NO\")\n",
    "                     .agg(\n",
    "                         # LOAD_FACTOR\n",
    "                         SWITCH_LOAD_FACTOR_MEAN   = (\"LOAD_FACTOR\", \"mean\"),\n",
    "                         SWITCH_LOAD_FACTOR_MEDIAN = (\"LOAD_FACTOR\", \"median\"),\n",
    "                         SWITCH_LOAD_FACTOR_STD    = (\"LOAD_FACTOR\", \"std\"),\n",
    "                         SWITCH_LOAD_FACTOR_MIN    = (\"LOAD_FACTOR\", \"min\"),\n",
    "                         SWITCH_LOAD_FACTOR_MAX    = (\"LOAD_FACTOR\", \"max\"),\n",
    "                         # Y_INST_VOLTAGE\n",
    "                         SWITCH_Y_INST_VOLTAGE_MEAN   = (\"Y_INST_VOLTAGE\", \"mean\"),\n",
    "                         SWITCH_Y_INST_VOLTAGE_MEDIAN = (\"Y_INST_VOLTAGE\", \"median\"),\n",
    "                         SWITCH_Y_INST_VOLTAGE_STD    = (\"Y_INST_VOLTAGE\", \"std\"),\n",
    "                         SWITCH_Y_INST_VOLTAGE_MIN    = (\"Y_INST_VOLTAGE\", \"min\"),\n",
    "                         SWITCH_Y_INST_VOLTAGE_MAX    = (\"Y_INST_VOLTAGE\", \"max\"),\n",
    "                         # categorical\n",
    "                         CLUSTER_TYPE = (\"CLUSTER_TYPE\", first_non_null)\n",
    "                     )\n",
    "                     .reset_index())\n",
    "\n",
    "# switch_stats[[\"SWITCH_LOAD_FACTOR_STD\",\"SWITCH_Y_INST_VOLTAGE_STD\"]] = \\\n",
    "#     switch_stats[[\"SWITCH_LOAD_FACTOR_STD\",\"SWITCH_Y_INST_VOLTAGE_STD\"]].fillna(0)\n",
    "\n",
    "df[\"FROM_SWITCH\"] = df[\"FROM_SWITCH\"].astype(str).str.strip()\n",
    "df = (df.merge(switch_stats, how=\"left\",\n",
    "               left_on=\"FROM_SWITCH\", right_on=\"SWITCH_NO\")\n",
    "        .drop(columns=[\"SWITCH_NO\"]))\n",
    "\n",
    "# ── 3) FEEDER-LEVEL STATS (FEEDERDETAILS) -----------------------------------\n",
    "feeder = (pd.read_csv(\n",
    "            FEEDERDETAIL_PATH,\n",
    "            usecols=[\"SWITCHID\", \"FEEDERLOAD\", \"LOADFACTOR\", \"LOADLOSSFACTOR\"],\n",
    "            low_memory=False\n",
    "         ).rename(str.upper, axis=1))\n",
    "\n",
    "feeder[\"SWITCHID\"]        = feeder[\"SWITCHID\"].astype(str).str.strip()\n",
    "for col in [\"FEEDERLOAD\", \"LOADFACTOR\", \"LOADLOSSFACTOR\"]:\n",
    "    feeder[col] = pd.to_numeric(feeder[col], errors=\"coerce\")\n",
    "\n",
    "feeder_stats = (feeder.dropna(subset=[\"SWITCHID\"])\n",
    "                      .groupby(\"SWITCHID\")\n",
    "                      .agg(\n",
    "                          # FEEDERLOAD\n",
    "                          FEEDER_LOAD_MEAN   = (\"FEEDERLOAD\", \"mean\"),\n",
    "                          FEEDER_LOAD_MEDIAN = (\"FEEDERLOAD\", \"median\"),\n",
    "                          FEEDER_LOAD_STD    = (\"FEEDERLOAD\", \"std\"),\n",
    "                          FEEDER_LOAD_MIN    = (\"FEEDERLOAD\", \"min\"),\n",
    "                          FEEDER_LOAD_MAX    = (\"FEEDERLOAD\", \"max\"),\n",
    "                          # LOADFACTOR\n",
    "                          FEEDER_LOAD_FACTOR_MEAN   = (\"LOADFACTOR\", \"mean\"),\n",
    "                          FEEDER_LOAD_FACTOR_MEDIAN = (\"LOADFACTOR\", \"median\"),\n",
    "                          FEEDER_LOAD_FACTOR_STD    = (\"LOADFACTOR\", \"std\"),\n",
    "                          FEEDER_LOAD_FACTOR_MIN    = (\"LOADFACTOR\", \"min\"),\n",
    "                          FEEDER_LOAD_FACTOR_MAX    = (\"LOADFACTOR\", \"max\"),\n",
    "                          # LOADLOSSFACTOR\n",
    "                          FEEDER_LOSS_FACTOR_MEAN   = (\"LOADLOSSFACTOR\", \"mean\"),\n",
    "                          FEEDER_LOSS_FACTOR_MEDIAN = (\"LOADLOSSFACTOR\", \"median\"),\n",
    "                          FEEDER_LOSS_FACTOR_STD    = (\"LOADLOSSFACTOR\", \"std\"),\n",
    "                          FEEDER_LOSS_FACTOR_MIN    = (\"LOADLOSSFACTOR\", \"min\"),\n",
    "                          FEEDER_LOSS_FACTOR_MAX    = (\"LOADLOSSFACTOR\", \"max\")\n",
    "                      )\n",
    "                      .reset_index())\n",
    "\n",
    "std_cols = [c for c in feeder_stats.columns if c.endswith(\"_STD\")]\n",
    "feeder_stats[std_cols] = feeder_stats[std_cols].fillna(0)\n",
    "\n",
    "# ...  merge feeder_stats ---------------------------------------\n",
    "df[\"FEEDER_ID\"] = df[\"FEEDER_ID\"].astype(str).str.strip()\n",
    "df = (df.merge(feeder_stats, how=\"left\",\n",
    "               left_on=\"FEEDER_ID\", right_on=\"SWITCHID\")\n",
    "        .drop(columns=[\"SWITCHID\"]))\n",
    "\n",
    "# ── KEEP FEEDER-STAT COLUMNS ONLY ON THE FIRST ROW OF EACH FEEDER ───────────\n",
    "# (place this right after the merge with feeder_stats and BEFORE step 4)\n",
    "\n",
    "# all columns that start with FEEDER_  *except* the ID itself\n",
    "feeder_stat_cols = [\n",
    "    c for c in df.columns\n",
    "    if c.startswith(\"FEEDER_\") and c != \"FEEDER_ID\"\n",
    "]\n",
    "\n",
    "# mask: True on very first row for each FEEDER_ID\n",
    "first_row_mask = ~df.duplicated(subset=\"FEEDER_ID\", keep=\"first\")\n",
    "\n",
    "# set stats to NA on later rows; FEEDER_ID column is left alone\n",
    "df.loc[~first_row_mask, feeder_stat_cols] = pd.NA\n",
    "\n",
    "# ── 4) APPEND GLOBAL MIN / MAX ROWS ----------------------------\n",
    "num_cols = df.select_dtypes(include=\"number\").columns\n",
    "min_row  = df[num_cols].min().rename(\"GLOBAL_MIN\")\n",
    "max_row  = df[num_cols].max().rename(\"GLOBAL_MAX\")\n",
    "\n",
    "\n",
    "for col in df.columns:\n",
    "    if col not in num_cols:\n",
    "        min_row[col] = \"\"\n",
    "        max_row[col] = \"\"\n",
    "\n",
    "df_full = pd.concat([df, min_row.to_frame().T, max_row.to_frame().T],\n",
    "                    ignore_index=True)\n",
    "\n",
    "# ── 5) WRITE SINGLE CSV ------------------------------------------------------\n",
    "df_full.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "print(f\"  All data   {OUTPUT_CSV_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd452569",
   "metadata": {},
   "source": [
    "ADD NETWORKDETAILS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3e7e436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Updated file written  /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/11_KV_FINAL_HEALTH/AFINAL_full.csv\n",
      "Rows with CABLESIZE filled: 35472\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_main    = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/11_KV_FINAL_HEALTH/AFINAL_full.csv\"\n",
    "csv_network = \"/media/sagark24/New Volume/MERGE CDIS/2-Year-data/CLEANED_DATA/networkdetails_cleaned.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_main, low_memory=False)\n",
    "df.columns = [c.upper() for c in df.columns]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "wanted = [\n",
    "    \"FROM_SWITCHID\", \"CABLESIZE\",        # string\n",
    "    \"RESISTANCE\", \"LENGTH\", \"LENGTH_KM\", # numeric → mean\n",
    "    \"NOOFJOINTS\", \"NOOFSUBSTATION\",      # **counts → sum**\n",
    "    \"LTCURRENT\", \"SECTIONLOAD\", \"SECTIONLOSS_KW\"\n",
    "]\n",
    "\n",
    "present  = pd.read_csv(csv_network, nrows=0).columns.str.upper()\n",
    "use_cols = [c for c in wanted if c in present]\n",
    "\n",
    "network = (pd.read_csv(csv_network, usecols=use_cols, low_memory=False)\n",
    "             .rename(str.upper, axis=1))\n",
    "\n",
    "network[\"FROM_SWITCHID\"] = network[\"FROM_SWITCHID\"].astype(str).str.strip()\n",
    "df[\"FROM_SWITCH\"]        = df[\"FROM_SWITCH\"].astype(str).str.strip()\n",
    "\n",
    "# force numerics where appropriate\n",
    "num_try = [\"RESISTANCE\",\"LENGTH\",\"LENGTH_KM\",\n",
    "           \"LTCURRENT\",\"SECTIONLOAD\",\"SECTIONLOSS_KW\"]\n",
    "for col in num_try + [\"NOOFJOINTS\",\"NOOFSUBSTATION\"]:\n",
    "    if col in network.columns:\n",
    "        network[col] = pd.to_numeric(network[col], errors=\"coerce\")\n",
    "\n",
    "# ----- aggregation rules ----------------------------------------------------\n",
    "def first_non_null(s):\n",
    "    x = s.dropna()\n",
    "    return x.iloc[0] if len(x) else None\n",
    "\n",
    "agg_dict = {\n",
    "    # default rule for all numeric columns we coerced earlier\n",
    "    **{c: \"mean\" for c in num_try if c in network.columns},\n",
    "    # counts → SUM so they remain integers\n",
    "    **{c: \"sum\"  for c in [\"NOOFJOINTS\",\"NOOFSUBSTATION\"] if c in network.columns},\n",
    "}\n",
    "\n",
    "if \"CABLESIZE\" in network.columns:\n",
    "    agg_dict[\"CABLESIZE\"] = first_non_null\n",
    "\n",
    "net_agg = (network.groupby(\"FROM_SWITCHID\", dropna=False)\n",
    "                  .agg(agg_dict)\n",
    "                  .reset_index())\n",
    "\n",
    "# join\n",
    "df = (df.merge(net_agg, how=\"left\",\n",
    "               left_on=\"FROM_SWITCH\", right_on=\"FROM_SWITCHID\")\n",
    "        .drop(columns=[\"FROM_SWITCHID\"]))\n",
    "\n",
    "# cast summed counts to nullable Int64 for cleanliness\n",
    "for c in [\"NOOFJOINTS\",\"NOOFSUBSTATION\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].round().astype(\"Int64\")\n",
    "\n",
    "# save\n",
    "df.to_csv(csv_main, index=False)\n",
    "print(f\"  Updated file written  {csv_main}\")\n",
    "\n",
    "if \"CABLESIZE\" in df.columns:\n",
    "    print(\"Rows with CABLESIZE filled:\", df[\"CABLESIZE\"].notna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1b4ef4",
   "metadata": {},
   "source": [
    "FAULT DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03aa0bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cable rows with CBL_FAULT_COUNT : 16032\n",
      " feeders with FDR_FAULT_COUNT    : 1543\n",
      " updated file                    : /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/11_KV_FINAL_HEALTH/AFINAL_full.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# ── FILE LOCATIONS ───────────────────────────────────────────────────────────\n",
    "CABLE_PATH  = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/11_KV_FINAL_HEALTH/AFINAL_full.csv\"\n",
    "FAULT_PATH  = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/HT_fault_cable_info_processed2.csv\"\n",
    "\n",
    "OUT_PATH    = CABLE_PATH   # overwrite; change if you want a new file\n",
    "# FEEDER_SUMMARY_PATH = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/11_KV_FINAL_HEALTH/FEEDER_FAULT_SUMMARY.csv\"\n",
    "\n",
    "# ── 1) LOAD ------------------------------------------------------------------\n",
    "cables = pd.read_csv(CABLE_PATH, low_memory=False)\n",
    "faults = pd.read_csv(FAULT_PATH, low_memory=False)\n",
    "\n",
    "cables.columns = [c.upper() for c in cables.columns]\n",
    "faults.columns = [c.upper() for c in faults.columns]\n",
    "\n",
    "# ── 2) NORMALISE ID STRINGS --------------------------------------------------\n",
    "def norm_id(s: pd.Series) -> pd.Series:\n",
    "    return (\n",
    "        s.astype(str)\n",
    "         .str.strip()\n",
    "         .str.upper()\n",
    "         .str.replace(r\"\\.0+$\", \"\", regex=True)        # 15454.0 → 15454\n",
    "         .str.replace(r\"[^A-Z0-9]\", \"\", regex=True)    # keep A-Z,0-9\n",
    "         .str.lstrip(\"0\")                              # drop leading zeros\n",
    "    )\n",
    "\n",
    "cables[\"FROM_SWITCH_N\"] = norm_id(cables[\"FROM_SWITCH\"])\n",
    "cables[\"FEEDER_ID_N\"]   = norm_id(cables[\"FEEDER_ID\"])\n",
    "faults[\"FROM_SWITCH_N\"] = norm_id(faults[\"FROM_SWITCH\"])\n",
    "faults[\"SWITCH_NO_N\"]   = norm_id(faults[\"SWITCH_NO\"])\n",
    "\n",
    "# ── 3) PREP FAULT NUMERICS ---------------------------------------------------\n",
    "faults[\"TIME_OUTAGE\"]           = pd.to_datetime(faults[\"TIME_OUTAGE\"], errors=\"coerce\")\n",
    "faults[\"TIME_DIFFERENCE_HOURS\"] = pd.to_numeric(faults[\"TIME_DIFFERENCE_HOURS\"], errors=\"coerce\")\n",
    "\n",
    "def mode(series):\n",
    "    nn = series.dropna()\n",
    "    return Counter(nn).most_common(1)[0][0] if len(nn) else None\n",
    "\n",
    "# ── 4-A) CABLE-LEVEL STATS  (prefix CBL_) -----------------------------------\n",
    "cbl_stats = (\n",
    "    faults.groupby(\"FROM_SWITCH_N\")\n",
    "          .agg(\n",
    "              CBL_FAULT_COUNT          = (\"FROM_SWITCH_N\", \"size\"),\n",
    "              CBL_AVG_REPAIR_HRS       = (\"TIME_DIFFERENCE_HOURS\", \"mean\"),\n",
    "              CBL_MAX_REPAIR_HRS       = (\"TIME_DIFFERENCE_HOURS\", \"max\"),\n",
    "              CBL_LATEST_OUTAGE        = (\"TIME_OUTAGE\", \"max\"),\n",
    "              CBL_COMMON_REASON_CAT    = (\"REASON_CATEGORY\", mode),\n",
    "              CBL_COMMON_REASON_TEXT   = (\"REASON_TEXT\",    mode),\n",
    "              CBL_COMMON_RELAY_FUSE    = (\"RELAY_FUSE\",     mode),\n",
    "          )\n",
    "          .reset_index()\n",
    ")\n",
    "cbl_stats[[\"CBL_AVG_REPAIR_HRS\",\"CBL_MAX_REPAIR_HRS\"]] = (\n",
    "    cbl_stats[[\"CBL_AVG_REPAIR_HRS\",\"CBL_MAX_REPAIR_HRS\"]].round(2)\n",
    ")\n",
    "\n",
    "# ── 4-B) FEEDER-LEVEL STATS  (prefix FDR_) ----------------------------------\n",
    "fdr_stats = (\n",
    "    faults.groupby(\"SWITCH_NO_N\")\n",
    "          .agg(\n",
    "              FDR_FAULT_COUNT          = (\"SWITCH_NO_N\", \"size\"),\n",
    "              FDR_AVG_REPAIR_HRS       = (\"TIME_DIFFERENCE_HOURS\", \"mean\"),\n",
    "              FDR_MAX_REPAIR_HRS       = (\"TIME_DIFFERENCE_HOURS\", \"max\"),\n",
    "              FDR_FIRST_OUTAGE         = (\"TIME_OUTAGE\", \"min\"),\n",
    "              FDR_LAST_OUTAGE          = (\"TIME_OUTAGE\", \"max\"),\n",
    "              FDR_COMMON_REASON_CAT    = (\"REASON_CATEGORY\", mode),\n",
    "              FDR_COMMON_REASON_TEXT   = (\"REASON_TEXT\",    mode),\n",
    "              FDR_COMMON_RELAY_FUSE    = (\"RELAY_FUSE\",     mode),\n",
    "          )\n",
    "          .reset_index()\n",
    ")\n",
    "fdr_stats[[\"FDR_AVG_REPAIR_HRS\",\"FDR_MAX_REPAIR_HRS\"]] = (\n",
    "    fdr_stats[[\"FDR_AVG_REPAIR_HRS\",\"FDR_MAX_REPAIR_HRS\"]].round(2)\n",
    ")\n",
    "\n",
    "# ── 5) MERGE NEW COLUMNS -----------------------------------------------------\n",
    "# (we never drop existing columns; we just add new ones)\n",
    "cables = cables.merge(cbl_stats, how=\"left\", on=\"FROM_SWITCH_N\")\n",
    "cables = cables.merge(\n",
    "            fdr_stats.rename(columns={\"SWITCH_NO_N\": \"FEEDER_ID_N\"}),\n",
    "            how=\"left\", on=\"FEEDER_ID_N\"\n",
    ")\n",
    "\n",
    "# ── 6) KEEP FEEDER_… COLUMNS ONLY ON FIRST CABLE OF EACH FEEDER -------------\n",
    "feeder_cols = [c for c in cables.columns if c.startswith(\"FDR_\")]\n",
    "first_row_mask = ~cables.duplicated(subset=\"FEEDER_ID_N\", keep=\"first\")\n",
    "cables.loc[~first_row_mask, feeder_cols] = pd.NA   # blanks after first row\n",
    "\n",
    "# ── 7) CLEAN-UP & SAVE -------------------------------------------------------\n",
    "cables = cables.drop(columns=[\"FROM_SWITCH_N\",\"FEEDER_ID_N\"])\n",
    "cables.to_csv(OUT_PATH, index=False)\n",
    "# fdr_stats.rename(columns={\"SWITCH_NO_N\":\"FEEDER_ID\"}).to_csv(FEEDER_SUMMARY_PATH, index=False)\n",
    "\n",
    "print(\" cable rows with CBL_FAULT_COUNT :\", cables['CBL_FAULT_COUNT'].notna().sum())\n",
    "print(\" feeders with FDR_FAULT_COUNT    :\", fdr_stats.shape[0])\n",
    "print(\" updated file                    :\", OUT_PATH)\n",
    "# print(\" feeder summary                  :\", FEEDER_SUMMARY_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e19d005",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "st_cytoscapejs() got an unexpected keyword argument 'layout'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstreamlit_cytoscapejs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcyto\u001b[39;00m\n\u001b[1;32m      4\u001b[0m elements \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode 1\u001b[39m\u001b[38;5;124m\"\u001b[39m}},\n\u001b[1;32m      6\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtwo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode 2\u001b[39m\u001b[38;5;124m\"\u001b[39m}},\n\u001b[1;32m      7\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtwo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEdge from 1 to 2\u001b[39m\u001b[38;5;124m\"\u001b[39m}}\n\u001b[1;32m      8\u001b[0m ]\n\u001b[0;32m---> 10\u001b[0m selected \u001b[38;5;241m=\u001b[39m \u001b[43mcyto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mst_cytoscapejs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43melements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43melements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m100\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m500px\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcytoscape\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selected:\n\u001b[1;32m     19\u001b[0m     st\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelected:\u001b[39m\u001b[38;5;124m\"\u001b[39m, selected)\n",
      "\u001b[0;31mTypeError\u001b[0m: st_cytoscapejs() got an unexpected keyword argument 'layout'"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import streamlit_cytoscapejs as cyto\n",
    "\n",
    "elements = [\n",
    "    {\"data\": {\"id\": \"one\", \"label\": \"Node 1\"}},\n",
    "    {\"data\": {\"id\": \"two\", \"label\": \"Node 2\"}},\n",
    "    {\"data\": {\"source\": \"one\", \"target\": \"two\", \"label\": \"Edge from 1 to 2\"}}\n",
    "]\n",
    "\n",
    "selected = cyto.st_cytoscapejs(\n",
    "    elements=elements,\n",
    "    layout={'name': 'cose'},\n",
    "    width='100%',\n",
    "    height='500px',\n",
    "    key=\"cytoscape\"\n",
    ")\n",
    "\n",
    "if selected:\n",
    "    st.write(\"Selected:\", selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06c98fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
