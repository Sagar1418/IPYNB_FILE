{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e012dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feeder_trace_latest_audit.py  –  dedup HT + audit average (v9)\n",
    "\"\"\"\n",
    "Workflow\n",
    "========\n",
    "1. **Load** `HTCABLE.csv`, drop unused columns, remove fully-identical rows.\n",
    "2. **Trace** every feeder edge‑by‑edge.\n",
    "3. **Load** `ENERGYAUDIT.csv`, for each transformer (`FUNC_LOC`) compute\n",
    "   * `LATEST_DT_DATE`  → most‑recent `SYSTEM_DATE`\n",
    "   * `DT_LOAD`         → **average** `MD_KVA` across all rows\n",
    "4. **Merge** audit stats onto trace (`DESTINATION_LOCATION` = `FUNC_LOC`).\n",
    "5. **Export** `final_two_column.xlsx` with the six requested columns.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List, Set, Optional\n",
    "\n",
    "# ── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "INPUT_HT      = \"HTCABLE.csv\"\n",
    "INPUT_ENERGY  = \"/media/sagarkumar/New Volume/SAGAR/ENERGYAUDIT.csv\"\n",
    "OUTPUT_PATH   = \"final_two_column.xlsx\"\n",
    "\n",
    "FEEDER_ID_COL  = \"FEEDERID\"\n",
    "SRC_SWITCH_COL = \"SOURCE_SWITCH_ID\"\n",
    "DST_SWITCH_COL = \"DESTINATION_SWITCH_ID\"\n",
    "SRC_LOC_COL    = \"SOURCE_SSFL\"\n",
    "DST_LOC_COL    = \"DESTINATION_SSFL\"   # ≡ FUNC_LOC in audit\n",
    "\n",
    "FUNC_LOC_COL = \"FUNC_LOC\"\n",
    "DATE_COL     = \"SYSTEM_DATE\"\n",
    "LOAD_COL     = \"MD_KVA\"\n",
    "\n",
    "REDUNDANT_COLS = [\n",
    "    \"COMMENTS\", \"GLOBALID\", \"MEASUREDLENGTH\", \"UNNAMED: 0\", \"OBJECTID\"\n",
    "]\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1️⃣  LOAD & CLEAN HT-CABLE ---------------------------------------------------\n",
    "ht_path = Path(INPUT_HT).expanduser()\n",
    "if not ht_path.exists():\n",
    "    raise FileNotFoundError(ht_path)\n",
    "\n",
    "ht = (pd.read_excel(ht_path, engine=\"openpyxl\")\n",
    "      if ht_path.suffix.lower() in {\".xls\", \".xlsx\"}\n",
    "      else pd.read_csv(ht_path, low_memory=False))\n",
    "ht.columns = [c.upper() for c in ht.columns]\n",
    "\n",
    "ht = ht.drop(columns=[c for c in REDUNDANT_COLS if c in ht.columns], errors=\"ignore\")\n",
    "ht = ht.drop_duplicates()  # remove fully-identical rows\n",
    "\n",
    "# helper to pull token after 2nd underscore\n",
    "\n",
    "def _feeder_token(val: str | int | float | None) -> Optional[str]:\n",
    "    if not isinstance(val, str):\n",
    "        val = str(val) if val is not None else \"\"\n",
    "    p = val.split(\"_\")\n",
    "    return p[2] if len(p) >= 3 else None\n",
    "\n",
    "ht[\"FEEDER_ID\"] = ht[FEEDER_ID_COL].apply(_feeder_token)\n",
    "\n",
    "for col in [SRC_SWITCH_COL, DST_SWITCH_COL, SRC_LOC_COL, DST_LOC_COL]:\n",
    "    ht[col] = ht[col].astype(str)\n",
    "\n",
    "edge_cols = [SRC_SWITCH_COL, DST_SWITCH_COL, SRC_LOC_COL, DST_LOC_COL]\n",
    "source_idx: Dict[Tuple[str, str], pd.DataFrame] = {\n",
    "    (k[0], k[1]): g[edge_cols]\n",
    "    for k, g in ht.groupby([SRC_LOC_COL, \"FEEDER_ID\"], sort=False)\n",
    "}\n",
    "\n",
    "# 2️⃣  FEEDER TRACER -----------------------------------------------------------\n",
    "\n",
    "def trace_feeder(fid: str) -> List[dict]:\n",
    "    rows: List[dict] = []\n",
    "    visited: Set[Tuple[str, str]] = set()\n",
    "\n",
    "    start = ht[(ht[SRC_SWITCH_COL] == fid) & (ht[\"FEEDER_ID\"] == fid)][edge_cols]\n",
    "    queue = start.to_records(index=False).tolist()\n",
    "\n",
    "    while queue:\n",
    "        from_sw, to_sw, src_loc, dst_loc = queue.pop(0)\n",
    "        if (from_sw, to_sw) in visited:\n",
    "            continue\n",
    "        visited.add((from_sw, to_sw))\n",
    "\n",
    "        rows.append({\n",
    "            \"FEEDER_ID\": fid,\n",
    "            \"FROM_TO\": f\"{from_sw}-{to_sw}\",\n",
    "            \"SOURCE_LOCATION\": src_loc,\n",
    "            \"DESTINATION_LOCATION\": dst_loc,\n",
    "        })\n",
    "\n",
    "        nxt = source_idx.get((dst_loc, fid))\n",
    "        if nxt is not None and not nxt.empty:\n",
    "            queue.extend(nxt.to_records(index=False).tolist())\n",
    "\n",
    "    return rows\n",
    "\n",
    "# 3️⃣  TRACE ALL FEEDERS -------------------------------------------------------\n",
    "all_edges: List[dict] = []\n",
    "feeder_ids = [str(f) for f in ht[\"FEEDER_ID\"].dropna().unique()]\n",
    "print(f\"Tracing {len(feeder_ids)} feeders …\")\n",
    "for i, fid in enumerate(feeder_ids, 1):\n",
    "    if i % 100 == 0 or i in {1, len(feeder_ids)}:\n",
    "        print(f\"  → {i}/{len(feeder_ids)}: {fid}\")\n",
    "    all_edges.extend(trace_feeder(fid))\n",
    "\n",
    "trace_df = pd.DataFrame(all_edges)\n",
    "\n",
    "# 4️⃣  LOAD ENERGY-AUDIT & AGGREGATE -----------------------------------------\n",
    "audit_path = Path(INPUT_ENERGY).expanduser()\n",
    "if not audit_path.exists():\n",
    "    raise FileNotFoundError(audit_path)\n",
    "\n",
    "print(\"\\nLoading energy-audit …\")\n",
    "audit = pd.read_csv(audit_path, low_memory=False, parse_dates=[DATE_COL])\n",
    "audit.columns = [c.upper() for c in audit.columns]\n",
    "\n",
    "audit[DATE_COL] = pd.to_datetime(audit[DATE_COL], errors=\"coerce\")\n",
    "\n",
    "audit = audit[[FUNC_LOC_COL, DATE_COL, LOAD_COL]].dropna(subset=[FUNC_LOC_COL])\n",
    "\n",
    "agg = (audit.groupby(FUNC_LOC_COL)\n",
    "           .agg(LATEST_DT_DATE=(DATE_COL, \"max\"),\n",
    "                DT_LOAD=(LOAD_COL,  \"mean\"))\n",
    "           .reset_index())\n",
    "agg[FUNC_LOC_COL] = agg[FUNC_LOC_COL].astype(str)\n",
    "\n",
    "# 5️⃣  MERGE TRACE ← AUDIT -----------------------------------------------------\n",
    "merged = (trace_df\n",
    "          .merge(agg, how=\"left\",\n",
    "                 left_on=\"DESTINATION_LOCATION\",\n",
    "                 right_on=FUNC_LOC_COL)\n",
    "          .drop(columns=[FUNC_LOC_COL]))\n",
    "\n",
    "merged[\"LATEST_DT_DATE\"] = pd.to_datetime(merged[\"LATEST_DT_DATE\"]).dt.date\n",
    "\n",
    "# 6️⃣  EXPORT ------------------------------------------------------------------\n",
    "cols = [\n",
    "    \"FEEDER_ID\", \"FROM_TO\",\n",
    "    \"SOURCE_LOCATION\", \"DESTINATION_LOCATION\",\n",
    "    \"LATEST_DT_DATE\", \"DT_LOAD\",\n",
    "]\n",
    "merged.to_excel(OUTPUT_PATH, index=False, engine=\"openpyxl\", columns=cols)\n",
    "print(f\"\\nSaved {len(merged):,} rows → {OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(merged.head())\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1859e64c",
   "metadata": {},
   "source": [
    "NEW FILE WITH RANK AND BFS BUT NOT FILTER THE 11KV FEEDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2526d605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing 1764 feeders …\n",
      "  → 1/1764: 15454\n",
      "  → 100/1764: 220AAR108\n",
      "  → 200/1764: 25326\n",
      "  → 300/1764: 11920\n",
      "  → 400/1764: 18619\n",
      "  → 500/1764: 40920\n",
      "  → 600/1764: 03717\n",
      "  → 700/1764: 31265\n",
      "  → 800/1764: 28672\n",
      "  → 900/1764: 35870\n",
      "  → 1000/1764: 29562\n",
      "  → 1100/1764: 18638\n",
      "  → 1200/1764: 40301\n",
      "  → 1300/1764: 34726\n",
      "  → 1400/1764: 220SAK12\n",
      "  → 1500/1764: 34673\n",
      "  → 1600/1764: 31981\n",
      "  → 1700/1764: 30956\n",
      "  → 1764/1764: BUSPT\n",
      "\n",
      "Loading energy-audit …\n",
      "\n",
      "Saved 17,319 rows → final_two_column_with_rank.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEEDER_ID</th>\n",
       "      <th>FROM_TO</th>\n",
       "      <th>SOURCE_LOCATION</th>\n",
       "      <th>DESTINATION_LOCATION</th>\n",
       "      <th>RANK</th>\n",
       "      <th>LATEST_DT_DATE</th>\n",
       "      <th>DT_LOAD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15454</td>\n",
       "      <td>15454-38196</td>\n",
       "      <td>1S-MH-MU-ZST-RSTN-24TH</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-1238</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>127.425882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15454</td>\n",
       "      <td>38195-34116</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-1238</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0894</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>233.628927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15454</td>\n",
       "      <td>38197-DT</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-1238</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-1238</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>127.425882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15454</td>\n",
       "      <td>34114-32764</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0894</td>\n",
       "      <td>1S-MH-MU-ZST-CL01-0860</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>134.062123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15454</td>\n",
       "      <td>34115-DT</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0894</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0894</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>233.628927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FEEDER_ID      FROM_TO         SOURCE_LOCATION    DESTINATION_LOCATION  \\\n",
       "0     15454  15454-38196  1S-MH-MU-ZST-RSTN-24TH  1S-MH-MU-ZST-CL02-1238   \n",
       "1     15454  38195-34116  1S-MH-MU-ZST-CL02-1238  1S-MH-MU-ZST-CL02-0894   \n",
       "2     15454     38197-DT  1S-MH-MU-ZST-CL02-1238  1S-MH-MU-ZST-CL02-1238   \n",
       "3     15454  34114-32764  1S-MH-MU-ZST-CL02-0894  1S-MH-MU-ZST-CL01-0860   \n",
       "4     15454     34115-DT  1S-MH-MU-ZST-CL02-0894  1S-MH-MU-ZST-CL02-0894   \n",
       "\n",
       "   RANK LATEST_DT_DATE     DT_LOAD  \n",
       "0     0     2025-04-04  127.425882  \n",
       "1     1     2025-04-04  233.628927  \n",
       "2     1     2025-04-04  127.425882  \n",
       "3     2     2025-04-04  134.062123  \n",
       "4     2     2025-04-04  233.628927  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feeder_trace_latest_audit_with_rank.py\n",
    "\"\"\"\n",
    "Workflow\n",
    "========\n",
    "1. Load HTCABLE.csv, drop unused columns, remove fully-identical rows.\n",
    "2. Trace every feeder edge‑by‑edge, annotate with RANK (distance from feeder start).\n",
    "3. Load ENERGYAUDIT.csv, for each transformer (FUNC_LOC) compute:\n",
    "   * LATEST_DT_DATE  → most‑recent SYSTEM_DATE\n",
    "   * DT_LOAD         → average MD_KVA across all rows\n",
    "4. Merge audit stats onto trace (DESTINATION_LOCATION = FUNC_LOC).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List, Set, Optional\n",
    "\n",
    "# ── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "INPUT_HT      = \"HTCABLE.csv\"\n",
    "INPUT_ENERGY  = \"/media/sagarkumar/New Volume/SAGAR/ENERGYAUDIT.csv\"\n",
    "OUTPUT_PATH   = \"final_two_column_with_rank.xlsx\"\n",
    "\n",
    "FEEDER_ID_COL  = \"FEEDERID\"\n",
    "SRC_SWITCH_COL = \"SOURCE_SWITCH_ID\"\n",
    "DST_SWITCH_COL = \"DESTINATION_SWITCH_ID\"\n",
    "SRC_LOC_COL    = \"SOURCE_SSFL\"\n",
    "DST_LOC_COL    = \"DESTINATION_SSFL\"   # ≡ FUNC_LOC in audit\n",
    "\n",
    "FUNC_LOC_COL = \"FUNC_LOC\"\n",
    "DATE_COL     = \"SYSTEM_DATE\"\n",
    "LOAD_COL     = \"MD_KVA\"\n",
    "\n",
    "REDUNDANT_COLS = [\n",
    "    \"COMMENTS\", \"GLOBALID\", \"MEASUREDLENGTH\", \"UNNAMED: 0\", \"OBJECTID\"\n",
    "]\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1️  LOAD & CLEAN HT-CABLE ---------------------------------------------------\n",
    "ht_path = Path(INPUT_HT).expanduser()\n",
    "\n",
    "ht = pd.read_csv(ht_path, low_memory=False)\n",
    "ht = ht.drop(columns=[c for c in REDUNDANT_COLS if c in ht.columns], errors=\"ignore\")\n",
    "ht = ht.drop_duplicates()  # remove fully-identical rows\n",
    "\n",
    "# helper to pull token after 2nd underscore\n",
    "def _feeder_token(val: str | int | float | None) -> Optional[str]:\n",
    "    if not isinstance(val, str):\n",
    "        val = str(val) if val is not None else \"\"\n",
    "    p = val.split(\"_\")\n",
    "    return p[2] if len(p) >= 3 else None\n",
    "\n",
    "ht[\"FEEDER_ID\"] = ht[FEEDER_ID_COL].apply(_feeder_token)\n",
    "\n",
    "for col in [SRC_SWITCH_COL, DST_SWITCH_COL, SRC_LOC_COL, DST_LOC_COL]:\n",
    "    ht[col] = ht[col].astype(str)\n",
    "\n",
    "edge_cols = [SRC_SWITCH_COL, DST_SWITCH_COL, SRC_LOC_COL, DST_LOC_COL]\n",
    "source_idx: Dict[Tuple[str, str], pd.DataFrame] = {\n",
    "    (k[0], k[1]): g[edge_cols]\n",
    "    for k, g in ht.groupby([SRC_LOC_COL, \"FEEDER_ID\"], sort=False)\n",
    "}\n",
    "\n",
    "# 2️  FEEDER TRACER (with RANK) -----------------------------------------------\n",
    "def trace_feeder(fid: str) -> List[dict]:\n",
    "    rows: List[dict] = []\n",
    "    visited: Set[Tuple[str, str]] = set()\n",
    "\n",
    "    # queue holds tuples: (edge_tuple, rank)\n",
    "    start = ht[(ht[SRC_SWITCH_COL] == fid) & (ht[\"FEEDER_ID\"] == fid)][edge_cols]\n",
    "    queue = [(row, 0) for row in start.to_records(index=False).tolist()]  # (edge, rank)\n",
    "\n",
    "    while queue:\n",
    "        (from_sw, to_sw, src_loc, dst_loc), rank = queue.pop(0)\n",
    "        if (from_sw, to_sw) in visited:\n",
    "            continue\n",
    "        visited.add((from_sw, to_sw))\n",
    "\n",
    "        rows.append({\n",
    "            \"FEEDER_ID\": fid,\n",
    "            \"FROM_TO\": f\"{from_sw}-{to_sw}\",\n",
    "            \"SOURCE_LOCATION\": src_loc,\n",
    "            \"DESTINATION_LOCATION\": dst_loc,\n",
    "            \"RANK\": rank  # Level in the feeder tree\n",
    "        })\n",
    "\n",
    "        nxt = source_idx.get((dst_loc, fid))\n",
    "        if nxt is not None and not nxt.empty:\n",
    "            # Each downstream edge gets rank+1\n",
    "            queue.extend([(row, rank + 1) for row in nxt.to_records(index=False).tolist()])\n",
    "\n",
    "    return rows\n",
    "\n",
    "# 3️  TRACE ALL FEEDERS -------------------------------------------------------\n",
    "all_edges: List[dict] = []\n",
    "feeder_ids = [str(f) for f in ht[\"FEEDER_ID\"].dropna().unique()]\n",
    "print(f\"Tracing {len(feeder_ids)} feeders …\")\n",
    "for i, fid in enumerate(feeder_ids, 1):\n",
    "    if i % 100 == 0 or i in {1, len(feeder_ids)}:\n",
    "        print(f\"  → {i}/{len(feeder_ids)}: {fid}\")\n",
    "    all_edges.extend(trace_feeder(fid))\n",
    "\n",
    "trace_df = pd.DataFrame(all_edges)\n",
    "\n",
    "# 4️ LOAD ENERGY-AUDIT & AGGREGATE -----------------------------------------\n",
    "audit_path = Path(INPUT_ENERGY).expanduser()\n",
    "if not audit_path.exists():\n",
    "    raise FileNotFoundError(audit_path)\n",
    "\n",
    "print(\"\\nLoading energy-audit …\")\n",
    "audit = pd.read_csv(audit_path, low_memory=False, parse_dates=[DATE_COL])\n",
    "audit.columns = [c.upper() for c in audit.columns]\n",
    "\n",
    "audit[DATE_COL] = pd.to_datetime(audit[DATE_COL], errors=\"coerce\")\n",
    "\n",
    "audit = audit[[FUNC_LOC_COL, DATE_COL, LOAD_COL]].dropna(subset=[FUNC_LOC_COL])\n",
    "\n",
    "agg = (audit.groupby(FUNC_LOC_COL)\n",
    "           .agg(LATEST_DT_DATE=(DATE_COL, \"max\"),\n",
    "                DT_LOAD=(LOAD_COL,  \"mean\"))\n",
    "           .reset_index())\n",
    "agg[FUNC_LOC_COL] = agg[FUNC_LOC_COL].astype(str)\n",
    "\n",
    "# 5️  MERGE TRACE ← AUDIT -----------------------------------------------------\n",
    "merged = (trace_df.merge(agg, how=\"left\",\n",
    "                 left_on=\"DESTINATION_LOCATION\",\n",
    "                 right_on=FUNC_LOC_COL).drop(columns=[FUNC_LOC_COL]))\n",
    "\n",
    "merged[\"LATEST_DT_DATE\"] = pd.to_datetime(merged[\"LATEST_DT_DATE\"]).dt.date\n",
    "\n",
    "# 6️  EXPORT ------------------------------------------------------------------\n",
    "cols = [\"FEEDER_ID\", \"FROM_TO\", \"SOURCE_LOCATION\", \"DESTINATION_LOCATION\", \"RANK\", \"LATEST_DT_DATE\", \"DT_LOAD\"]\n",
    "merged.to_excel(OUTPUT_PATH, index=False, engine=\"openpyxl\", columns=cols)\n",
    "print(f\"\\nSaved {len(merged):,} rows → {OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(merged.head())\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3175495c",
   "metadata": {},
   "source": [
    "FILTER 11KV BUT NOT INCLUDE ALL THE VALUE OF 11KV BECAUSE OF 11Kv, 11KV, 11kV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "550c5b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing 1192 feeders …\n",
      "  → 1/1192: 15451\n",
      "  → 100/1192: 43878\n",
      "  → 200/1192: 31002\n",
      "  → 300/1192: 35414\n",
      "  → 400/1192: 01514\n",
      "  → 500/1192: 30965\n",
      "  → 600/1192: 07825\n",
      "  → 700/1192: 26106\n",
      "  → 800/1192: 14414\n",
      "  → 900/1192: 03132\n",
      "  → 1000/1192: 41516\n",
      "  → 1100/1192: 41750\n",
      "  → 1192/1192: BUSPT\n",
      "\n",
      "Loading energy-audit …\n",
      "\n",
      "Saved 14,045 rows → final_two_column_with_rank_11.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEEDER_ID</th>\n",
       "      <th>FROM_TO</th>\n",
       "      <th>SOURCE_LOCATION</th>\n",
       "      <th>DESTINATION_LOCATION</th>\n",
       "      <th>RANK</th>\n",
       "      <th>LATEST_DT_DATE</th>\n",
       "      <th>DT_LOAD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15451</td>\n",
       "      <td>15451-31551</td>\n",
       "      <td>1S-MH-MU-ZST-RSTN-24TH</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0885</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>372.357927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15451</td>\n",
       "      <td>31553-39603</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0885</td>\n",
       "      <td>1S-MH-MU-ZST-RSTN-KHAR</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>102.345161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15451</td>\n",
       "      <td>39786-39785</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0885</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0414</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>442.128986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15451</td>\n",
       "      <td>31554-DT</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0885</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0885</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>372.357927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15451</td>\n",
       "      <td>15770-29785</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0414</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0198</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>536.223301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FEEDER_ID      FROM_TO         SOURCE_LOCATION    DESTINATION_LOCATION  \\\n",
       "0     15451  15451-31551  1S-MH-MU-ZST-RSTN-24TH  1S-MH-MU-ZST-CL02-0885   \n",
       "1     15451  31553-39603  1S-MH-MU-ZST-CL02-0885  1S-MH-MU-ZST-RSTN-KHAR   \n",
       "2     15451  39786-39785  1S-MH-MU-ZST-CL02-0885  1S-MH-MU-ZST-CL02-0414   \n",
       "3     15451     31554-DT  1S-MH-MU-ZST-CL02-0885  1S-MH-MU-ZST-CL02-0885   \n",
       "4     15451  15770-29785  1S-MH-MU-ZST-CL02-0414  1S-MH-MU-ZST-CL02-0198   \n",
       "\n",
       "   RANK LATEST_DT_DATE     DT_LOAD  \n",
       "0     0     2025-04-04  372.357927  \n",
       "1     1     2025-04-04  102.345161  \n",
       "2     1     2025-04-04  442.128986  \n",
       "3     1     2025-04-04  372.357927  \n",
       "4     2     2025-04-04  536.223301  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feeder_trace_latest_audit_with_rank.py\n",
    "\"\"\"\n",
    "Workflow\n",
    "========\n",
    "1. Load HTCABLE.csv, drop unused columns, remove fully-identical rows.\n",
    "2. Trace every feeder edge‑by‑edge, annotate with RANK (distance from feeder start).\n",
    "3. Load ENERGYAUDIT.csv, for each transformer (FUNC_LOC) compute:\n",
    "   * LATEST_DT_DATE  → most‑recent SYSTEM_DATE\n",
    "   * DT_LOAD         → average MD_KVA across all rows\n",
    "4. Merge audit stats onto trace (DESTINATION_LOCATION = FUNC_LOC).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List, Set, Optional\n",
    "\n",
    "# ── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "INPUT_HT      = \"HTCABLE.csv\"\n",
    "INPUT_ENERGY  = \"/media/sagarkumar/New Volume/SAGAR/ENERGYAUDIT.csv\"\n",
    "OUTPUT_PATH   = \"final_two_column_with_rank_11.xlsx\"\n",
    "\n",
    "FEEDER_ID_COL  = \"FEEDERID\"\n",
    "SRC_SWITCH_COL = \"SOURCE_SWITCH_ID\"\n",
    "DST_SWITCH_COL = \"DESTINATION_SWITCH_ID\"\n",
    "SRC_LOC_COL    = \"SOURCE_SSFL\"\n",
    "DST_LOC_COL    = \"DESTINATION_SSFL\"   # ≡ FUNC_LOC in audit\n",
    "\n",
    "FUNC_LOC_COL = \"FUNC_LOC\"\n",
    "DATE_COL     = \"SYSTEM_DATE\"\n",
    "LOAD_COL     = \"MD_KVA\"\n",
    "\n",
    "REDUNDANT_COLS = [\n",
    "    \"COMMENTS\", \"GLOBALID\", \"MEASUREDLENGTH\", \"UNNAMED: 0\", \"OBJECTID\"\n",
    "]\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1️  LOAD & CLEAN HT-CABLE ---------------------------------------------------\n",
    "ht_path = Path(INPUT_HT).expanduser()\n",
    "\n",
    "ht = pd.read_csv(ht_path, low_memory=False)\n",
    "ht = ht.drop(columns=[c for c in REDUNDANT_COLS if c in ht.columns], errors=\"ignore\")\n",
    "ht = ht.drop_duplicates()  # remove fully-identical rows\n",
    "\n",
    "# helper to pull token after 2nd underscore\n",
    "def _feeder_token(val: str | int | float | None) -> Optional[str]:\n",
    "    if not isinstance(val, str):\n",
    "        val = str(val) if val is not None else \"\"\n",
    "    p = val.split(\"_\")\n",
    "    return p[2] if len(p) >= 3 and p[1] == '11kV' else None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def extract_feeder_id(value: str | int | float | None) -> Optional[str]:\n",
    "#     \"\"\"\n",
    "#     Return the token after the 2nd underscore only if the middle token is '11kV'.\n",
    "#     Example: 'AMBVLI_11kV_19556' ➜ '19556'\n",
    "#     \"\"\"\n",
    "#     if not isinstance(value, str):\n",
    "#         value = str(value) if value is not None else \"\"\n",
    "#     parts = value.split(\"_\")\n",
    "#     if len(parts) >= 3 :\n",
    "#         return parts[2]\n",
    "#     return None\n",
    "\n",
    "\n",
    "\n",
    "ht[\"FEEDER_ID\"] = ht[FEEDER_ID_COL].apply(_feeder_token)\n",
    "\n",
    "for col in [SRC_SWITCH_COL, DST_SWITCH_COL, SRC_LOC_COL, DST_LOC_COL]:\n",
    "    ht[col] = ht[col].astype(str)\n",
    "\n",
    "edge_cols = [SRC_SWITCH_COL, DST_SWITCH_COL, SRC_LOC_COL, DST_LOC_COL]\n",
    "source_idx: Dict[Tuple[str, str], pd.DataFrame] = {\n",
    "    (k[0], k[1]): g[edge_cols]\n",
    "    for k, g in ht.groupby([SRC_LOC_COL, \"FEEDER_ID\"], sort=False)\n",
    "}\n",
    "\n",
    "# 2️  FEEDER TRACER (with RANK) -----------------------------------------------\n",
    "def trace_feeder(fid: str) -> List[dict]:\n",
    "    rows: List[dict] = []\n",
    "    visited: Set[Tuple[str, str]] = set()\n",
    "\n",
    "    # queue holds tuples: (edge_tuple, rank)\n",
    "    start = ht[(ht[SRC_SWITCH_COL] == fid) & (ht[\"FEEDER_ID\"] == fid)][edge_cols]\n",
    "    queue = [(row, 0) for row in start.to_records(index=False).tolist()]  # (edge, rank)\n",
    "\n",
    "    while queue:\n",
    "        (from_sw, to_sw, src_loc, dst_loc), rank = queue.pop(0)\n",
    "        if (from_sw, to_sw) in visited:\n",
    "            continue\n",
    "        visited.add((from_sw, to_sw))\n",
    "\n",
    "        rows.append({\n",
    "            \"FEEDER_ID\": fid,\n",
    "            \"FROM_TO\": f\"{from_sw}-{to_sw}\",\n",
    "            \"SOURCE_LOCATION\": src_loc,\n",
    "            \"DESTINATION_LOCATION\": dst_loc,\n",
    "            \"RANK\": rank  # Level in the feeder tree\n",
    "        })\n",
    "\n",
    "        nxt = source_idx.get((dst_loc, fid))\n",
    "        if nxt is not None and not nxt.empty:\n",
    "            # Each downstream edge gets rank+1\n",
    "            queue.extend([(row, rank + 1) for row in nxt.to_records(index=False).tolist()])\n",
    "\n",
    "    return rows\n",
    "\n",
    "# 3️  TRACE ALL FEEDERS -------------------------------------------------------\n",
    "all_edges: List[dict] = []\n",
    "feeder_ids = [str(f) for f in ht[\"FEEDER_ID\"].dropna().unique()]\n",
    "print(f\"Tracing {len(feeder_ids)} feeders …\")\n",
    "for i, fid in enumerate(feeder_ids, 1):\n",
    "    if i % 100 == 0 or i in {1, len(feeder_ids)}:\n",
    "        print(f\"  → {i}/{len(feeder_ids)}: {fid}\")\n",
    "    all_edges.extend(trace_feeder(fid))\n",
    "\n",
    "trace_df = pd.DataFrame(all_edges)\n",
    "\n",
    "# 4️ LOAD ENERGY-AUDIT & AGGREGATE -----------------------------------------\n",
    "audit_path = Path(INPUT_ENERGY).expanduser()\n",
    "if not audit_path.exists():\n",
    "    raise FileNotFoundError(audit_path)\n",
    "\n",
    "print(\"\\nLoading energy-audit …\")\n",
    "audit = pd.read_csv(audit_path, low_memory=False, parse_dates=[DATE_COL])\n",
    "audit.columns = [c.upper() for c in audit.columns]\n",
    "\n",
    "audit[DATE_COL] = pd.to_datetime(audit[DATE_COL], errors=\"coerce\")\n",
    "\n",
    "audit = audit[[FUNC_LOC_COL, DATE_COL, LOAD_COL]].dropna(subset=[FUNC_LOC_COL])\n",
    "\n",
    "agg = (audit.groupby(FUNC_LOC_COL)\n",
    "           .agg(LATEST_DT_DATE=(DATE_COL, \"max\"),\n",
    "                DT_LOAD=(LOAD_COL,  \"mean\"))\n",
    "           .reset_index())\n",
    "agg[FUNC_LOC_COL] = agg[FUNC_LOC_COL].astype(str)\n",
    "\n",
    "# 5️  MERGE TRACE ← AUDIT -----------------------------------------------------\n",
    "merged = (trace_df.merge(agg, how=\"left\",\n",
    "                 left_on=\"DESTINATION_LOCATION\",\n",
    "                 right_on=FUNC_LOC_COL).drop(columns=[FUNC_LOC_COL]))\n",
    "\n",
    "merged[\"LATEST_DT_DATE\"] = pd.to_datetime(merged[\"LATEST_DT_DATE\"]).dt.date\n",
    "\n",
    "# 6️  EXPORT ------------------------------------------------------------------\n",
    "cols = [\"FEEDER_ID\", \"FROM_TO\", \"SOURCE_LOCATION\", \"DESTINATION_LOCATION\", \"RANK\", \"LATEST_DT_DATE\", \"DT_LOAD\"]\n",
    "merged.to_excel(OUTPUT_PATH, index=False, engine=\"openpyxl\", columns=cols)\n",
    "print(f\"\\nSaved {len(merged):,} rows → {OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(merged.head())\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7e400d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing 1424 feeders …\n",
      "  → 1/1424: 15454\n",
      "  → 100/1424: 41897\n",
      "  → 200/1424: 28223\n",
      "  → 300/1424: 39624\n",
      "  → 400/1424: 41709\n",
      "  → 500/1424: 31267\n",
      "  → 600/1424: 28674\n",
      "  → 700/1424: 35873\n",
      "  → 800/1424: 30135\n",
      "  → 900/1424: 18093\n",
      "  → 1000/1424: 30031\n",
      "  → 1100/1424: 03101\n",
      "  → 1200/1424: 35038\n",
      "  → 1300/1424: 30886\n",
      "  → 1400/1424: 19090\n",
      "  → 1424/1424: BUSPT\n",
      "\n",
      "Loading energy-audit …\n",
      "\n",
      "Saved 16,541 rows  final_two_column_with_rank_11_full.csv\n"
     ]
    }
   ],
   "source": [
    "# feeder_trace_latest_audit_with_rank_updated.py\n",
    "\"\"\"\n",
    "Workflow\n",
    "========\n",
    "1. Load HTCABLE.csv, drop unused columns, remove fully‑identical rows.\n",
    "2. Trace every feeder edge‑by‑edge, annotate with RANK (distance from feeder start).\n",
    "3. Load ENERGYAUDIT.csv, for each transformer (FUNC_LOC) compute:\n",
    "   * LATEST_DT_DATE  → most‑recent SYSTEM_DATE\n",
    "   * DT_LOAD         → average MD_KVA across all rows\n",
    "4. Merge audit stats onto trace (DESTINATION_LOCATION = FUNC_LOC).\n",
    "5. Create LOCATION column identical to DESTINATION_LOCATION for easier downstream joins.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List, Set, Optional\n",
    "\n",
    "# ── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "INPUT_HT      = \"HTCABLE.csv\"\n",
    "INPUT_ENERGY  = \"/media/sagarkumar/New Volume/SAGAR/ENERGYAUDIT.csv\"\n",
    "OUTPUT_PATH   = \"final_two_column_with_rank_11_full.csv\"\n",
    "\n",
    "FEEDER_ID_COL  = \"FEEDERID\"\n",
    "SRC_SWITCH_COL = \"SOURCE_SWITCH_ID\"\n",
    "DST_SWITCH_COL = \"DESTINATION_SWITCH_ID\"\n",
    "SRC_LOC_COL    = \"SOURCE_SSFL\"\n",
    "DST_LOC_COL    = \"DESTINATION_SSFL\"   # ≡ FUNC_LOC in audit\n",
    "\n",
    "FUNC_LOC_COL = \"FUNC_LOC\"\n",
    "DATE_COL     = \"SYSTEM_DATE\"\n",
    "LOAD_COL     = \"MD_KVA\"\n",
    "\n",
    "REDUNDANT_COLS = [\n",
    "    \"COMMENTS\", \"GLOBALID\", \"MEASUREDLENGTH\", \"UNNAMED: 0\", \"OBJECTID\"\n",
    "]\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1️  LOAD & CLEAN HT-CABLE ---------------------------------------------------\n",
    "ht_path = Path(INPUT_HT).expanduser()\n",
    "\n",
    "ht = pd.read_csv(ht_path, low_memory=False)\n",
    "ht = ht.drop(columns=[c for c in REDUNDANT_COLS if c in ht.columns], errors=\"ignore\")\n",
    "ht = ht.drop_duplicates()  # remove fully-identical rows\n",
    "\n",
    "# helper to pull token after 2nd underscore\n",
    "def _feeder_token(val: str | int | float | None) -> Optional[str]:\n",
    "    if not isinstance(val, str):\n",
    "        val = str(val) if val is not None else \"\"\n",
    "    p = val.split(\"_\")\n",
    "    return p[2] if len(p) >= 3 and (p[1] == '11kV' or p[1]=='11Kv' or p[1]=='11KV') else None\n",
    "\n",
    "ht[\"FEEDER_ID\"] = ht[FEEDER_ID_COL].apply(_feeder_token)\n",
    "\n",
    "for col in [SRC_SWITCH_COL, DST_SWITCH_COL, SRC_LOC_COL, DST_LOC_COL]:\n",
    "    ht[col] = ht[col].astype(str)\n",
    "\n",
    "edge_cols = [SRC_SWITCH_COL, DST_SWITCH_COL, SRC_LOC_COL, DST_LOC_COL]\n",
    "source_idx: Dict[Tuple[str, str], pd.DataFrame] = {\n",
    "    (k[0], k[1]): g[edge_cols]\n",
    "    for k, g in ht.groupby([SRC_LOC_COL, \"FEEDER_ID\"], sort=False)\n",
    "}\n",
    "\n",
    "# 2️  FEEDER TRACER (with RANK) -----------------------------------------------\n",
    "def trace_feeder(fid: str) -> List[dict]:\n",
    "    rows: List[dict] = []\n",
    "    visited: Set[Tuple[str, str]] = set()\n",
    "\n",
    "    # queue holds tuples: (edge_tuple, rank)\n",
    "    start = ht[(ht[SRC_SWITCH_COL] == fid) & (ht[\"FEEDER_ID\"] == fid)][edge_cols]\n",
    "    queue = [(row, 0) for row in start.to_records(index=False).tolist()]  # (edge, rank)\n",
    "\n",
    "    while queue:\n",
    "        (from_sw, to_sw, src_loc, dst_loc), rank = queue.pop(0)\n",
    "        if (from_sw, to_sw) in visited:\n",
    "            continue\n",
    "        visited.add((from_sw, to_sw))\n",
    "\n",
    "        rows.append({\n",
    "            \"FEEDER_ID\": fid,\n",
    "            \"FROM_TO\": f\"{from_sw}-{to_sw}\",\n",
    "            \"SOURCE_LOCATION\": src_loc,\n",
    "            \"DESTINATION_LOCATION\": dst_loc,\n",
    "            \"RANK\": rank  # Level in the feeder tree\n",
    "        })\n",
    "\n",
    "        nxt = source_idx.get((dst_loc, fid))\n",
    "        if nxt is not None and not nxt.empty:\n",
    "            # Each downstream edge gets rank+1\n",
    "            queue.extend([(row, rank + 1) for row in nxt.to_records(index=False).tolist()])\n",
    "\n",
    "    return rows\n",
    "\n",
    "# 3️  TRACE ALL FEEDERS -------------------------------------------------------\n",
    "all_edges: List[dict] = []\n",
    "feeder_ids = [str(f) for f in ht[\"FEEDER_ID\"].dropna().unique()]\n",
    "print(f\"Tracing {len(feeder_ids)} feeders …\")\n",
    "for i, fid in enumerate(feeder_ids, 1):\n",
    "    if i % 100 == 0 or i in {1, len(feeder_ids)}:\n",
    "        print(f\"  → {i}/{len(feeder_ids)}: {fid}\")\n",
    "    all_edges.extend(trace_feeder(fid))\n",
    "\n",
    "trace_df = pd.DataFrame(all_edges)\n",
    "\n",
    "# 4️ LOAD ENERGY-AUDIT & AGGREGATE -----------------------------------------\n",
    "audit_path = Path(INPUT_ENERGY).expanduser()\n",
    "if not audit_path.exists():\n",
    "    raise FileNotFoundError(audit_path)\n",
    "\n",
    "print(\"\\nLoading energy-audit …\")\n",
    "audit = pd.read_csv(audit_path, low_memory=False, parse_dates=[DATE_COL])\n",
    "audit.columns = [c.upper() for c in audit.columns]\n",
    "\n",
    "audit[DATE_COL] = pd.to_datetime(audit[DATE_COL], errors=\"coerce\")\n",
    "\n",
    "audit = audit[[FUNC_LOC_COL, DATE_COL, LOAD_COL]].dropna(subset=[FUNC_LOC_COL])\n",
    "\n",
    "agg = (audit.groupby(FUNC_LOC_COL)\n",
    "           .agg(LATEST_DT_DATE=(DATE_COL, \"max\"),\n",
    "                DT_LOAD=(LOAD_COL,  \"mean\"))\n",
    "           .reset_index())\n",
    "agg[FUNC_LOC_COL] = agg[FUNC_LOC_COL].astype(str)\n",
    "\n",
    "# 5️  MERGE TRACE ← AUDIT -----------------------------------------------------\n",
    "merged = (trace_df.merge(agg, how=\"left\",\n",
    "                 left_on=\"DESTINATION_LOCATION\",\n",
    "                 right_on=FUNC_LOC_COL).drop(columns=[FUNC_LOC_COL]))\n",
    "\n",
    "merged[\"LATEST_DT_DATE\"] = pd.to_datetime(merged[\"LATEST_DT_DATE\"]).dt.date\n",
    "\n",
    "# NEW COLUMN: LOCATION identical to DESTINATION_LOCATION\n",
    "merged[\"LOCATION\"] = merged[\"DESTINATION_LOCATION\"]\n",
    "\n",
    "# 6️  EXPORT ------------------------------------------------------------------\n",
    "cols = [\n",
    "    \"FEEDER_ID\", \"FROM_TO\", \"SOURCE_LOCATION\", \"DESTINATION_LOCATION\", \"LOCATION\",\n",
    "    \"RANK\", \"LATEST_DT_DATE\", \"DT_LOAD\"\n",
    "]\n",
    "merged.to_csv(OUTPUT_PATH, index=False, columns=cols)\n",
    "print(f\"\\nSaved {len(merged):,} rows  {OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "        from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bc1cbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique mid tokens from FEEDER_ID_COL: [None '11KV' '11kV' 'GOR0552' 'BOR00552' 'BOR00952' 'SAK1152' 'SAK0452'\n",
      " '33KV' '33kV' '22kV' 'GOI00152' '11Kv' 'REACTOR' '22KV' 'VER00152'\n",
      " 'GHO01452' 'GHO01652' 'AAR01352' 'AAR01552' 'GOI00352' 'GOI00852'\n",
      " 'CHE00952' 'CHE00152' 'CHE00552' 'CHE00652' 'DHN00352' '40973' '40974'\n",
      " '40976' '40977' '40978' '40980' '33360' 'AAR00452' 'AAR00552' 'AAR00652'\n",
      " 'AAR00852' 'AAR01152' 'BOR00152' 'BOR00752' 'CHE00252' 'CHE00852'\n",
      " 'CHE01052' 'GHD00752' 'GHD00952' 'GHD1052' 'GOI00252' 'GOI00752'\n",
      " 'GOR00252' 'GOR00752' 'GOR01052' 'SAK00752' 'SAK00952' 'SAK01252'\n",
      " 'VER00652' 'VER00752' 'VER00852' 'VER01052' 'VER01352' '33Kv' 'BOR00352'\n",
      " 'BOR01652' 'VER01452' 'VER01552' 'SAK00652' 'SAK00352' 'GOI00652'\n",
      " 'GOR00452' 'GOR00152']\n"
     ]
    }
   ],
   "source": [
    "def extract_mid_token(val: str | int | float | None) -> Optional[str]:\n",
    "    if not isinstance(val, str):\n",
    "        val = str(val) if val is not None else \"\"\n",
    "    p = val.split(\"_\")\n",
    "    return p[1] if len(p) >= 2 else None\n",
    "\n",
    "unique_mid_tokens = ht[FEEDER_ID_COL].apply(extract_mid_token).unique()\n",
    "print(\"Unique mid tokens from FEEDER_ID_COL:\", unique_mid_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "360d1f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique mid tokens from FEEDER_ID_COL: [None '24THRD' 'AAR01752' 'AAR01852' 'AAR01952' 'AAR1252' 'AAR1452' 'ACRO'\n",
      " 'AMBVLI' 'ANDHRI' 'ANIK' 'ARY220' 'ARY' 'BANDRA' 'BBLWDI' 'BHAVANS'\n",
      " 'BHAYW' 'BHYNDR' 'BKC' 'BNDRTE' 'BORIVLI' 'BORVLI' 'BOR' 'CAMA' 'CHAKALA'\n",
      " 'CHBNDR' 'CHDNGR' 'CHDVLI' 'CHE' 'CHMBUR' 'CHMBU' 'CHMB' 'CHUNA' 'CHVALI'\n",
      " 'CPWDMAREC' 'DAHICHNKA' 'DAHISRW' 'DAHISR' 'DEVIDAS' 'DHA' 'DINDO'\n",
      " 'ERANGL' 'ESIC' 'GHO00452' 'GHOD' 'GKLDHM' 'GNSHNG' 'GODREJBKC'\n",
      " 'GOI00452' 'GOI220' 'GOR220' 'GORAI' 'GOREG' 'HCC' 'HINGWALA' 'HIRANANDA'\n",
      " 'HIRANA' 'HULL' 'JANKALYAN' 'JBNGR' 'JUHUN' 'JUHU' 'KADAMWADI' 'KALANR'\n",
      " 'KALINA' 'KALPATARU' 'KANA' 'KANDI' 'KHAR' 'KIE' 'KOHINR' 'KURLA' 'KURL'\n",
      " 'LKHWLA' 'MAHANANDA' 'MAHULSRA' 'MAKERS' 'MALAD' 'MAL' 'MANK' 'MAROL'\n",
      " 'MBI00152' 'MBI00652' 'MBO00152' 'MBR00152' 'MBR00252' 'MBR00552'\n",
      " 'MBR00752' 'MBR00852' 'MGHWDI' 'MHADAMANK' 'MHADASAH' 'MHADSAH' 'MIDC'\n",
      " 'MINDSP' 'MIRA' 'MMRDA' 'MNR00152' 'MTR00152' 'MTR00252' 'MTR00352'\n",
      " 'MVR00152' 'NAHAR SHAKTI DSS' 'NAHAR' 'NATPAR' 'NESCO' 'NETMAGICDC9NO1'\n",
      " 'NIRLON' 'OMKAR' 'OSHIWARA' 'PALI' 'PALMCT' 'POISAR' 'POWAI' 'PT220'\n",
      " 'RAHEJA' 'RAVI' 'RIL' 'RNARYL' 'ROYAL PAL' 'RUNWAL' 'SADG' 'SAHARPLZ'\n",
      " 'SAHAR' 'SAKI' 'SAK' 'SAMBHNGR' 'SARVODAY' 'SCRUZ' 'SEEPZ' 'SHANTIST'\n",
      " 'SHIMP' 'SHRADDHAN' 'SHVNGR ' 'SIDNGR ' 'SRSWTI' 'SWAN' 'SWMSMTNGR'\n",
      " 'TBR00252' 'TGRNGR ' 'TGRNGR' 'TIMES' 'TLKNGR ' 'TPCVER0152' 'TPCVER0252'\n",
      " 'TSK00152' 'TSK00652' 'VAZIRA' 'VER00352' 'VER01152' 'VER01252' 'VER220'\n",
      " 'VER' 'VIHAR ' 'VIHAR' 'VIKHR ' 'VIK' 'VINA' 'VPARLE' 'VSV220']\n"
     ]
    }
   ],
   "source": [
    "def extract_mid_token(val: str | int | float | None) -> Optional[str]:\n",
    "    if not isinstance(val, str):\n",
    "        val = str(val) if val is not None else \"\"\n",
    "    p = val.split(\"_\")\n",
    "    return p[0] if len(p) >= 2 else None\n",
    "\n",
    "unique_mid_tokens = ht[FEEDER_ID_COL].apply(extract_mid_token).unique()\n",
    "print(\"Unique mid tokens from FEEDER_ID_COL:\", unique_mid_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71c3bf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique mid tokens from FEEDER_ID_COL: [None '15454' '15451' ... '40877' '40878' 'BUSPT']\n"
     ]
    }
   ],
   "source": [
    "def extract_mid_token(val: str | int | float | None) -> Optional[str]:\n",
    "    if not isinstance(val, str):\n",
    "        val = str(val) if val is not None else \"\"\n",
    "    p = val.split(\"_\")\n",
    "    return p[2] if len(p) >= 3 else None\n",
    "\n",
    "unique_mid_tokens = ht[FEEDER_ID_COL].apply(extract_mid_token).unique()\n",
    "print(\"Unique mid tokens from FEEDER_ID_COL:\", unique_mid_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8d77ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "149c89be",
   "metadata": {},
   "source": [
    "REMOVE DT AND USE 11KV VOATGE FILE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cd34aa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing 1424 feeders …\n",
      "  → 1/1424: 15454\n",
      "  → 100/1424: 41897\n",
      "  → 200/1424: 28223\n",
      "  → 300/1424: 39624\n",
      "  → 400/1424: 41709\n",
      "  → 500/1424: 31267\n",
      "  → 600/1424: 28674\n",
      "  → 700/1424: 35873\n",
      "  → 800/1424: 30135\n",
      "  → 900/1424: 18093\n",
      "  → 1000/1424: 30031\n",
      "  → 1100/1424: 03101\n",
      "  → 1200/1424: 35038\n",
      "  → 1300/1424: 30886\n",
      "  → 1400/1424: 19090\n",
      "  → 1424/1424: BUSPT\n",
      "\n",
      "Loading energy-audit …\n",
      "\n",
      "Saved 7,333 rows → /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/final_two_column_with_rank_11_withoutDT_another_feeder.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEEDER_ID</th>\n",
       "      <th>FROM_TO</th>\n",
       "      <th>SOURCE_LOCATION</th>\n",
       "      <th>DESTINATION_LOCATION</th>\n",
       "      <th>RANK</th>\n",
       "      <th>LATEST_DT_DATE</th>\n",
       "      <th>DT_LOAD</th>\n",
       "      <th>LOCATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15454</td>\n",
       "      <td>15454-38196</td>\n",
       "      <td>1S-MH-MU-ZST-RSTN-24TH</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-1238</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>127.425882</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-1238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15454</td>\n",
       "      <td>38195-34116</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-1238</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0894</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>233.628927</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15454</td>\n",
       "      <td>34114-32764</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0894</td>\n",
       "      <td>1S-MH-MU-ZST-CL01-0860</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>134.062123</td>\n",
       "      <td>1S-MH-MU-ZST-CL01-0860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15454</td>\n",
       "      <td>32766-31556</td>\n",
       "      <td>1S-MH-MU-ZST-CL01-0860</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0815</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>245.587090</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15454</td>\n",
       "      <td>31555-4467</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0815</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0054</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>364.485990</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FEEDER_ID      FROM_TO         SOURCE_LOCATION    DESTINATION_LOCATION  \\\n",
       "0     15454  15454-38196  1S-MH-MU-ZST-RSTN-24TH  1S-MH-MU-ZST-CL02-1238   \n",
       "1     15454  38195-34116  1S-MH-MU-ZST-CL02-1238  1S-MH-MU-ZST-CL02-0894   \n",
       "3     15454  34114-32764  1S-MH-MU-ZST-CL02-0894  1S-MH-MU-ZST-CL01-0860   \n",
       "5     15454  32766-31556  1S-MH-MU-ZST-CL01-0860  1S-MH-MU-ZST-CL02-0815   \n",
       "7     15454   31555-4467  1S-MH-MU-ZST-CL02-0815  1S-MH-MU-ZST-CL02-0054   \n",
       "\n",
       "   RANK LATEST_DT_DATE     DT_LOAD                LOCATION  \n",
       "0     0     2025-04-04  127.425882  1S-MH-MU-ZST-CL02-1238  \n",
       "1     1     2025-04-04  233.628927  1S-MH-MU-ZST-CL02-0894  \n",
       "3     2     2025-04-04  134.062123  1S-MH-MU-ZST-CL01-0860  \n",
       "5     3     2025-04-04  245.587090  1S-MH-MU-ZST-CL02-0815  \n",
       "7     4     2025-04-04  364.485990  1S-MH-MU-ZST-CL02-0054  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feeder_trace_latest_audit_with_rank_updated.py\n",
    "\"\"\"\n",
    "Workflow\n",
    "========\n",
    "1. Load HTCABLE.csv, drop unused columns, remove fully‑identical rows.\n",
    "2. Trace every feeder edge‑by‑edge, annotate with RANK (distance from feeder start).\n",
    "3. Load ENERGYAUDIT.csv, for each transformer (FUNC_LOC) compute:\n",
    "   * LATEST_DT_DATE  → most‑recent SYSTEM_DATE\n",
    "   * DT_LOAD         → average MD_KVA across all rows\n",
    "4. Merge audit stats onto trace (DESTINATION_LOCATION = FUNC_LOC).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List, Set, Optional\n",
    "import re\n",
    "\n",
    "# ── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "INPUT_HT = \"/media/sagark24/New Volume/MERGE CDIS/2-Year-data/HTCABLE_Clean.csv\"\n",
    "INPUT_ENERGY = \"/media/sagark24/New Volume/MERGE CDIS/2-Year-data/ENERGYAUDIT.csv\"\n",
    "# FEEDER_LIST_PATH = \"/media/sagark24/New Volume/MERGE CDIS/2-Year-data/FEEDERDETAILS.csv\"\n",
    "OUTPUT_PATH = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/final_two_column_with_rank_11_withoutDT_another_feeder.csv\"\n",
    "\n",
    "FEEDER_ID_COL  = \"FEEDERID\"\n",
    "SRC_SWITCH_COL = \"SOURCE_SWITCH_ID\"\n",
    "DST_SWITCH_COL = \"DESTINATION_SWITCH_ID\"\n",
    "SRC_LOC_COL    = \"SOURCE_SSFL\"\n",
    "DST_LOC_COL    = \"DESTINATION_SSFL\"   # ≡ FUNC_LOC in audit\n",
    "\n",
    "FUNC_LOC_COL = \"FUNC_LOC\"\n",
    "DATE_COL     = \"SYSTEM_DATE\"\n",
    "LOAD_COL     = \"MD_KVA\"\n",
    "\n",
    "REDUNDANT_COLS = [\n",
    "\n",
    "]\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1️  LOAD & CLEAN HT-CABLE ---------------------------------------------------\n",
    "ht_path = Path(INPUT_HT).expanduser()\n",
    "\n",
    "ht = pd.read_csv(ht_path, low_memory=False)\n",
    "ht = ht.drop(columns=[c for c in REDUNDANT_COLS if c in ht.columns], errors=\"ignore\")\n",
    "ht = ht.drop_duplicates()  # remove fully-identical rows\n",
    "\n",
    "# helper to pull token after 2nd underscore\n",
    "def _feeder_token(val: str | int | float | None) -> Optional[str]:\n",
    "    if not isinstance(val, str):\n",
    "        val = str(val) if val is not None else \"\"\n",
    "    p = val.split(\"_\")\n",
    "    return p[2] if len(p) >= 3 and (p[1] == '11kV' or p[1]=='11Kv' or p[1]=='11KV')  else None\n",
    "\n",
    "ht[\"FEEDER_ID\"] = ht[FEEDER_ID_COL].apply(_feeder_token)\n",
    "\n",
    "for col in [SRC_SWITCH_COL, DST_SWITCH_COL, SRC_LOC_COL, DST_LOC_COL]:\n",
    "    ht[col] = ht[col].astype(str)\n",
    "\n",
    "edge_cols = [SRC_SWITCH_COL, DST_SWITCH_COL, SRC_LOC_COL, DST_LOC_COL]\n",
    "source_idx: Dict[Tuple[str, str], pd.DataFrame] = {\n",
    "    (k[0], k[1]): g[edge_cols]\n",
    "    for k, g in ht.groupby([SRC_LOC_COL, \"FEEDER_ID\"], sort=False)\n",
    "}\n",
    "\n",
    "# 2️  FEEDER TRACER (with RANK) -----------------------------------------------\n",
    "def trace_feeder(fid: str) -> List[dict]:\n",
    "    rows: List[dict] = []\n",
    "    visited: Set[Tuple[str, str]] = set()\n",
    "\n",
    "    # queue holds tuples: (edge_tuple, rank)\n",
    "    start = ht[(ht[SRC_SWITCH_COL] == fid) & (ht[\"FEEDER_ID\"] == fid)][edge_cols]\n",
    "    queue = [(row, 0) for row in start.to_records(index=False).tolist()]  # (edge, rank)\n",
    "\n",
    "    while queue:\n",
    "        (from_sw, to_sw, src_loc, dst_loc), rank = queue.pop(0)\n",
    "        if (from_sw, to_sw) in visited:\n",
    "            continue\n",
    "        visited.add((from_sw, to_sw))\n",
    "\n",
    "        rows.append({\n",
    "            \"FEEDER_ID\": fid,\n",
    "            \"FROM_TO\": f\"{from_sw}-{to_sw}\",\n",
    "            \"SOURCE_LOCATION\": src_loc,\n",
    "            \"DESTINATION_LOCATION\": dst_loc,\n",
    "            \"RANK\": rank  # Level in the feeder tree\n",
    "        })\n",
    "\n",
    "        nxt = source_idx.get((dst_loc, fid))\n",
    "        if nxt is not None and not nxt.empty:\n",
    "            # Each downstream edge gets rank+1\n",
    "            queue.extend([(row, rank + 1) for row in nxt.to_records(index=False).tolist()])\n",
    "\n",
    "    return rows\n",
    "\n",
    "# 3️  TRACE ALL FEEDERS -------------------------------------------------------\n",
    "all_edges: List[dict] = []\n",
    "feeder_ids = [str(f) for f in ht[\"FEEDER_ID\"].dropna().unique()]\n",
    "print(f\"Tracing {len(feeder_ids)} feeders …\")\n",
    "for i, fid in enumerate(feeder_ids, 1):\n",
    "    if i % 100 == 0 or i in {1, len(feeder_ids)}:\n",
    "        print(f\"  → {i}/{len(feeder_ids)}: {fid}\")\n",
    "    all_edges.extend(trace_feeder(fid))\n",
    "\n",
    "trace_df = pd.DataFrame(all_edges)\n",
    "\n",
    "# 4️ LOAD ENERGY-AUDIT & AGGREGATE -----------------------------------------\n",
    "audit_path = Path(INPUT_ENERGY).expanduser()\n",
    "if not audit_path.exists():\n",
    "    raise FileNotFoundError(audit_path)\n",
    "\n",
    "print(\"\\nLoading energy-audit …\")\n",
    "audit = pd.read_csv(audit_path, low_memory=False, parse_dates=[DATE_COL])\n",
    "audit.columns = [c.upper() for c in audit.columns]\n",
    "\n",
    "audit[DATE_COL] = pd.to_datetime(audit[DATE_COL], errors=\"coerce\")\n",
    "\n",
    "audit = audit[[FUNC_LOC_COL, DATE_COL, LOAD_COL]].dropna(subset=[FUNC_LOC_COL])\n",
    "\n",
    "agg = (audit.groupby(FUNC_LOC_COL)\n",
    "           .agg(LATEST_DT_DATE=(DATE_COL, \"max\"),\n",
    "                DT_LOAD=(LOAD_COL,  \"mean\"))\n",
    "           .reset_index())\n",
    "agg[FUNC_LOC_COL] = agg[FUNC_LOC_COL].astype(str)\n",
    "\n",
    "# 5️  MERGE TRACE ← AUDIT -----------------------------------------------------\n",
    "merged = (trace_df.merge(agg, how=\"left\",\n",
    "                 left_on=\"DESTINATION_LOCATION\",\n",
    "                 right_on=FUNC_LOC_COL).drop(columns=[FUNC_LOC_COL]))\n",
    "\n",
    "merged[\"LATEST_DT_DATE\"] = pd.to_datetime(merged[\"LATEST_DT_DATE\"]).dt.date\n",
    "\n",
    "# Add LOCATION column as a copy of DESTINATION_LOCATION\n",
    "merged[\"LOCATION\"] = merged[\"DESTINATION_LOCATION\"]\n",
    "\n",
    "# KEEP ONLY ROWS WHERE FROM_TO IS xxxx-yyyy BOTH NUMERIC\n",
    "def from_to_is_numeric(s):\n",
    "    match = re.fullmatch(r'(\\d+)-(\\d+)', str(s))\n",
    "    return bool(match)\n",
    "merged = merged[merged['FROM_TO'].apply(from_to_is_numeric)]\n",
    "\n",
    "# 6️  EXPORT ------------------------------------------------------------------\n",
    "cols = [\"FEEDER_ID\", \"FROM_TO\", \"SOURCE_LOCATION\", \"DESTINATION_LOCATION\", \"LOCATION\", \"RANK\", \"LATEST_DT_DATE\", \"DT_LOAD\"]\n",
    "merged.to_csv(OUTPUT_PATH, index=False, columns=cols)\n",
    "print(f\"\\nSaved {len(merged):,} rows → {OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(merged.head())\n",
    "    except Exception:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03855293",
   "metadata": {},
   "source": [
    "FEEDER FROM THE ANOTHER FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "846e847e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/final_two_column_with_rank_11_withoutDT.csv')\n",
    "col = df['FEEDER_ID'].dropna().unique()\n",
    "print(len(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1ef80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
