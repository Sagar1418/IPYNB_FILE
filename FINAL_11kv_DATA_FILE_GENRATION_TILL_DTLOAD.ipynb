{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3175495c",
   "metadata": {},
   "source": [
    "FILTER 11KV BUT  INCLUDE ALL THE VALUE OF 11KV BECAUSE OF 11Kv, 11KV, 11kV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "550c5b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing 1424 feeders …\n",
      "  → 1/1424: 15454\n",
      "  → 100/1424: 41897\n",
      "  → 200/1424: 28223\n",
      "  → 300/1424: 39624\n",
      "  → 400/1424: 41709\n",
      "  → 500/1424: 31267\n",
      "  → 600/1424: 28674\n",
      "  → 700/1424: 35873\n",
      "  → 800/1424: 30135\n",
      "  → 900/1424: 18093\n",
      "  → 1000/1424: 30031\n",
      "  → 1100/1424: 03101\n",
      "  → 1200/1424: 35038\n",
      "  → 1300/1424: 30886\n",
      "  → 1400/1424: 19090\n",
      "  → 1424/1424: BUSPT\n",
      "\n",
      "Loading energy-audit …\n",
      "\n",
      "Saved 16,844 rows → /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/final_two_column_with_rank_11_with_DT.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEEDER_ID</th>\n",
       "      <th>FROM_TO</th>\n",
       "      <th>FROM_SWITCH</th>\n",
       "      <th>TO_SWITCH</th>\n",
       "      <th>SOURCE_LOCATION</th>\n",
       "      <th>DESTINATION_LOCATION</th>\n",
       "      <th>RANK</th>\n",
       "      <th>LATEST_DT_DATE</th>\n",
       "      <th>DT_LOAD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15454</td>\n",
       "      <td>15454-38196</td>\n",
       "      <td>15454</td>\n",
       "      <td>38196</td>\n",
       "      <td>1S-MH-MU-ZST-RSTN-24TH</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-1238</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>127.425882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15454</td>\n",
       "      <td>38195-34116</td>\n",
       "      <td>38195</td>\n",
       "      <td>34116</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-1238</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0894</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>233.628927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15454</td>\n",
       "      <td>38197-DT</td>\n",
       "      <td>38197</td>\n",
       "      <td>DT</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-1238</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-1238</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>127.425882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15454</td>\n",
       "      <td>34114-32764</td>\n",
       "      <td>34114</td>\n",
       "      <td>32764</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0894</td>\n",
       "      <td>1S-MH-MU-ZST-CL01-0860</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>134.062123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15454</td>\n",
       "      <td>34115-DT</td>\n",
       "      <td>34115</td>\n",
       "      <td>DT</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0894</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0894</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>233.628927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FEEDER_ID      FROM_TO FROM_SWITCH TO_SWITCH         SOURCE_LOCATION  \\\n",
       "0     15454  15454-38196       15454     38196  1S-MH-MU-ZST-RSTN-24TH   \n",
       "1     15454  38195-34116       38195     34116  1S-MH-MU-ZST-CL02-1238   \n",
       "2     15454     38197-DT       38197        DT  1S-MH-MU-ZST-CL02-1238   \n",
       "3     15454  34114-32764       34114     32764  1S-MH-MU-ZST-CL02-0894   \n",
       "4     15454     34115-DT       34115        DT  1S-MH-MU-ZST-CL02-0894   \n",
       "\n",
       "     DESTINATION_LOCATION  RANK LATEST_DT_DATE     DT_LOAD  \n",
       "0  1S-MH-MU-ZST-CL02-1238     0     2025-04-04  127.425882  \n",
       "1  1S-MH-MU-ZST-CL02-0894     1     2025-04-04  233.628927  \n",
       "2  1S-MH-MU-ZST-CL02-1238     1     2025-04-04  127.425882  \n",
       "3  1S-MH-MU-ZST-CL01-0860     2     2025-04-04  134.062123  \n",
       "4  1S-MH-MU-ZST-CL02-0894     2     2025-04-04  233.628927  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feeder_trace_latest_audit_with_rank.py\n",
    "\"\"\"\n",
    "Workflow\n",
    "========\n",
    "1. Load HTCABLE.csv, drop unused columns, remove fully-identical rows.\n",
    "2. Trace every feeder edge‑by‑edge, annotate with RANK (distance from feeder start).\n",
    "3. Load ENERGYAUDIT.csv, for each transformer (FUNC_LOC) compute:\n",
    "   * LATEST_DT_DATE  → most‑recent SYSTEM_DATE\n",
    "   * DT_LOAD         → average MD_KVA across all rows\n",
    "4. Merge audit stats onto trace (DESTINATION_LOCATION = FUNC_LOC).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List, Set, Optional\n",
    "\n",
    "# ── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "INPUT_HT      = \"/media/sagark24/New Volume/MERGE CDIS/2-Year-data/CLEANED_DATA/ht_cleaned.csv\"\n",
    "INPUT_ENERGY  = \"/media/sagark24/New Volume/MERGE CDIS/2-Year-data/CLEANED_DATA/energyaudit_cleaned.csv\"\n",
    "OUTPUT_PATH   = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/final_two_column_with_rank_11_with_DT.csv\"\n",
    "\n",
    "\n",
    "FEEDER_ID_COL  = \"FEEDERID\"\n",
    "SRC_SWITCH_COL = \"SOURCE_SWITCH_ID\"\n",
    "DST_SWITCH_COL = \"DESTINATION_SWITCH_ID\"\n",
    "SRC_LOC_COL    = \"SOURCE_SSFL\"\n",
    "DST_LOC_COL    = \"DESTINATION_SSFL\"   # ≡ FUNC_LOC in audit\n",
    "\n",
    "FUNC_LOC_COL = \"FUNC_LOC\"\n",
    "DATE_COL     = \"SYSTEM_DATE\"\n",
    "LOAD_COL     = \"MD_KVA\"\n",
    "\n",
    "REDUNDANT_COLS = [\n",
    "    \"COMMENTS\", \"GLOBALID\", \"MEASUREDLENGTH\", \"UNNAMED: 0\", \"OBJECTID\"\n",
    "]\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1️  LOAD & CLEAN HT-CABLE ---------------------------------------------------\n",
    "ht_path = Path(INPUT_HT).expanduser()\n",
    "\n",
    "ht = pd.read_csv(ht_path, low_memory=False)\n",
    "ht = ht.drop(columns=[c for c in REDUNDANT_COLS if c in ht.columns], errors=\"ignore\")\n",
    "ht = ht.drop_duplicates()  # remove fully-identical rows\n",
    "\n",
    "# helper to pull token after 2nd underscore\n",
    "def _feeder_token(val: str | int | float | None) -> Optional[str]:\n",
    "    if not isinstance(val, str):\n",
    "        val = str(val) if val is not None else \"\"\n",
    "    p = val.split(\"_\")\n",
    "    return p[2] if len(p) >= 3 and (p[1] == '11kV' or p[1]=='11Kv' or p[1]=='11KV') else None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def extract_feeder_id(value: str | int | float | None) -> Optional[str]:\n",
    "#     \"\"\"\n",
    "#     Return the token after the 2nd underscore only if the middle token is '11kV'.\n",
    "#     Example: 'AMBVLI_11kV_19556' ➜ '19556'\n",
    "#     \"\"\"\n",
    "#     if not isinstance(value, str):\n",
    "#         value = str(value) if value is not None else \"\"\n",
    "#     parts = value.split(\"_\")\n",
    "#     if len(parts) >= 3 :\n",
    "#         return parts[2]\n",
    "#     return None\n",
    "\n",
    "\n",
    "\n",
    "ht[\"FEEDER_ID\"] = ht[FEEDER_ID_COL].apply(_feeder_token)\n",
    "\n",
    "for col in [SRC_SWITCH_COL, DST_SWITCH_COL, SRC_LOC_COL, DST_LOC_COL]:\n",
    "    ht[col] = ht[col].astype(str)\n",
    "\n",
    "edge_cols = [SRC_SWITCH_COL, DST_SWITCH_COL, SRC_LOC_COL, DST_LOC_COL]\n",
    "source_idx: Dict[Tuple[str, str], pd.DataFrame] = {\n",
    "    (k[0], k[1]): g[edge_cols]\n",
    "    for k, g in ht.groupby([SRC_LOC_COL, \"FEEDER_ID\"], sort=False)\n",
    "}\n",
    "\n",
    "# 2️  FEEDER TRACER (with RANK) -----------------------------------------------\n",
    "from collections import deque\n",
    "\n",
    "def trace_feeder(fid: str) -> list:\n",
    "    rows = []\n",
    "    visited = set()\n",
    "    feeder_edges = ht[ht[\"FEEDER_ID\"] == fid][[SRC_LOC_COL, DST_LOC_COL, SRC_SWITCH_COL, DST_SWITCH_COL]].copy()\n",
    "    feeder_edges[SRC_LOC_COL] = feeder_edges[SRC_LOC_COL].astype(str).str.strip()\n",
    "    feeder_edges[DST_LOC_COL] = feeder_edges[DST_LOC_COL].astype(str).str.strip()\n",
    "\n",
    "    from_loc_map = {}\n",
    "    for _, r in feeder_edges.iterrows():\n",
    "        from_loc_map.setdefault(r[SRC_LOC_COL], []).append(\n",
    "            tuple(r[c] for c in [SRC_LOC_COL, DST_LOC_COL, SRC_SWITCH_COL, DST_SWITCH_COL])\n",
    "        )\n",
    "\n",
    "    all_from = set(feeder_edges[SRC_LOC_COL])\n",
    "    all_to = set(feeder_edges[DST_LOC_COL])\n",
    "    root_candidates = (all_from - all_to) or all_from or set(feeder_edges[SRC_LOC_COL].unique())\n",
    "\n",
    "    all_edges = set((row[SRC_LOC_COL], row[DST_LOC_COL]) for _, row in feeder_edges.iterrows())\n",
    "    unvisited_edges = all_edges - visited\n",
    "\n",
    "    from collections import deque\n",
    "    while unvisited_edges:\n",
    "        # Find the next root (or any remaining edge)\n",
    "        found = False\n",
    "        for root in root_candidates:\n",
    "            start_rows = [e for e in feeder_edges.to_records(index=False)\n",
    "                          if e[0] == root and (e[0], e[1]) in unvisited_edges]\n",
    "            if start_rows:\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            # Just pick any edge not yet visited\n",
    "            start_rows = [e for e in feeder_edges.to_records(index=False)\n",
    "                          if (e[0], e[1]) in unvisited_edges]\n",
    "            if not start_rows:\n",
    "                break\n",
    "        queue = deque()\n",
    "        for srow in start_rows:\n",
    "            queue.append((srow, 0))\n",
    "        while queue:\n",
    "            (src_loc, dst_loc, src_sw, dst_sw), rank = queue.popleft()\n",
    "            if (src_loc, dst_loc) in visited:\n",
    "                continue\n",
    "            visited.add((src_loc, dst_loc))\n",
    "            unvisited_edges.discard((src_loc, dst_loc))\n",
    "            rows.append({\n",
    "                \"FEEDER_ID\": fid,\n",
    "                \"FROM_TO\": f\"{src_sw}-{dst_sw}\",\n",
    "                \"FROM_SWITCH\": src_sw,\n",
    "                \"TO_SWITCH\": dst_sw,\n",
    "                \"SOURCE_LOCATION\": src_loc,\n",
    "                \"DESTINATION_LOCATION\": dst_loc,\n",
    "                \"RANK\": rank\n",
    "            })\n",
    "            for next_edge in from_loc_map.get(dst_loc, []):\n",
    "                if (next_edge[0], next_edge[1]) not in visited:\n",
    "                    queue.append((next_edge, rank + 1))\n",
    "    return rows\n",
    "\n",
    "# 3️  TRACE ALL FEEDERS -------------------------------------------------------\n",
    "all_edges: List[dict] = []\n",
    "feeder_ids = [str(f) for f in ht[\"FEEDER_ID\"].dropna().unique()]\n",
    "print(f\"Tracing {len(feeder_ids)} feeders …\")\n",
    "for i, fid in enumerate(feeder_ids, 1):\n",
    "    if i % 100 == 0 or i in {1, len(feeder_ids)}:\n",
    "        print(f\"  → {i}/{len(feeder_ids)}: {fid}\")\n",
    "    all_edges.extend(trace_feeder(fid))\n",
    "\n",
    "trace_df = pd.DataFrame(all_edges)\n",
    "\n",
    "# 4️ LOAD ENERGY-AUDIT & AGGREGATE -----------------------------------------\n",
    "audit_path = Path(INPUT_ENERGY).expanduser()\n",
    "if not audit_path.exists():\n",
    "    raise FileNotFoundError(audit_path)\n",
    "\n",
    "print(\"\\nLoading energy-audit …\")\n",
    "audit = pd.read_csv(audit_path, low_memory=False, parse_dates=[DATE_COL])\n",
    "audit.columns = [c.upper() for c in audit.columns]\n",
    "\n",
    "audit[DATE_COL] = pd.to_datetime(audit[DATE_COL], errors=\"coerce\")\n",
    "\n",
    "audit = audit[[FUNC_LOC_COL, DATE_COL, LOAD_COL]].dropna(subset=[FUNC_LOC_COL])\n",
    "\n",
    "agg = (audit.groupby(FUNC_LOC_COL)\n",
    "           .agg(LATEST_DT_DATE=(DATE_COL, \"max\"),\n",
    "                DT_LOAD=(LOAD_COL,  \"mean\"))\n",
    "           .reset_index())\n",
    "agg[FUNC_LOC_COL] = agg[FUNC_LOC_COL].astype(str)\n",
    "\n",
    "# 5️  MERGE TRACE ← AUDIT -----------------------------------------------------\n",
    "merged = (trace_df.merge(agg, how=\"left\",\n",
    "                 left_on=\"DESTINATION_LOCATION\",\n",
    "                 right_on=FUNC_LOC_COL).drop(columns=[FUNC_LOC_COL]))\n",
    "\n",
    "merged[\"LATEST_DT_DATE\"] = pd.to_datetime(merged[\"LATEST_DT_DATE\"]).dt.date\n",
    "\n",
    "# 6️  EXPORT ------------------------------------------------------------------\n",
    "cols = [\"FEEDER_ID\", \"FROM_TO\", \"SOURCE_LOCATION\", \"DESTINATION_LOCATION\", \"RANK\", \"LATEST_DT_DATE\", \"DT_LOAD\"]\n",
    "merged.to_csv(OUTPUT_PATH, index=False, columns=cols)\n",
    "print(f\"\\nSaved {len(merged):,} rows → {OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(merged.head())\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bc1cbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique mid tokens from FEEDER_ID_COL: [None '11KV' '11kV' 'GOR0552' 'BOR00552' 'BOR00952' 'SAK1152' 'SAK0452'\n",
      " '33KV' '33kV' '22kV' 'GOI00152' '11Kv' 'REACTOR' '22KV' 'VER00152'\n",
      " 'GHO01452' 'GHO01652' 'AAR01352' 'AAR01552' 'GOI00352' 'GOI00852'\n",
      " 'CHE00952' 'CHE00152' 'CHE00552' 'CHE00652' 'DHN00352' '40973' '40974'\n",
      " '40976' '40977' '40978' '40980' '33360' 'AAR00452' 'AAR00552' 'AAR00652'\n",
      " 'AAR00852' 'AAR01152' 'BOR00152' 'BOR00752' 'CHE00252' 'CHE00852'\n",
      " 'CHE01052' 'GHD00752' 'GHD00952' 'GHD1052' 'GOI00252' 'GOI00752'\n",
      " 'GOR00252' 'GOR00752' 'GOR01052' 'SAK00752' 'SAK00952' 'SAK01252'\n",
      " 'VER00652' 'VER00752' 'VER00852' 'VER01052' 'VER01352' '33Kv' 'BOR00352'\n",
      " 'BOR01652' 'VER01452' 'VER01552' 'SAK00652' 'SAK00352' 'GOI00652'\n",
      " 'GOR00452' 'GOR00152']\n"
     ]
    }
   ],
   "source": [
    "def extract_mid_token(val: str | int | float | None) -> Optional[str]:\n",
    "    if not isinstance(val, str):\n",
    "        val = str(val) if val is not None else \"\"\n",
    "    p = val.split(\"_\")\n",
    "    return p[1] if len(p) >= 2 else None\n",
    "\n",
    "unique_mid_tokens = ht[FEEDER_ID_COL].apply(extract_mid_token).unique()\n",
    "print(\"Unique mid tokens from FEEDER_ID_COL:\", unique_mid_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "360d1f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique mid tokens from FEEDER_ID_COL: [None '24THRD' 'AAR01752' 'AAR01852' 'AAR01952' 'AAR1252' 'AAR1452' 'ACRO'\n",
      " 'AMBVLI' 'ANDHRI' 'ANIK' 'ARY220' 'ARY' 'BANDRA' 'BBLWDI' 'BHAVANS'\n",
      " 'BHAYW' 'BHYNDR' 'BKC' 'BNDRTE' 'BORIVLI' 'BORVLI' 'BOR' 'CAMA' 'CHAKALA'\n",
      " 'CHBNDR' 'CHDNGR' 'CHDVLI' 'CHE' 'CHMBUR' 'CHMBU' 'CHMB' 'CHUNA' 'CHVALI'\n",
      " 'CPWDMAREC' 'DAHICHNKA' 'DAHISRW' 'DAHISR' 'DEVIDAS' 'DHA' 'DINDO'\n",
      " 'ERANGL' 'ESIC' 'GHO00452' 'GHOD' 'GKLDHM' 'GNSHNG' 'GODREJBKC'\n",
      " 'GOI00452' 'GOI220' 'GOR220' 'GORAI' 'GOREG' 'HCC' 'HINGWALA' 'HIRANANDA'\n",
      " 'HIRANA' 'HULL' 'JANKALYAN' 'JBNGR' 'JUHUN' 'JUHU' 'KADAMWADI' 'KALANR'\n",
      " 'KALINA' 'KALPATARU' 'KANA' 'KANDI' 'KHAR' 'KIE' 'KOHINR' 'KURLA' 'KURL'\n",
      " 'LKHWLA' 'MAHANANDA' 'MAHULSRA' 'MAKERS' 'MALAD' 'MAL' 'MANK' 'MAROL'\n",
      " 'MBI00152' 'MBI00652' 'MBO00152' 'MBR00152' 'MBR00252' 'MBR00552'\n",
      " 'MBR00752' 'MBR00852' 'MGHWDI' 'MHADAMANK' 'MHADASAH' 'MHADSAH' 'MIDC'\n",
      " 'MINDSP' 'MIRA' 'MMRDA' 'MNR00152' 'MTR00152' 'MTR00252' 'MTR00352'\n",
      " 'MVR00152' 'NAHAR SHAKTI DSS' 'NAHAR' 'NATPAR' 'NESCO' 'NETMAGICDC9NO1'\n",
      " 'NIRLON' 'OMKAR' 'OSHIWARA' 'PALI' 'PALMCT' 'POISAR' 'POWAI' 'PT220'\n",
      " 'RAHEJA' 'RAVI' 'RIL' 'RNARYL' 'ROYAL PAL' 'RUNWAL' 'SADG' 'SAHARPLZ'\n",
      " 'SAHAR' 'SAKI' 'SAK' 'SAMBHNGR' 'SARVODAY' 'SCRUZ' 'SEEPZ' 'SHANTIST'\n",
      " 'SHIMP' 'SHRADDHAN' 'SHVNGR ' 'SIDNGR ' 'SRSWTI' 'SWAN' 'SWMSMTNGR'\n",
      " 'TBR00252' 'TGRNGR ' 'TGRNGR' 'TIMES' 'TLKNGR ' 'TPCVER0152' 'TPCVER0252'\n",
      " 'TSK00152' 'TSK00652' 'VAZIRA' 'VER00352' 'VER01152' 'VER01252' 'VER220'\n",
      " 'VER' 'VIHAR ' 'VIHAR' 'VIKHR ' 'VIK' 'VINA' 'VPARLE' 'VSV220']\n"
     ]
    }
   ],
   "source": [
    "def extract_mid_token(val: str | int | float | None) -> Optional[str]:\n",
    "    if not isinstance(val, str):\n",
    "        val = str(val) if val is not None else \"\"\n",
    "    p = val.split(\"_\")\n",
    "    return p[0] if len(p) >= 2 else None\n",
    "\n",
    "unique_mid_tokens = ht[FEEDER_ID_COL].apply(extract_mid_token).unique()\n",
    "print(\"Unique mid tokens from FEEDER_ID_COL:\", unique_mid_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71c3bf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique mid tokens from FEEDER_ID_COL: [None '15454' '15451' ... '40877' '40878' 'BUSPT']\n"
     ]
    }
   ],
   "source": [
    "def extract_mid_token(val: str | int | float | None) -> Optional[str]:\n",
    "    if not isinstance(val, str):\n",
    "        val = str(val) if val is not None else \"\"\n",
    "    p = val.split(\"_\")\n",
    "    return p[2] if len(p) >= 3 else None\n",
    "\n",
    "unique_mid_tokens = ht[FEEDER_ID_COL].apply(extract_mid_token).unique()\n",
    "print(\"Unique mid tokens from FEEDER_ID_COL:\", unique_mid_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8d77ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "149c89be",
   "metadata": {},
   "source": [
    "REMOVE DT AND USE 11KV VOATGE FILE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd34aa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing 1425 feeders …\n",
      "  -> 1/1425: None\n",
      "  -> 100/1425: 41896\n",
      "  -> 200/1425: 1339\n",
      "  -> 300/1425: 39622\n",
      "  -> 400/1425: 41588\n",
      "  -> 500/1425: 31266\n",
      "  -> 600/1425: 28673\n",
      "  -> 700/1425: 35872\n",
      "  -> 800/1425: 30134\n",
      "  -> 900/1425: 18092\n",
      "  -> 1000/1425: 27084\n",
      "  -> 1100/1425: 36251\n",
      "  -> 1200/1425: 35037\n",
      "  -> 1300/1425: 30885\n",
      "  -> 1400/1425: 19088\n",
      "  -> 1425/1425: BUSPT\n",
      "\n",
      "Loading energy-audit …\n",
      "Lost feeders: 301\n",
      "Lost feeder IDs saved  -> /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/lost_feeder_ids.csv\n",
      "Full data for lost feeders saved -> /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/lost_feeders_full_data.csv\n",
      "\n",
      "Saved 8,699 rows -> /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/final_two_column_with_rank_11_withoutDT_connected.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEEDER_ID</th>\n",
       "      <th>FROM_TO</th>\n",
       "      <th>SOURCE_LOCATION</th>\n",
       "      <th>DESTINATION_LOCATION</th>\n",
       "      <th>RANK</th>\n",
       "      <th>LATEST_DT_DATE</th>\n",
       "      <th>DT_LOAD</th>\n",
       "      <th>LOCATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15454</td>\n",
       "      <td>15454-38196</td>\n",
       "      <td>1S-MH-MU-ZST-RSTN-24TH</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-1238</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>127.425882</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-1238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15454</td>\n",
       "      <td>38195-34116</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-1238</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0894</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>233.628927</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15454</td>\n",
       "      <td>34114-32764</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0894</td>\n",
       "      <td>1S-MH-MU-ZST-CL01-0860</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>134.062123</td>\n",
       "      <td>1S-MH-MU-ZST-CL01-0860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15454</td>\n",
       "      <td>32766-31556</td>\n",
       "      <td>1S-MH-MU-ZST-CL01-0860</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0815</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>245.587090</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15454</td>\n",
       "      <td>31555-4467</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0815</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0054</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>364.485990</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FEEDER_ID      FROM_TO         SOURCE_LOCATION    DESTINATION_LOCATION  \\\n",
       "0     15454  15454-38196  1S-MH-MU-ZST-RSTN-24TH  1S-MH-MU-ZST-CL02-1238   \n",
       "1     15454  38195-34116  1S-MH-MU-ZST-CL02-1238  1S-MH-MU-ZST-CL02-0894   \n",
       "3     15454  34114-32764  1S-MH-MU-ZST-CL02-0894  1S-MH-MU-ZST-CL01-0860   \n",
       "5     15454  32766-31556  1S-MH-MU-ZST-CL01-0860  1S-MH-MU-ZST-CL02-0815   \n",
       "7     15454   31555-4467  1S-MH-MU-ZST-CL02-0815  1S-MH-MU-ZST-CL02-0054   \n",
       "\n",
       "   RANK LATEST_DT_DATE     DT_LOAD                LOCATION  \n",
       "0     0     2025-04-04  127.425882  1S-MH-MU-ZST-CL02-1238  \n",
       "1     1     2025-04-04  233.628927  1S-MH-MU-ZST-CL02-0894  \n",
       "3     2     2025-04-04  134.062123  1S-MH-MU-ZST-CL01-0860  \n",
       "5     3     2025-04-04  245.587090  1S-MH-MU-ZST-CL02-0815  \n",
       "7     4     2025-04-04  364.485990  1S-MH-MU-ZST-CL02-0054  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feeder_trace_latest_audit_with_rank_updated.py\n",
    "\"\"\"\n",
    "Workflow\n",
    "========\n",
    "1. Load HTCABLE.csv, drop unused columns, remove fully‑identical rows.\n",
    "2. Trace every feeder edge‑by‑edge, annotate with RANK (distance from feeder start).\n",
    "3. Load ENERGYAUDIT.csv, for each transformer (FUNC_LOC) compute:\n",
    "   * LATEST_DT_DATE  → most‑recent SYSTEM_DATE\n",
    "   * DT_LOAD         → average MD_KVA across all rows\n",
    "4. Merge audit stats onto trace (DESTINATION_LOCATION = FUNC_LOC).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List, Set, Optional\n",
    "import re\n",
    "\n",
    "# ── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "INPUT_HT      = \"/media/sagark24/New Volume/MERGE CDIS/2-Year-data/CLEANED_DATA/ht_cleaned.csv\"\n",
    "INPUT_ENERGY  = \"/media/sagark24/New Volume/MERGE CDIS/2-Year-data/CLEANED_DATA/energyaudit_cleaned.csv\"\n",
    "# FEEDER_LIST_PATH = \"/media/sagark24/New Volume/MERGE CDIS/2-Year-data/FEEDERDETAILS.csv\"\n",
    "OUTPUT_PATH = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/final_two_column_with_rank_11_withoutDT_connected.csv\"\n",
    "\n",
    "FEEDER_ID_COL  = \"FEEDERID\"\n",
    "SRC_SWITCH_COL = \"SOURCE_SWITCH_ID\"\n",
    "DST_SWITCH_COL = \"DESTINATION_SWITCH_ID\"\n",
    "SRC_LOC_COL    = \"SOURCE_SSFL\"\n",
    "DST_LOC_COL    = \"DESTINATION_SSFL\"   # ≡ FUNC_LOC in audit\n",
    "\n",
    "FUNC_LOC_COL = \"FUNC_LOC\"\n",
    "DATE_COL     = \"SYSTEM_DATE\"\n",
    "LOAD_COL     = \"MD_KVA\"\n",
    "\n",
    "REDUNDANT_COLS = [\n",
    "\n",
    "]\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1️  LOAD & CLEAN HT-CABLE ---------------------------------------------------\n",
    "ht_path = Path(INPUT_HT).expanduser()\n",
    "\n",
    "ht = pd.read_csv(ht_path, low_memory=False)\n",
    "ht = ht.drop(columns=[c for c in REDUNDANT_COLS if c in ht.columns], errors=\"ignore\")\n",
    "ht = ht.drop_duplicates()  # remove fully-identical rows\n",
    "\n",
    "# helper to pull token after 2nd underscore\n",
    "# ── helper to pull token after the 2nd underscore ────────────────────────────\n",
    "def _feeder_token(val: str | int | float | None) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract the FEEDER_ID part (token‑3 in strings like XXX_11kV_000123…)\n",
    "    and drop any *leading* ‘0’ characters from that token.\n",
    "    \"\"\"\n",
    "    if not isinstance(val, str):\n",
    "        val = str(val) if val is not None else \"\"\n",
    "    parts = val.split(\"_\")\n",
    "\n",
    "    # keep only rows whose middle token really marks 11 kV\n",
    "    if len(parts) < 3 or parts[1].upper() != \"11KV\":\n",
    "        return None\n",
    "\n",
    "    token = parts[2].lstrip(\"0\")        # ← strips leading zeros\n",
    "    return token if token else None     # keep None instead of empty string\n",
    "\n",
    "ht[\"FEEDER_ID\"] = ht[FEEDER_ID_COL].apply(_feeder_token).astype(str).str.strip()\n",
    "\n",
    "for col in [SRC_SWITCH_COL, DST_SWITCH_COL, SRC_LOC_COL, DST_LOC_COL]:\n",
    "    ht[col] = ht[col].astype(str)\n",
    "\n",
    "edge_cols = [SRC_SWITCH_COL, DST_SWITCH_COL, SRC_LOC_COL, DST_LOC_COL]\n",
    "source_idx: Dict[Tuple[str, str], pd.DataFrame] = {\n",
    "    (k[0], k[1]): g[edge_cols]\n",
    "    for k, g in ht.groupby([SRC_LOC_COL, \"FEEDER_ID\"], sort=False)\n",
    "}\n",
    "#   feeder_edges = ht[ht[\"FEEDER_ID\"] == fid][[SRC_LOC_COL, DST_LOC_COL, SRC_SWITCH_COL, DST_SWITCH_COL]].copy()\n",
    "#     feeder_edges[SRC_LOC_COL] = feeder_edges[SRC_LOC_COL].astype(str).str.strip()\n",
    "#     feeder_edges[DST_LOC_COL] = feeder_edges[DST_LOC_COL].astype(str).str.strip()\n",
    "ht[FEEDER_ID_COL] = ht[\"FEEDER_ID\"].astype(str).str.strip()\n",
    "# 2️  FEEDER TRACER (with RANK) -----------------------------------------------\n",
    "def trace_feeder(fid: str) -> List[dict]:\n",
    "    rows: List[dict] = []\n",
    "    visited: Set[Tuple[str, str]] = set()\n",
    "    \n",
    "    # queue holds tuples: (edge_tuple, rank)\n",
    "    start = ht[(ht[SRC_SWITCH_COL] == fid) & (ht[\"FEEDER_ID\"] == fid)][edge_cols]\n",
    "    queue = [(row, 0) for row in start.to_records(index=False).tolist()]  # (edge, rank)\n",
    "  \n",
    "    while queue:\n",
    "        (from_sw, to_sw, src_loc, dst_loc), rank = queue.pop(0)\n",
    "        if (from_sw, to_sw) in visited:\n",
    "            continue\n",
    "        visited.add((from_sw, to_sw))\n",
    "\n",
    "        rows.append({\n",
    "            \"FEEDER_ID\": fid,\n",
    "            \"FROM_TO\": f\"{from_sw}-{to_sw}\",\n",
    "            \"SOURCE_LOCATION\": src_loc,\n",
    "            \"DESTINATION_LOCATION\": dst_loc,\n",
    "            \"RANK\": rank  # Level in the feeder tree\n",
    "        })\n",
    "\n",
    "        nxt = source_idx.get((dst_loc, fid))\n",
    "        if nxt is not None and not nxt.empty:\n",
    "            # Each downstream edge gets rank+1\n",
    "            queue.extend([(row, rank + 1) for row in nxt.to_records(index=False).tolist()])\n",
    "\n",
    "    return rows\n",
    "\n",
    "# 3️  TRACE ALL FEEDERS -------------------------------------------------------\n",
    "all_edges: List[dict] = []\n",
    "feeder_ids = [str(f) for f in ht[\"FEEDER_ID\"].dropna().unique()]\n",
    "print(f\"Tracing {len(feeder_ids)} feeders …\")\n",
    "for i, fid in enumerate(feeder_ids, 1):\n",
    "    if i % 100 == 0 or i in {1, len(feeder_ids)}:\n",
    "        print(f\"  -> {i}/{len(feeder_ids)}: {fid}\")\n",
    "    all_edges.extend(trace_feeder(fid))\n",
    "\n",
    "trace_df = pd.DataFrame(all_edges)\n",
    "\n",
    "# 4️ LOAD ENERGY-AUDIT & AGGREGATE -----------------------------------------\n",
    "audit_path = Path(INPUT_ENERGY).expanduser()\n",
    "if not audit_path.exists():\n",
    "    raise FileNotFoundError(audit_path)\n",
    "\n",
    "print(\"\\nLoading energy-audit …\")\n",
    "audit = pd.read_csv(audit_path, low_memory=False, parse_dates=[DATE_COL])\n",
    "audit.columns = [c.upper() for c in audit.columns]\n",
    "\n",
    "audit[DATE_COL] = pd.to_datetime(audit[DATE_COL], errors=\"coerce\")\n",
    "\n",
    "audit = audit[[FUNC_LOC_COL, DATE_COL, LOAD_COL]].dropna(subset=[FUNC_LOC_COL])\n",
    "\n",
    "agg = (audit.groupby(FUNC_LOC_COL)\n",
    "           .agg(LATEST_DT_DATE=(DATE_COL, \"max\"),\n",
    "                DT_LOAD=(LOAD_COL,  \"mean\"))\n",
    "           .reset_index())\n",
    "agg[FUNC_LOC_COL] = agg[FUNC_LOC_COL].astype(str)\n",
    "\n",
    "# 5️  MERGE TRACE ← AUDIT -----------------------------------------------------\n",
    "merged = (trace_df.merge(agg, how=\"left\",\n",
    "                 left_on=\"DESTINATION_LOCATION\",\n",
    "                 right_on=FUNC_LOC_COL).drop(columns=[FUNC_LOC_COL]))\n",
    "\n",
    "merged[\"LATEST_DT_DATE\"] = pd.to_datetime(merged[\"LATEST_DT_DATE\"]).dt.date\n",
    "\n",
    "# Add LOCATION column as a copy of DESTINATION_LOCATION\n",
    "merged[\"LOCATION\"] = merged[\"DESTINATION_LOCATION\"]\n",
    "\n",
    "# KEEP ONLY ROWS WHERE FROM_TO IS xxxx-yyyy BOTH NUMERIC\n",
    "# def from_to_is_numeric(s):\n",
    "#     match = re.fullmatch(r'(\\d+)-(\\d+)', str(s))\n",
    "#     return bool(match)\n",
    "# merged = merged[merged['FROM_TO'].apply(from_to_is_numeric)]\n",
    "\n",
    "merged_raw = merged.copy()        \\\n",
    "\n",
    "# ── 1. FROM_TO numeric ----------------------------------------\n",
    "def from_to_is_numeric(s):\n",
    "    return bool(re.fullmatch(r'(\\d+)-(\\d+)', str(s)))\n",
    "\n",
    "mask_numeric  = merged_raw['FROM_TO'].apply(from_to_is_numeric)\n",
    "removed_rows  = merged_raw.loc[~mask_numeric]     # non numeric rows\n",
    "kept_rows     = merged_raw.loc[mask_numeric]      # numeric rows\n",
    "# ── 2. comparision of before and after FEEDER_ID  --------------------------\n",
    "all_feeders_before = set(merged_raw['FEEDER_ID'].dropna().unique())\n",
    "all_feeders_after  = set(kept_rows['FEEDER_ID'].dropna().unique())\n",
    "lost_feeders       = sorted(all_feeders_before - all_feeders_after)\n",
    "\n",
    "print(f\"Lost feeders: {len(lost_feeders)}\")\n",
    "\n",
    "# ── 3. LOST feeders full data  --------------------------\n",
    "lost_data = merged_raw[merged_raw['FEEDER_ID'].isin(lost_feeders)].copy()\n",
    "\n",
    "lost_ids_path   = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/lost_feeder_ids.csv\"\n",
    "lost_data_path  = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/lost_feeders_full_data.csv\"\n",
    "# LOST_FEEDER_ID \n",
    "pd.Series(lost_feeders, name=\"LOST_FEEDER_ID\").to_csv(lost_ids_path, index=False)\n",
    "# lost_data \n",
    "lost_data.to_csv(lost_data_path, index=False)\n",
    "\n",
    "print(f\"Lost feeder IDs saved  -> {lost_ids_path}\")\n",
    "print(f\"Full data for lost feeders saved -> {lost_data_path}\")\n",
    "\n",
    "# ── 4.  kept_rows ------------------------\n",
    "merged = kept_rows.copy()       \n",
    "\n",
    "# 6️  EXPORT ------------------------------------------------------------------\n",
    "cols = [\"FEEDER_ID\", \"FROM_TO\", \"SOURCE_LOCATION\", \"DESTINATION_LOCATION\", \"LOCATION\", \"RANK\", \"LATEST_DT_DATE\", \"DT_LOAD\"]\n",
    "merged.to_csv(OUTPUT_PATH, index=False, columns=cols)\n",
    "print(f\"\\nSaved {len(merged):,} rows -> {OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(merged.head())\n",
    "    except Exception:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "effc3dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique FEEDER_ID values: 945\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(OUTPUT_PATH)\n",
    "col = df['FEEDER_ID'].unique()\n",
    "print(\"Unique FEEDER_ID values:\", len(col))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03855293",
   "metadata": {},
   "source": [
    "REMOVING LEADING 00 FROM THE FEEDERID AND SWITCH_IDS and retain also diconnected switches\n",
    "and map all the link by source sfl and find all root note and then do bfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ecb2bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing 1424 feeders …\n",
      "  -> 1/1424: 15454\n",
      "  -> 100/1424: 41897\n",
      "  -> 200/1424: 28223\n",
      "  -> 300/1424: 39624\n",
      "  -> 400/1424: 41709\n",
      "  -> 500/1424: 31267\n",
      "  -> 600/1424: 28674\n",
      "  -> 700/1424: 35873\n",
      "  -> 800/1424: 30135\n",
      "  -> 900/1424: 18093\n",
      "  -> 1000/1424: 30031\n",
      "  -> 1100/1424: 03101\n",
      "  -> 1200/1424: 35038\n",
      "  -> 1300/1424: 30886\n",
      "  -> 1400/1424: 19090\n",
      "  -> 1424/1424: BUSPT\n",
      "\n",
      "Loading energy-audit …\n",
      "Lost feeders: 304\n",
      "Lost feeder IDs saved  -> /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/lost_feeder_ids.csv\n",
      "Full data for lost feeders saved -> /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/lost_feeders_full_data.csv\n",
      "\n",
      "Saved 9,308 rows -> /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/final_two_column_with_rank_11_withoutDT_disconnected_component.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEEDER_ID</th>\n",
       "      <th>FROM_TO</th>\n",
       "      <th>SOURCE_LOCATION</th>\n",
       "      <th>DESTINATION_LOCATION</th>\n",
       "      <th>RANK</th>\n",
       "      <th>LATEST_DT_DATE</th>\n",
       "      <th>DT_LOAD</th>\n",
       "      <th>LOCATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15454</td>\n",
       "      <td>15454-38196</td>\n",
       "      <td>1S-MH-MU-ZST-RSTN-24TH</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-1238</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>127.425882</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-1238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15454</td>\n",
       "      <td>38195-34116</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-1238</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0894</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>233.628927</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15454</td>\n",
       "      <td>34114-32764</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0894</td>\n",
       "      <td>1S-MH-MU-ZST-CL01-0860</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>134.062123</td>\n",
       "      <td>1S-MH-MU-ZST-CL01-0860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15454</td>\n",
       "      <td>32766-31556</td>\n",
       "      <td>1S-MH-MU-ZST-CL01-0860</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0815</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>245.587090</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15454</td>\n",
       "      <td>31555-4467</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0815</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0054</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>364.485990</td>\n",
       "      <td>1S-MH-MU-ZST-CL02-0054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FEEDER_ID      FROM_TO         SOURCE_LOCATION    DESTINATION_LOCATION  \\\n",
       "0     15454  15454-38196  1S-MH-MU-ZST-RSTN-24TH  1S-MH-MU-ZST-CL02-1238   \n",
       "1     15454  38195-34116  1S-MH-MU-ZST-CL02-1238  1S-MH-MU-ZST-CL02-0894   \n",
       "3     15454  34114-32764  1S-MH-MU-ZST-CL02-0894  1S-MH-MU-ZST-CL01-0860   \n",
       "5     15454  32766-31556  1S-MH-MU-ZST-CL01-0860  1S-MH-MU-ZST-CL02-0815   \n",
       "7     15454   31555-4467  1S-MH-MU-ZST-CL02-0815  1S-MH-MU-ZST-CL02-0054   \n",
       "\n",
       "   RANK LATEST_DT_DATE     DT_LOAD                LOCATION  \n",
       "0     0     2025-04-04  127.425882  1S-MH-MU-ZST-CL02-1238  \n",
       "1     1     2025-04-04  233.628927  1S-MH-MU-ZST-CL02-0894  \n",
       "3     2     2025-04-04  134.062123  1S-MH-MU-ZST-CL01-0860  \n",
       "5     3     2025-04-04  245.587090  1S-MH-MU-ZST-CL02-0815  \n",
       "7     4     2025-04-04  364.485990  1S-MH-MU-ZST-CL02-0054  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feeder_trace_latest_audit_with_rank_updated.py\n",
    "\"\"\"\n",
    "Workflow\n",
    "========\n",
    "1. Load HTCABLE.csv, drop unused columns, remove fully‑identical rows.\n",
    "2. Trace every feeder edge‑by‑edge, annotate with RANK (distance from feeder start).\n",
    "3. Load ENERGYAUDIT.csv, for each transformer (FUNC_LOC) compute:\n",
    "   * LATEST_DT_DATE  → most‑recent SYSTEM_DATE\n",
    "   * DT_LOAD         → average MD_KVA across all rows\n",
    "4. Merge audit stats onto trace (DESTINATION_LOCATION = FUNC_LOC).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List, Set, Optional\n",
    "import re\n",
    "\n",
    "# ── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "INPUT_HT      = \"/media/sagark24/New Volume/MERGE CDIS/2-Year-data/CLEANED_DATA/ht_cleaned.csv\"\n",
    "INPUT_ENERGY  = \"/media/sagark24/New Volume/MERGE CDIS/2-Year-data/CLEANED_DATA/energyaudit_cleaned.csv\"\n",
    "# FEEDER_LIST_PATH = \"/media/sagark24/New Volume/MERGE CDIS/2-Year-data/FEEDERDETAILS.csv\"\n",
    "OUTPUT_PATH = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/final_two_column_with_rank_11_withoutDT_disconnected_component.csv\"\n",
    "\n",
    "FEEDER_ID_COL  = \"FEEDERID\"\n",
    "SRC_SWITCH_COL = \"SOURCE_SWITCH_ID\"\n",
    "DST_SWITCH_COL = \"DESTINATION_SWITCH_ID\"\n",
    "SRC_LOC_COL    = \"SOURCE_SSFL\"\n",
    "DST_LOC_COL    = \"DESTINATION_SSFL\"   # ≡ FUNC_LOC in audit\n",
    "\n",
    "FUNC_LOC_COL = \"FUNC_LOC\"\n",
    "DATE_COL     = \"SYSTEM_DATE\"\n",
    "LOAD_COL     = \"MD_KVA\"\n",
    "\n",
    "REDUNDANT_COLS = [\n",
    "\n",
    "]\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1️  LOAD & CLEAN HT-CABLE ---------------------------------------------------\n",
    "ht_path = Path(INPUT_HT).expanduser()\n",
    "\n",
    "ht = pd.read_csv(ht_path, low_memory=False)\n",
    "ht = ht.drop(columns=[c for c in REDUNDANT_COLS if c in ht.columns], errors=\"ignore\")\n",
    "# ht = ht.drop_duplicates()  # remove fully-identical rows\n",
    "\n",
    "# helper to pull token after 2nd underscore\n",
    "def _feeder_token(val: str | int | float | None) -> Optional[str]:\n",
    "    if not isinstance(val, str):\n",
    "        val = str(val) if val is not None else \"\"\n",
    "    p = val.split(\"_\")\n",
    "    return p[2] if len(p) >= 3 and (p[1] == '11kV' or p[1]=='11Kv' or p[1]=='11KV')  else None\n",
    "\n",
    "ht[\"FEEDER_ID\"] = ht[FEEDER_ID_COL].apply(_feeder_token)\n",
    "\n",
    "for col in [SRC_SWITCH_COL, DST_SWITCH_COL, SRC_LOC_COL, DST_LOC_COL]:\n",
    "    ht[col] = ht[col].astype(str)\n",
    "\n",
    "edge_cols = [SRC_SWITCH_COL, DST_SWITCH_COL, SRC_LOC_COL, DST_LOC_COL]\n",
    "source_idx: Dict[Tuple[str, str], pd.DataFrame] = {\n",
    "    (k[0], k[1]): g[edge_cols]\n",
    "    for k, g in ht.groupby([SRC_LOC_COL, \"FEEDER_ID\"], sort=False)\n",
    "}\n",
    "\n",
    "# 2️  FEEDER TRACER (with RANK) -----------------------------------------------from collections import deque\n",
    "from collections import deque\n",
    "\n",
    "def trace_feeder(fid: str) -> list:\n",
    "    rows = []\n",
    "    visited = set()\n",
    "    feeder_edges = ht[ht[\"FEEDER_ID\"] == fid][[SRC_LOC_COL, DST_LOC_COL, SRC_SWITCH_COL, DST_SWITCH_COL]].copy()\n",
    "    feeder_edges[SRC_LOC_COL] = feeder_edges[SRC_LOC_COL].astype(str).str.strip()\n",
    "    feeder_edges[DST_LOC_COL] = feeder_edges[DST_LOC_COL].astype(str).str.strip()\n",
    "\n",
    "    from_loc_map = {}\n",
    "    for _, r in feeder_edges.iterrows():\n",
    "        from_loc_map.setdefault(r[SRC_LOC_COL], []).append(\n",
    "            tuple(r[c] for c in [SRC_LOC_COL, DST_LOC_COL, SRC_SWITCH_COL, DST_SWITCH_COL])\n",
    "        )\n",
    "\n",
    "    all_from = set(feeder_edges[SRC_LOC_COL])\n",
    "    all_to = set(feeder_edges[DST_LOC_COL])\n",
    "    root_candidates = (all_from - all_to) or all_from or set(feeder_edges[SRC_LOC_COL].unique())\n",
    "\n",
    "    all_edges = set((row[SRC_LOC_COL], row[DST_LOC_COL]) for _, row in feeder_edges.iterrows())\n",
    "    unvisited_edges = all_edges - visited\n",
    "\n",
    "    from collections import deque\n",
    "    while unvisited_edges:\n",
    "        # Find the next root (or any remaining edge)\n",
    "        found = False\n",
    "        for root in root_candidates:\n",
    "            start_rows = [e for e in feeder_edges.to_records(index=False)\n",
    "                          if e[0] == root and (e[0], e[1]) in unvisited_edges]\n",
    "            if start_rows:\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            # Just pick any edge not yet visited\n",
    "            start_rows = [e for e in feeder_edges.to_records(index=False)\n",
    "                          if (e[0], e[1]) in unvisited_edges]\n",
    "            if not start_rows:\n",
    "                break\n",
    "        queue = deque()\n",
    "        for srow in start_rows:\n",
    "            queue.append((srow, 0))\n",
    "        while queue:\n",
    "            (src_loc, dst_loc, src_sw, dst_sw), rank = queue.popleft()\n",
    "            if (src_loc, dst_loc) in visited:\n",
    "                continue\n",
    "            visited.add((src_loc, dst_loc))\n",
    "            unvisited_edges.discard((src_loc, dst_loc))\n",
    "            rows.append({\n",
    "                \"FEEDER_ID\": fid,\n",
    "                \"FROM_TO\": f\"{src_sw}-{dst_sw}\",\n",
    "                \"SOURCE_LOCATION\": src_loc,\n",
    "                \"DESTINATION_LOCATION\": dst_loc,\n",
    "                \"RANK\": rank\n",
    "            })\n",
    "            for next_edge in from_loc_map.get(dst_loc, []):\n",
    "                if (next_edge[0], next_edge[1]) not in visited:\n",
    "                    queue.append((next_edge, rank + 1))\n",
    "    return rows\n",
    "\n",
    "\n",
    "# 3️  TRACE ALL FEEDERS -------------------------------------------------------\n",
    "all_edges: List[dict] = []\n",
    "feeder_ids = [str(f) for f in ht[\"FEEDER_ID\"].dropna().unique()]\n",
    "print(f\"Tracing {len(feeder_ids)} feeders …\")\n",
    "for i, fid in enumerate(feeder_ids, 1):\n",
    "    if i % 100 == 0 or i in {1, len(feeder_ids)}:\n",
    "        print(f\"  -> {i}/{len(feeder_ids)}: {fid}\")\n",
    "    all_edges.extend(trace_feeder(fid))\n",
    "\n",
    "trace_df = pd.DataFrame(all_edges)\n",
    "\n",
    "# 4️ LOAD ENERGY-AUDIT & AGGREGATE -----------------------------------------\n",
    "audit_path = Path(INPUT_ENERGY).expanduser()\n",
    "if not audit_path.exists():\n",
    "    raise FileNotFoundError(audit_path)\n",
    "\n",
    "print(\"\\nLoading energy-audit …\")\n",
    "audit = pd.read_csv(audit_path, low_memory=False, parse_dates=[DATE_COL])\n",
    "audit.columns = [c.upper() for c in audit.columns]\n",
    "\n",
    "audit[DATE_COL] = pd.to_datetime(audit[DATE_COL], errors=\"coerce\")\n",
    "\n",
    "audit = audit[[FUNC_LOC_COL, DATE_COL, LOAD_COL]].dropna(subset=[FUNC_LOC_COL])\n",
    "\n",
    "agg = (audit.groupby(FUNC_LOC_COL)\n",
    "           .agg(LATEST_DT_DATE=(DATE_COL, \"max\"),\n",
    "                DT_LOAD=(LOAD_COL,  \"mean\"))\n",
    "           .reset_index())\n",
    "agg[FUNC_LOC_COL] = agg[FUNC_LOC_COL].astype(str)\n",
    "\n",
    "# 5️  MERGE TRACE ← AUDIT -----------------------------------------------------\n",
    "merged = (trace_df.merge(agg, how=\"left\",\n",
    "                 left_on=\"DESTINATION_LOCATION\",\n",
    "                 right_on=FUNC_LOC_COL).drop(columns=[FUNC_LOC_COL]))\n",
    "\n",
    "merged[\"LATEST_DT_DATE\"] = pd.to_datetime(merged[\"LATEST_DT_DATE\"]).dt.date\n",
    "\n",
    "# Add LOCATION column as a copy of DESTINATION_LOCATION\n",
    "merged[\"LOCATION\"] = merged[\"DESTINATION_LOCATION\"]\n",
    "\n",
    "# KEEP ONLY ROWS WHERE FROM_TO IS xxxx-yyyy BOTH NUMERIC\n",
    "# def from_to_is_numeric(s):\n",
    "#     match = re.fullmatch(r'(\\d+)-(\\d+)', str(s))\n",
    "#     return bool(match)\n",
    "# merged = merged[merged['FROM_TO'].apply(from_to_is_numeric)]\n",
    "\n",
    "merged_raw = merged.copy()        \\\n",
    "\n",
    "# ── 1. FROM_TO numeric ----------------------------------------\n",
    "def from_to_is_numeric(s):\n",
    "    return bool(re.fullmatch(r'(\\d+)-(\\d+)', str(s)))\n",
    "\n",
    "mask_numeric  = merged_raw['FROM_TO'].apply(from_to_is_numeric)\n",
    "removed_rows  = merged_raw.loc[~mask_numeric]     # non numeric rows\n",
    "kept_rows     = merged_raw.loc[mask_numeric]      # numeric rows\n",
    "# ── 2. comparision of before and after FEEDER_ID  --------------------------\n",
    "all_feeders_before = set(merged_raw['FEEDER_ID'].dropna().unique())\n",
    "all_feeders_after  = set(kept_rows['FEEDER_ID'].dropna().unique())\n",
    "lost_feeders       = sorted(all_feeders_before - all_feeders_after)\n",
    "\n",
    "print(f\"Lost feeders: {len(lost_feeders)}\")\n",
    "\n",
    "# ── 3. LOST feeders full data  --------------------------\n",
    "lost_data = merged_raw[merged_raw['FEEDER_ID'].isin(lost_feeders)].copy()\n",
    "\n",
    "lost_ids_path   = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/lost_feeder_ids.csv\"\n",
    "lost_data_path  = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/lost_feeders_full_data.csv\"\n",
    "# LOST_FEEDER_ID \n",
    "pd.Series(lost_feeders, name=\"LOST_FEEDER_ID\").to_csv(lost_ids_path, index=False)\n",
    "# lost_data \n",
    "lost_data.to_csv(lost_data_path, index=False)\n",
    "\n",
    "print(f\"Lost feeder IDs saved  -> {lost_ids_path}\")\n",
    "print(f\"Full data for lost feeders saved -> {lost_data_path}\")\n",
    "\n",
    "# ── 4.  kept_rows ------------------------\n",
    "merged = kept_rows.copy()       \n",
    "\n",
    "# 6️  EXPORT ------------------------------------------------------------------\n",
    "cols = [\"FEEDER_ID\", \"FROM_TO\", \"SOURCE_LOCATION\", \"DESTINATION_LOCATION\", \"LOCATION\", \"RANK\", \"LATEST_DT_DATE\", \"DT_LOAD\"]\n",
    "merged.to_csv(OUTPUT_PATH, index=False, columns=cols)\n",
    "print(f\"\\nSaved {len(merged):,} rows -> {OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(merged.head())\n",
    "    except Exception:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16f1ef80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique FEEDER_ID values: 1120\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(OUTPUT_PATH)\n",
    "col = df['FEEDER_ID'].unique()\n",
    "print(\"Unique FEEDER_ID values:\", len(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8e4ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2452f3ac",
   "metadata": {},
   "source": [
    "rank logic new with connected component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd134e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9874e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "► 1. load & prep ht_cleaned.csv\n",
      "► 2. trace feeders\n",
      "    1  FEEDER 15454\n",
      "    100  FEEDER 41897\n",
      "    200  FEEDER 28223\n",
      "    300  FEEDER 39624\n",
      "    400  FEEDER 41709\n",
      "    500  FEEDER 31267\n",
      "    600  FEEDER 28674\n",
      "    700  FEEDER 35873\n",
      "    800  FEEDER 30135\n",
      "    900  FEEDER 18093\n",
      "    1000  FEEDER 30031\n",
      "    1100  FEEDER 3101\n",
      "    1200  FEEDER 35038\n",
      "    1300  FEEDER 30886\n",
      "    1400  FEEDER 19090\n",
      "► 3. merge audit\n",
      "► 4. saved 8,764 rows → /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_SORTED.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20562/16760067.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  kept[\"_RKEY\"] = kept[\"RANK\"].map(_rank_key)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEEDER_ID</th>\n",
       "      <th>FROM_TO</th>\n",
       "      <th>SOURCE_LOCATION</th>\n",
       "      <th>DESTINATION_LOCATION</th>\n",
       "      <th>RANK</th>\n",
       "      <th>LATEST_DT_DATE</th>\n",
       "      <th>DT_LOAD</th>\n",
       "      <th>LOCATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>10205</td>\n",
       "      <td>10205-18556</td>\n",
       "      <td>1S-MH-MU-ZSC-RSTN-AMBI</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2382</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>616.320000</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>10205</td>\n",
       "      <td>18558-18559</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2382</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2383</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>674.080488</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>10205</td>\n",
       "      <td>10205-10634</td>\n",
       "      <td>1S-MH-MU-ZSC-RSTN-AMBI</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2723</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-03-06</td>\n",
       "      <td>473.442574</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>10205</td>\n",
       "      <td>10632-573</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2723</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2074</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2025-01-03</td>\n",
       "      <td>207.080874</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>10205</td>\n",
       "      <td>572-39962</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2074</td>\n",
       "      <td>1S-MH-MU-ZSC-CL06-3404</td>\n",
       "      <td>2.1.1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1S-MH-MU-ZSC-CL06-3404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>10205</td>\n",
       "      <td>39964-39632</td>\n",
       "      <td>1S-MH-MU-ZSC-CL06-3404</td>\n",
       "      <td>1S-MH-MU-ZSC-CL06-3489</td>\n",
       "      <td>2.1.2</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>203.070423</td>\n",
       "      <td>1S-MH-MU-ZSC-CL06-3489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>10205</td>\n",
       "      <td>39633-5821</td>\n",
       "      <td>1S-MH-MU-ZSC-CL06-3489</td>\n",
       "      <td>1S-MH-MU-ZSC-CL06-2088</td>\n",
       "      <td>2.1.3</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>508.627873</td>\n",
       "      <td>1S-MH-MU-ZSC-CL06-2088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>10205</td>\n",
       "      <td>574-39130</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2074</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-3373</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>50.304000</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-3373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>10205</td>\n",
       "      <td>39131-28802</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-3373</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-3058</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2025-03-06</td>\n",
       "      <td>126.298109</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-3058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>10205</td>\n",
       "      <td>28803-32963</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-3058</td>\n",
       "      <td>1S-MH-MU-ZSC-CL08-3241</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>103.742577</td>\n",
       "      <td>1S-MH-MU-ZSC-CL08-3241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>10205</td>\n",
       "      <td>32965-4857</td>\n",
       "      <td>1S-MH-MU-ZSC-CL08-3241</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2185</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2025-03-06</td>\n",
       "      <td>225.762872</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>10205</td>\n",
       "      <td>16951-4853</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2723</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2366</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-03-06</td>\n",
       "      <td>148.855367</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>10205</td>\n",
       "      <td>4855-6089</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2366</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2348</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-03-06</td>\n",
       "      <td>400.698507</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>10205</td>\n",
       "      <td>6088-25212</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2348</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2019</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-03-06</td>\n",
       "      <td>212.974384</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>10206</td>\n",
       "      <td>10206-4804</td>\n",
       "      <td>1S-MH-MU-ZSC-RSTN-AMBI</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2298</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>335.924000</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    FEEDER_ID      FROM_TO         SOURCE_LOCATION    DESTINATION_LOCATION  \\\n",
       "205     10205  10205-18556  1S-MH-MU-ZSC-RSTN-AMBI  1S-MH-MU-ZSC-CL09-2382   \n",
       "206     10205  18558-18559  1S-MH-MU-ZSC-CL09-2382  1S-MH-MU-ZSC-CL09-2383   \n",
       "208     10205  10205-10634  1S-MH-MU-ZSC-RSTN-AMBI  1S-MH-MU-ZSC-CL09-2723   \n",
       "214     10205    10632-573  1S-MH-MU-ZSC-CL09-2723  1S-MH-MU-ZSC-CL09-2074   \n",
       "222     10205    572-39962  1S-MH-MU-ZSC-CL09-2074  1S-MH-MU-ZSC-CL06-3404   \n",
       "223     10205  39964-39632  1S-MH-MU-ZSC-CL06-3404  1S-MH-MU-ZSC-CL06-3489   \n",
       "224     10205   39633-5821  1S-MH-MU-ZSC-CL06-3489  1S-MH-MU-ZSC-CL06-2088   \n",
       "215     10205    574-39130  1S-MH-MU-ZSC-CL09-2074  1S-MH-MU-ZSC-CL09-3373   \n",
       "216     10205  39131-28802  1S-MH-MU-ZSC-CL09-3373  1S-MH-MU-ZSC-CL09-3058   \n",
       "217     10205  28803-32963  1S-MH-MU-ZSC-CL09-3058  1S-MH-MU-ZSC-CL08-3241   \n",
       "218     10205   32965-4857  1S-MH-MU-ZSC-CL08-3241  1S-MH-MU-ZSC-CL09-2185   \n",
       "209     10205   16951-4853  1S-MH-MU-ZSC-CL09-2723  1S-MH-MU-ZSC-CL09-2366   \n",
       "210     10205    4855-6089  1S-MH-MU-ZSC-CL09-2366  1S-MH-MU-ZSC-CL09-2348   \n",
       "211     10205   6088-25212  1S-MH-MU-ZSC-CL09-2348  1S-MH-MU-ZSC-CL09-2019   \n",
       "228     10206   10206-4804  1S-MH-MU-ZSC-RSTN-AMBI  1S-MH-MU-ZSC-CL09-2298   \n",
       "\n",
       "      RANK LATEST_DT_DATE     DT_LOAD                LOCATION  \n",
       "205      1     2025-04-04  616.320000  1S-MH-MU-ZSC-CL09-2382  \n",
       "206      2     2025-04-04  674.080488  1S-MH-MU-ZSC-CL09-2383  \n",
       "208      2     2025-03-06  473.442574  1S-MH-MU-ZSC-CL09-2723  \n",
       "214    2.1     2025-01-03  207.080874  1S-MH-MU-ZSC-CL09-2074  \n",
       "222  2.1.1            NaT         NaN  1S-MH-MU-ZSC-CL06-3404  \n",
       "223  2.1.2     2025-04-04  203.070423  1S-MH-MU-ZSC-CL06-3489  \n",
       "224  2.1.3     2025-04-04  508.627873  1S-MH-MU-ZSC-CL06-2088  \n",
       "215    2.2     2025-04-04   50.304000  1S-MH-MU-ZSC-CL09-3373  \n",
       "216    2.3     2025-03-06  126.298109  1S-MH-MU-ZSC-CL09-3058  \n",
       "217    2.4     2025-04-04  103.742577  1S-MH-MU-ZSC-CL08-3241  \n",
       "218    2.5     2025-03-06  225.762872  1S-MH-MU-ZSC-CL09-2185  \n",
       "209      3     2025-03-06  148.855367  1S-MH-MU-ZSC-CL09-2366  \n",
       "210      4     2025-03-06  400.698507  1S-MH-MU-ZSC-CL09-2348  \n",
       "211      5     2025-03-06  212.974384  1S-MH-MU-ZSC-CL09-2019  \n",
       "228      1     2025-04-04  335.924000  1S-MH-MU-ZSC-CL09-2298  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "feeder_trace_latest_audit_with_rank_updated.py\n",
    "---------------------------------------------\n",
    "\n",
    "Outputs a clean, *single‑component* feeder trace with hierarchical RANK labels,\n",
    "joined to energy‑audit stats and sorted feeder‑by‑feeder by that RANK.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Set, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# bump recursion depth so deep radials do not crash\n",
    "sys.setrecursionlimit(200_000)\n",
    "\n",
    "# ── FILE PATHS ───────────────────────────────────────────────────────────────\n",
    "INPUT_HT = \"/media/sagark24/New Volume/MERGE CDIS/2-Year-data/CLEANED_DATA/ht_cleaned.csv\"\n",
    "INPUT_EN = \"/media/sagark24/New Volume/MERGE CDIS/2-Year-data/CLEANED_DATA/energyaudit_cleaned.csv\"\n",
    "OUTPUT    = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_SORTED.csv\"\n",
    "\n",
    "# debug dumps (unchanged)\n",
    "LOST_IDS_OUT  = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/lost_feeder_ids.csv\"\n",
    "LOST_DATA_OUT = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/lost_feeders_full_data.csv\"\n",
    "\n",
    "# ── COLUMN NAMES ─────────────────────────────────────────────────────────────\n",
    "FEEDER_ID_COL  = \"FEEDERID\"\n",
    "SRC_SWITCH_COL = \"SOURCE_SWITCH_ID\"\n",
    "DST_SWITCH_COL = \"DESTINATION_SWITCH_ID\"\n",
    "SRC_LOC_COL    = \"SOURCE_SSFL\"\n",
    "DST_LOC_COL    = \"DESTINATION_SSFL\"   # ≡ FUNC_LOC in audit\n",
    "\n",
    "FUNC_LOC_COL = \"FUNC_LOC\"             # audit\n",
    "DATE_COL     = \"SYSTEM_DATE\"\n",
    "LOAD_COL     = \"MD_KVA\"\n",
    "\n",
    "edge_cols = [SRC_SWITCH_COL, DST_SWITCH_COL, SRC_LOC_COL, DST_LOC_COL]\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(\"► 1. load & prep ht_cleaned.csv\")\n",
    "ht = (\n",
    "    pd.read_csv(Path(INPUT_HT).expanduser(), low_memory=False)\n",
    "      .drop_duplicates()\n",
    ")\n",
    "\n",
    "def _feeder_token(val) -> Optional[str]:\n",
    "    if not isinstance(val, str):\n",
    "        val = str(val) if val is not None else \"\"\n",
    "    p = val.upper().split(\"_\")\n",
    "    return p[2].lstrip(\"0\") if len(p) >= 3 and p[1] == \"11KV\" else None\n",
    "\n",
    "ht[\"FEEDER_ID\"] = ht[FEEDER_ID_COL].apply(_feeder_token)\n",
    "\n",
    "for c in [SRC_SWITCH_COL, DST_SWITCH_COL, SRC_LOC_COL, DST_LOC_COL]:\n",
    "    ht[c] = ht[c].astype(str)\n",
    "\n",
    "# ── 2. build adjacency (deduped) ─────────────────────────────────────────────\n",
    "def _adjacency(df: pd.DataFrame, fid: str):\n",
    "    sub = df[df[\"FEEDER_ID\"] == fid][edge_cols].drop_duplicates()\n",
    "    adj: Dict[str, List[Tuple]] = defaultdict(list)\n",
    "    for tup in map(tuple, sub.to_records(index=False)):\n",
    "        adj[tup[2]].append(tup)                       # tup[2] = SRC_LOC\n",
    "    for k in adj:                                     # deterministic order\n",
    "        adj[k].sort(key=lambda e: (e[1], e[3]))       # by DEST_SWITCH, DEST_LOC\n",
    "    return adj\n",
    "\n",
    "# ── 3. trace only the component rooted at SOURCE_SWITCH == FEEDER_ID ─────────\n",
    "def trace_feeder(fid: str) -> List[dict]:\n",
    "    adj = _adjacency(ht, fid)\n",
    "\n",
    "    # roots == rows where SOURCE_SWITCH_ID == FEEDER_ID\n",
    "    roots = [\n",
    "        tuple(t) for t in\n",
    "        ht[(ht[SRC_SWITCH_COL] == fid) & (ht[\"FEEDER_ID\"] == fid)]\n",
    "        [edge_cols].to_records(index=False)\n",
    "    ]\n",
    "    if not roots:                   # no such row → feeder is malformed → skip\n",
    "        return []\n",
    "\n",
    "    visited: Set[Tuple] = set()\n",
    "    rows:    List[dict] = []\n",
    "    side_counter: Dict[str, int] = {}\n",
    "    global_idx = 0\n",
    "\n",
    "    def dfs(edge: Tuple, prefix: str, spine: bool):\n",
    "        nonlocal global_idx\n",
    "        if edge in visited:\n",
    "            return                    # loop guard\n",
    "        visited.add(edge)\n",
    "\n",
    "        f_sw, t_sw, s_loc, d_loc = edge\n",
    "\n",
    "        # hierarchical RANK assignment\n",
    "        if prefix == \"\":\n",
    "            global_idx += 1\n",
    "            rank = str(global_idx)\n",
    "        elif spine:\n",
    "            *base, last = map(int, prefix.split(\".\"))\n",
    "            rank = \".\".join([*map(str, base), str(last + 1)]) if base else str(last + 1)\n",
    "        else:\n",
    "            n = side_counter[prefix] = side_counter.get(prefix, 0) + 1\n",
    "            rank = f\"{prefix}.{n}\"\n",
    "\n",
    "        rows.append({\n",
    "            \"FEEDER_ID\": fid,\n",
    "            \"FROM_TO\": f\"{f_sw}-{t_sw}\",\n",
    "            \"SOURCE_LOCATION\": s_loc,\n",
    "            \"DESTINATION_LOCATION\": d_loc,\n",
    "            \"RANK\": rank,\n",
    "        })\n",
    "\n",
    "        kids = adj.get(d_loc, [])\n",
    "        if not kids:\n",
    "            return\n",
    "        first, *rest = kids\n",
    "        dfs(first,  rank, True)       # continue spine\n",
    "        for ch in rest:               # side branches\n",
    "            dfs(ch,  rank, False)\n",
    "\n",
    "    # walk every root (there could be >1 if the feeder splits immediately)\n",
    "    for r in roots:\n",
    "        dfs(r, \"\", True)\n",
    "\n",
    "    return rows\n",
    "\n",
    "# ── 4. collect traces for every feeder ───────────────────────────────────────\n",
    "print(\"► 2. trace feeders\")\n",
    "traces: List[dict] = []\n",
    "for i, fid in enumerate(ht[\"FEEDER_ID\"].dropna().unique(), 1):\n",
    "    if i % 100 == 0 or i == 1:\n",
    "        print(f\"    {i}  FEEDER {fid}\")\n",
    "    traces.extend(trace_feeder(str(fid)))\n",
    "\n",
    "trace_df = pd.DataFrame(traces)\n",
    "\n",
    "# ── 5. merge energy‑audit stats ──────────────────────────────────────────────\n",
    "print(\"► 3. merge audit\")\n",
    "audit = pd.read_csv(Path(INPUT_EN).expanduser(),\n",
    "                    low_memory=False,\n",
    "                    parse_dates=[DATE_COL])\n",
    "audit.columns = [c.upper() for c in audit.columns]\n",
    "audit[DATE_COL] = pd.to_datetime(audit[DATE_COL], errors=\"coerce\")\n",
    "agg = (audit[[FUNC_LOC_COL, DATE_COL, LOAD_COL]]\n",
    "          .dropna(subset=[FUNC_LOC_COL])\n",
    "          .groupby(FUNC_LOC_COL)\n",
    "          .agg(LATEST_DT_DATE=(DATE_COL, \"max\"), DT_LOAD=(LOAD_COL, \"mean\"))\n",
    "          .reset_index())\n",
    "agg[FUNC_LOC_COL] = agg[FUNC_LOC_COL].astype(str)\n",
    "\n",
    "merged = (trace_df\n",
    "          .merge(agg, how=\"left\",\n",
    "                 left_on=\"DESTINATION_LOCATION\",\n",
    "                 right_on=FUNC_LOC_COL)\n",
    "          .drop(columns=[FUNC_LOC_COL]))\n",
    "merged[\"LATEST_DT_DATE\"] = pd.to_datetime(merged[\"LATEST_DT_DATE\"]).dt.date\n",
    "merged[\"LOCATION\"] = merged[\"DESTINATION_LOCATION\"]\n",
    "\n",
    "# ── 6. keep only FROM_TO = digits‑digits & write debug info ─────────────────\n",
    "def _digdig(s): return bool(re.fullmatch(r\"\\d+-\\d+\", str(s)))\n",
    "mask = merged[\"FROM_TO\"].apply(_digdig)\n",
    "kept  = merged[mask]\n",
    "dropped = merged[~mask]\n",
    "\n",
    "lost = sorted(set(merged[\"FEEDER_ID\"]) - set(kept[\"FEEDER_ID\"]))\n",
    "pd.Series(lost, name=\"LOST_FEEDER_ID\").to_csv(LOST_IDS_OUT, index=False)\n",
    "dropped.to_csv(LOST_DATA_OUT, index=False)\n",
    "\n",
    "# # ── 7. sort by RANK within each feeder ───────────────────────────────────────\n",
    "def _rank_key(r: str) -> Tuple[int, ...]:\n",
    "    return tuple(int(x) for x in r.split(\".\"))\n",
    "\n",
    "kept[\"_RKEY\"] = kept[\"RANK\"].map(_rank_key)\n",
    "kept = kept.sort_values(by=[\"FEEDER_ID\", \"_RKEY\"]).drop(columns=\"_RKEY\")\n",
    "\n",
    "\n",
    "# ── 8. export CSV ────────────────────────────────────────────────────────────\n",
    "cols_out = [\"FEEDER_ID\", \"FROM_TO\",\n",
    "            \"SOURCE_LOCATION\", \"DESTINATION_LOCATION\", \"LOCATION\",\n",
    "            \"RANK\", \"LATEST_DT_DATE\", \"DT_LOAD\"]\n",
    "kept.to_csv(OUTPUT, index=False, columns=cols_out)\n",
    "print(f\"► 4. saved {len(kept):,} rows → {OUTPUT}\")\n",
    "\n",
    "if __name__ == \"__main__\":          # show a quick peek when run in notebooks\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(kept.head(15))\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65bb20c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. load & prep ht_cleaned.csv\n",
      " 2. trace feeders\n",
      "    1. FEEDER 15454\n",
      "    100. FEEDER 41897\n",
      "    200. FEEDER 28223\n",
      "    300. FEEDER 39624\n",
      "    400. FEEDER 41709\n",
      "    500. FEEDER 31267\n",
      "    600. FEEDER 28674\n",
      "    700. FEEDER 35873\n",
      "    800. FEEDER 30135\n",
      "    900. FEEDER 18093\n",
      "    1000. FEEDER 30031\n",
      "    1100. FEEDER 3101\n",
      "    1200. FEEDER 35038\n",
      "    1300. FEEDER 30886\n",
      "    1400. FEEDER 19090\n",
      "► . merge audit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_185003/967037875.py:259: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  kept[\"_RKEY\"] = kept[\"RANK\"].map(_rk)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "► 4. saved 8,584 rows → /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/11_WITHOUTDT_CONNECTED_RANKED.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_185003/967037875.py:264: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(_renumber_ranks)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEEDER_ID</th>\n",
       "      <th>FROM_TO</th>\n",
       "      <th>SOURCE_LOCATION</th>\n",
       "      <th>DESTINATION_LOCATION</th>\n",
       "      <th>RANK</th>\n",
       "      <th>LATEST_DT_DATE</th>\n",
       "      <th>DT_LOAD</th>\n",
       "      <th>LOCATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>10205</td>\n",
       "      <td>10205-18556</td>\n",
       "      <td>1S-MH-MU-ZSC-RSTN-AMBI</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2382</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>616.320000</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>10205</td>\n",
       "      <td>18558-18559</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2382</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2383</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>674.080488</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>10205</td>\n",
       "      <td>10205-10634</td>\n",
       "      <td>1S-MH-MU-ZSC-RSTN-AMBI</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2723</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-03-06</td>\n",
       "      <td>473.442574</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>10205</td>\n",
       "      <td>16951-4853</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2723</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2366</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2025-03-06</td>\n",
       "      <td>148.855367</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>10205</td>\n",
       "      <td>4855-6089</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2366</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2348</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2025-03-06</td>\n",
       "      <td>400.698507</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>10205</td>\n",
       "      <td>6088-25212</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2348</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2019</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2025-03-06</td>\n",
       "      <td>212.974384</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>10205</td>\n",
       "      <td>10632-573</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2723</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2074</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-01-03</td>\n",
       "      <td>207.080874</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>10205</td>\n",
       "      <td>572-39962</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2074</td>\n",
       "      <td>1S-MH-MU-ZSC-CL06-3404</td>\n",
       "      <td>4.1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1S-MH-MU-ZSC-CL06-3404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>10205</td>\n",
       "      <td>39964-39632</td>\n",
       "      <td>1S-MH-MU-ZSC-CL06-3404</td>\n",
       "      <td>1S-MH-MU-ZSC-CL06-3489</td>\n",
       "      <td>4.2</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>203.070423</td>\n",
       "      <td>1S-MH-MU-ZSC-CL06-3489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>10205</td>\n",
       "      <td>39633-5821</td>\n",
       "      <td>1S-MH-MU-ZSC-CL06-3489</td>\n",
       "      <td>1S-MH-MU-ZSC-CL06-2088</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>508.627873</td>\n",
       "      <td>1S-MH-MU-ZSC-CL06-2088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>10205</td>\n",
       "      <td>574-39130</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2074</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-3373</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>50.304000</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-3373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>10205</td>\n",
       "      <td>39131-28802</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-3373</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-3058</td>\n",
       "      <td>6</td>\n",
       "      <td>2025-03-06</td>\n",
       "      <td>126.298109</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-3058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>10205</td>\n",
       "      <td>28803-32963</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-3058</td>\n",
       "      <td>1S-MH-MU-ZSC-CL08-3241</td>\n",
       "      <td>7</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>103.742577</td>\n",
       "      <td>1S-MH-MU-ZSC-CL08-3241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>10205</td>\n",
       "      <td>32965-4857</td>\n",
       "      <td>1S-MH-MU-ZSC-CL08-3241</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2185</td>\n",
       "      <td>8</td>\n",
       "      <td>2025-03-06</td>\n",
       "      <td>225.762872</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>10206</td>\n",
       "      <td>10206-4804</td>\n",
       "      <td>1S-MH-MU-ZSC-RSTN-AMBI</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2298</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>335.924000</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    FEEDER_ID      FROM_TO         SOURCE_LOCATION    DESTINATION_LOCATION  \\\n",
       "119     10205  10205-18556  1S-MH-MU-ZSC-RSTN-AMBI  1S-MH-MU-ZSC-CL09-2382   \n",
       "120     10205  18558-18559  1S-MH-MU-ZSC-CL09-2382  1S-MH-MU-ZSC-CL09-2383   \n",
       "121     10205  10205-10634  1S-MH-MU-ZSC-RSTN-AMBI  1S-MH-MU-ZSC-CL09-2723   \n",
       "130     10205   16951-4853  1S-MH-MU-ZSC-CL09-2723  1S-MH-MU-ZSC-CL09-2366   \n",
       "131     10205    4855-6089  1S-MH-MU-ZSC-CL09-2366  1S-MH-MU-ZSC-CL09-2348   \n",
       "132     10205   6088-25212  1S-MH-MU-ZSC-CL09-2348  1S-MH-MU-ZSC-CL09-2019   \n",
       "122     10205    10632-573  1S-MH-MU-ZSC-CL09-2723  1S-MH-MU-ZSC-CL09-2074   \n",
       "127     10205    572-39962  1S-MH-MU-ZSC-CL09-2074  1S-MH-MU-ZSC-CL06-3404   \n",
       "128     10205  39964-39632  1S-MH-MU-ZSC-CL06-3404  1S-MH-MU-ZSC-CL06-3489   \n",
       "129     10205   39633-5821  1S-MH-MU-ZSC-CL06-3489  1S-MH-MU-ZSC-CL06-2088   \n",
       "123     10205    574-39130  1S-MH-MU-ZSC-CL09-2074  1S-MH-MU-ZSC-CL09-3373   \n",
       "124     10205  39131-28802  1S-MH-MU-ZSC-CL09-3373  1S-MH-MU-ZSC-CL09-3058   \n",
       "125     10205  28803-32963  1S-MH-MU-ZSC-CL09-3058  1S-MH-MU-ZSC-CL08-3241   \n",
       "126     10205   32965-4857  1S-MH-MU-ZSC-CL08-3241  1S-MH-MU-ZSC-CL09-2185   \n",
       "133     10206   10206-4804  1S-MH-MU-ZSC-RSTN-AMBI  1S-MH-MU-ZSC-CL09-2298   \n",
       "\n",
       "    RANK LATEST_DT_DATE     DT_LOAD                LOCATION  \n",
       "119    1     2025-04-04  616.320000  1S-MH-MU-ZSC-CL09-2382  \n",
       "120    2     2025-04-04  674.080488  1S-MH-MU-ZSC-CL09-2383  \n",
       "121    3     2025-03-06  473.442574  1S-MH-MU-ZSC-CL09-2723  \n",
       "130  3.1     2025-03-06  148.855367  1S-MH-MU-ZSC-CL09-2366  \n",
       "131  3.2     2025-03-06  400.698507  1S-MH-MU-ZSC-CL09-2348  \n",
       "132  3.3     2025-03-06  212.974384  1S-MH-MU-ZSC-CL09-2019  \n",
       "122    4     2025-01-03  207.080874  1S-MH-MU-ZSC-CL09-2074  \n",
       "127  4.1            NaT         NaN  1S-MH-MU-ZSC-CL06-3404  \n",
       "128  4.2     2025-04-04  203.070423  1S-MH-MU-ZSC-CL06-3489  \n",
       "129  4.3     2025-04-04  508.627873  1S-MH-MU-ZSC-CL06-2088  \n",
       "123    5     2025-04-04   50.304000  1S-MH-MU-ZSC-CL09-3373  \n",
       "124    6     2025-03-06  126.298109  1S-MH-MU-ZSC-CL09-3058  \n",
       "125    7     2025-04-04  103.742577  1S-MH-MU-ZSC-CL08-3241  \n",
       "126    8     2025-03-06  225.762872  1S-MH-MU-ZSC-CL09-2185  \n",
       "133    1     2025-04-04  335.924000  1S-MH-MU-ZSC-CL09-2298  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "feeder_trace_latest_audit_with_rank_updated.py\n",
    "---------------------------------------------\n",
    "\n",
    "Safer, non‑recursive feeder trace with hierarchical RANK labels\n",
    "(no gaps, longest‐chain first), joined to energy‑audit stats.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import sys, re\n",
    "from collections import defaultdict, deque\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Set, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# bump recursion limit just in case pandas etc. need it\n",
    "sys.setrecursionlimit(200_000)\n",
    "\n",
    "# ── FILE PATHS ───────────────────────────────────────────────────────────────\n",
    "INPUT_HT      = \"/media/sagark24/New Volume/MERGE CDIS/2-Year-data/CLEANED_DATA/ht_cleaned.csv\"\n",
    "INPUT_EN      = \"/media/sagark24/New Volume/MERGE CDIS/2-Year-data/CLEANED_DATA/energyaudit_cleaned.csv\"\n",
    "OUTPUT        = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/11_WITHOUTDT_CONNECTED_RANKED.csv\"\n",
    "LOST_IDS_OUT  = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/lost_feeder_ids.csv\"\n",
    "LOST_DATA_OUT = \"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/lost_feeders_full_data.csv\"\n",
    "\n",
    "# ── COLUMN NAMES ─────────────────────────────────────────────────────────────\n",
    "FEEDER_ID_COL  = \"FEEDERID\"\n",
    "SRC_SWITCH_COL = \"SOURCE_SWITCH_ID\"\n",
    "DST_SWITCH_COL = \"DESTINATION_SWITCH_ID\"\n",
    "SRC_LOC_COL    = \"SOURCE_SSFL\"\n",
    "DST_LOC_COL    = \"DESTINATION_SSFL\"       # ≡ FUNC_LOC\n",
    "FUNC_LOC_COL   = \"FUNC_LOC\"\n",
    "DATE_COL       = \"SYSTEM_DATE\"\n",
    "LOAD_COL       = \"MD_KVA\"\n",
    "\n",
    "edge_cols = [SRC_SWITCH_COL, DST_SWITCH_COL, SRC_LOC_COL, DST_LOC_COL]\n",
    "\n",
    "\n",
    "# ── 1. load & prep ht_cleaned.csv ─────────────────────────────────────────────\n",
    "print(\"1. load & prep ht_cleaned.csv\")\n",
    "ht = (\n",
    "    pd.read_csv(Path(INPUT_HT).expanduser(), low_memory=False)\n",
    "      .drop_duplicates()\n",
    ")\n",
    "\n",
    "def _feeder_token(val) -> Optional[str]:\n",
    "    if not isinstance(val, str):\n",
    "        val = str(val) if val is not None else \"\"\n",
    "    p = val.upper().split(\"_\")\n",
    "    return p[2].lstrip(\"0\") if len(p) >= 3 and p[1] == \"11KV\" else None\n",
    "\n",
    "ht[\"FEEDER_ID\"] = ht[FEEDER_ID_COL].apply(_feeder_token)\n",
    "for c in edge_cols:\n",
    "    ht[c] = ht[c].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "def trace_feeder(fid: str) -> List[dict]:\n",
    "    # 1) grab only this feeder’s edges\n",
    "    sub = ht[ht[\"FEEDER_ID\"] == fid][edge_cols].drop_duplicates()\n",
    "    if sub.empty:\n",
    "        return []\n",
    "\n",
    "    # 2) build directed adjacency and reverse‐adjacency\n",
    "    edges: List[Tuple[str,str,str,str]] = [\n",
    "        tuple(r) for r in sub.itertuples(index=False, name=None)\n",
    "    ]\n",
    "    adj: Dict[str, List[Tuple]] = defaultdict(list)\n",
    "    # rev_adj: Dict[str, List[str]]  = defaultdict(list)\n",
    "    for src_sw, dst_sw, src_loc, dst_loc in edges:\n",
    "        adj[src_loc].append((src_sw, dst_sw, src_loc, dst_loc))\n",
    "        # rev_adj[dst_loc].append(src_loc)\n",
    "\n",
    "    # 3) detect root‐edges (SOURCE_SWITCH == feeder_id)\n",
    "    roots = [e for e in edges if e[0] == fid]\n",
    "    # if not roots:\n",
    "    #     # fallback → nodes with zero in‑degree\n",
    "    #     indeg: Dict[str,int] = defaultdict(int)\n",
    "    #     nodes: Set[str] = set()\n",
    "    #     for _,_,s_loc,d_loc in edges:\n",
    "    #         nodes |= {s_loc, d_loc}\n",
    "    #         indeg[d_loc] += 1\n",
    "    #         indeg.setdefault(s_loc, 0)\n",
    "    #     zeros = [n for n in nodes if indeg[n] == 0]\n",
    "    #     if zeros:\n",
    "    #         roots = [e for e in edges if e[2] in zeros]\n",
    "    #     else:\n",
    "    #         # truly cyclical/disconnected → just pick the first edge\n",
    "    #         roots = [edges[0]]\n",
    "\n",
    "    # 4) prune to the *reachable*, *acyclic* subgraph\n",
    "    pruned: Dict[str, List[Tuple]] = defaultdict(list)\n",
    "    visited_locs: Set[str] = set()\n",
    "    for _,_,start_loc,_ in roots:\n",
    "        stack = [start_loc]\n",
    "        while stack:\n",
    "            loc = stack.pop()\n",
    "            if loc in visited_locs:\n",
    "                continue\n",
    "            visited_locs.add(loc)\n",
    "            for edge in adj.get(loc, []):\n",
    "                _,_,_,child = edge\n",
    "                if child in visited_locs:\n",
    "                    # skipping would‐be cycle\n",
    "                    continue\n",
    "                pruned[loc].append(edge)\n",
    "                stack.append(child)\n",
    "\n",
    "    # 5) compute subtree‐depth via Kahn’s algorithm on pruned DAG\n",
    "    all_nodes = set(pruned) | {e[3] for edges in pruned.values() for e in edges}\n",
    "    out_deg = {n: len(pruned.get(n, [])) for n in all_nodes}\n",
    "    rev2: Dict[str,List[str]] = defaultdict(list)\n",
    "    for parent, child_edges in pruned.items():\n",
    "        for *_, child in child_edges:\n",
    "            rev2[child].append(parent)\n",
    "\n",
    "    depth: Dict[str,int] = {}\n",
    "    q = deque([n for n,d in out_deg.items() if d == 0])\n",
    "    for leaf in q:\n",
    "        depth[leaf] = 1\n",
    "    while q:\n",
    "        node = q.popleft()\n",
    "        for parent in rev2.get(node, []):\n",
    "            nd = depth[node] + 1\n",
    "            if depth.get(parent, 0) < nd:\n",
    "                depth[parent] = nd\n",
    "            out_deg[parent] -= 1\n",
    "            if out_deg[parent] == 0:\n",
    "                q.append(parent)\n",
    "\n",
    "    # 6) explicit‐stack traversal: longest‐chain first\n",
    "    rows: List[dict] = []\n",
    "    side_cnt: Dict[str,int] = defaultdict(int)\n",
    "    global_cnt = 0\n",
    "    stack: List[Tuple[Tuple,str,bool]] = [\n",
    "        (r, \"\", True) for r in reversed(roots)\n",
    "    ]\n",
    "\n",
    "    while stack:\n",
    "        edge, prefix, spine = stack.pop()\n",
    "        src_sw, dst_sw, src_loc, dst_loc = edge\n",
    "\n",
    "        # assign temporary (possibly‐gappy) rank\n",
    "        if prefix == \"\":\n",
    "            global_cnt += 1\n",
    "            rank_t = str(global_cnt)\n",
    "        elif spine:\n",
    "            *parts, last = prefix.split(\".\")\n",
    "            rank_t = \".\".join([*parts, str(int(last) + 1)]) if parts else str(int(last) + 1)\n",
    "        else:\n",
    "            side_cnt[prefix] += 1\n",
    "            rank_t = f\"{prefix}.{side_cnt[prefix]}\"\n",
    "\n",
    "        rows.append({\n",
    "            \"FEEDER_ID\": fid,\n",
    "            \"FROM_TO\": f\"{src_sw}-{dst_sw}\",\n",
    "            \"SOURCE_LOCATION\": src_loc,\n",
    "            \"DESTINATION_LOCATION\": dst_loc,\n",
    "            \"RANK\": rank_t,\n",
    "        })\n",
    "\n",
    "        # get pruned children, sort by depth desc\n",
    "        kids = pruned.get(dst_loc, [])\n",
    "        if not kids:\n",
    "            continue\n",
    "        kids_sorted = sorted(\n",
    "            kids,\n",
    "            key=lambda e: depth.get(e[3], 1),\n",
    "            reverse=True\n",
    "        )\n",
    "        # push side‐branches *first* (so they execute *after* the spine)\n",
    "        for side in reversed(kids_sorted[1:]):\n",
    "            stack.append((side, rank_t, False))\n",
    "        # then the main trunk\n",
    "        stack.append((kids_sorted[0], rank_t, True))\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ── 4. collect traces ────────────────────────────────────────────────────────\n",
    "print(\" 2. trace feeders\")\n",
    "all_traces: List[dict] = []\n",
    "for i, fid in enumerate(ht[\"FEEDER_ID\"].dropna().unique(), 1):\n",
    "    if i == 1 or i % 100 == 0:\n",
    "        print(f\"    {i}. FEEDER {fid}\")\n",
    "    all_traces.extend(trace_feeder(str(fid)))\n",
    "\n",
    "trace_df = pd.DataFrame(all_traces)\n",
    "\n",
    "\n",
    "# ── 5. merge audit stats ─────────────────────────────────────────────────────\n",
    "print(\"► . merge audit\")\n",
    "audit = pd.read_csv(Path(INPUT_EN).expanduser(),\n",
    "                    low_memory=False,\n",
    "                    parse_dates=[DATE_COL])\n",
    "audit.columns = [c.upper() for c in audit.columns]\n",
    "audit[DATE_COL] = pd.to_datetime(audit[DATE_COL], errors=\"coerce\")\n",
    "\n",
    "agg = (\n",
    "    audit[[FUNC_LOC_COL, DATE_COL, LOAD_COL]]\n",
    "      .dropna(subset=[FUNC_LOC_COL])\n",
    "      .groupby(FUNC_LOC_COL)\n",
    "      .agg(LATEST_DT_DATE=(DATE_COL, \"max\"),\n",
    "           DT_LOAD=(LOAD_COL, \"mean\"))\n",
    "      .reset_index()\n",
    ")\n",
    "agg[FUNC_LOC_COL] = agg[FUNC_LOC_COL].astype(str)\n",
    "\n",
    "merged = (\n",
    "    trace_df\n",
    "      .merge(agg, how=\"left\",\n",
    "             left_on=\"DESTINATION_LOCATION\",\n",
    "             right_on=FUNC_LOC_COL)\n",
    "      .drop(columns=[FUNC_LOC_COL])\n",
    ")\n",
    "merged[\"LATEST_DT_DATE\"] = pd.to_datetime(merged[\"LATEST_DT_DATE\"]).dt.date\n",
    "merged[\"LOCATION\"]        = merged[\"DESTINATION_LOCATION\"]\n",
    "\n",
    "\n",
    "# ── 6. filter valid FROM_TO, debug dumps ────────────────────────────────────\n",
    "def _digdig(x): return bool(re.fullmatch(r\"\\d+-\\d+\", str(x)))\n",
    "mask    = merged[\"FROM_TO\"].apply(_digdig)\n",
    "kept    = merged[mask]\n",
    "dropped = merged[~mask]\n",
    "\n",
    "lost = sorted(set(merged[\"FEEDER_ID\"]) - set(kept[\"FEEDER_ID\"]))\n",
    "pd.Series(lost, name=\"LOST_FEEDER_ID\").to_csv(LOST_IDS_OUT, index=False)\n",
    "dropped.to_csv(LOST_DATA_OUT, index=False)\n",
    "\n",
    "\n",
    "# ── 7A. gap‑free renumber per feeder ──────────────────────────────────────────\n",
    "def _renumber_ranks(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    mapping: Dict[str,str]        = {}\n",
    "    counters: Dict[str,int]       = defaultdict(int)\n",
    "    new_ranks: List[str]          = []\n",
    "\n",
    "    for old in df[\"RANK\"]:\n",
    "        prefix_old = \".\".join(old.split(\".\")[:-1])\n",
    "        prefix_new = mapping.get(prefix_old, \"\")\n",
    "        counters[prefix_new] += 1\n",
    "        idx = counters[prefix_new]\n",
    "        new = f\"{prefix_new}.{idx}\" if prefix_new else str(idx)\n",
    "        mapping[old] = new\n",
    "        new_ranks.append(new)\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"RANK\"] = new_ranks\n",
    "    return out\n",
    "\n",
    "# ── 7. final sort & renumber ─────────────────────────────────────────────────\n",
    "def _rk(r: str) -> Tuple[int,...]:\n",
    "    return tuple(int(x) for x in r.split(\".\"))\n",
    "\n",
    "kept[\"_RKEY\"] = kept[\"RANK\"].map(_rk)\n",
    "kept = (\n",
    "    kept.sort_values([\"FEEDER_ID\",\"_RKEY\"])\n",
    "        .drop(columns=\"_RKEY\")\n",
    "        .groupby(\"FEEDER_ID\", group_keys=False)\n",
    "        .apply(_renumber_ranks)\n",
    ")\n",
    "\n",
    "\n",
    "# ── 8. export ────────────────────────────────────────────────────────────────\n",
    "OUT_COLS = [\n",
    "    \"FEEDER_ID\",\"FROM_TO\",\n",
    "    \"SOURCE_LOCATION\",\"DESTINATION_LOCATION\",\"LOCATION\",\n",
    "    \"RANK\",\"LATEST_DT_DATE\",\"DT_LOAD\"\n",
    "]\n",
    "kept.to_csv(OUTPUT, index=False, columns=OUT_COLS)\n",
    "print(f\"► 4. saved {len(kept):,} rows → {OUTPUT}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(kept.head(15))\n",
    "    except ImportError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09805276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "518670bd",
   "metadata": {},
   "source": [
    "NEW CODE EWMOVW DUPICATE ROWS AND HANDLE DTATA TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eafe7d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. load + clean HT\n",
      "2. trace feeders\n",
      "   1. FEEDER 15454\n",
      "   100. FEEDER 41897\n",
      "   200. FEEDER 28223\n",
      "   300. FEEDER 39624\n",
      "   400. FEEDER 41709\n",
      "   500. FEEDER 31267\n",
      "   600. FEEDER 28674\n",
      "   700. FEEDER 35873\n",
      "   800. FEEDER 30135\n",
      "   900. FEEDER 18093\n",
      "   1000. FEEDER 30031\n",
      "   1100. FEEDER 3101\n",
      "   1200. FEEDER 35038\n",
      "   1300. FEEDER 30886\n",
      "   1400. FEEDER 19090\n",
      "3. merge energy audit\n",
      " saved 8,481 rows → /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/11_WITHOUTDT_CONNECTED_RANKED.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEEDER_ID</th>\n",
       "      <th>FROM_TO</th>\n",
       "      <th>SOURCE_LOCATION</th>\n",
       "      <th>DESTINATION_LOCATION</th>\n",
       "      <th>RANK</th>\n",
       "      <th>FROM_SWITCH</th>\n",
       "      <th>TO_SWITCH</th>\n",
       "      <th>LATEST_DT_DATE</th>\n",
       "      <th>DT_LOAD</th>\n",
       "      <th>LOCATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>10205</td>\n",
       "      <td>10205-18556</td>\n",
       "      <td>1S-MH-MU-ZSC-RSTN-AMBI</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2382</td>\n",
       "      <td>1</td>\n",
       "      <td>10205</td>\n",
       "      <td>18556</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>616.320000</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>10205</td>\n",
       "      <td>18558-18559</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2382</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2383</td>\n",
       "      <td>2</td>\n",
       "      <td>18558</td>\n",
       "      <td>18559</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>674.080488</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>10205</td>\n",
       "      <td>10205-10634</td>\n",
       "      <td>1S-MH-MU-ZSC-RSTN-AMBI</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2723</td>\n",
       "      <td>2</td>\n",
       "      <td>10205</td>\n",
       "      <td>10634</td>\n",
       "      <td>2025-03-06</td>\n",
       "      <td>473.442574</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>10205</td>\n",
       "      <td>16951-4853</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2723</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2366</td>\n",
       "      <td>2.1</td>\n",
       "      <td>16951</td>\n",
       "      <td>4853</td>\n",
       "      <td>2025-03-06</td>\n",
       "      <td>148.855367</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>10205</td>\n",
       "      <td>4855-6089</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2366</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2348</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4855</td>\n",
       "      <td>6089</td>\n",
       "      <td>2025-03-06</td>\n",
       "      <td>400.698507</td>\n",
       "      <td>1S-MH-MU-ZSC-CL09-2348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    FEEDER_ID      FROM_TO         SOURCE_LOCATION    DESTINATION_LOCATION  \\\n",
       "119     10205  10205-18556  1S-MH-MU-ZSC-RSTN-AMBI  1S-MH-MU-ZSC-CL09-2382   \n",
       "120     10205  18558-18559  1S-MH-MU-ZSC-CL09-2382  1S-MH-MU-ZSC-CL09-2383   \n",
       "121     10205  10205-10634  1S-MH-MU-ZSC-RSTN-AMBI  1S-MH-MU-ZSC-CL09-2723   \n",
       "130     10205   16951-4853  1S-MH-MU-ZSC-CL09-2723  1S-MH-MU-ZSC-CL09-2366   \n",
       "131     10205    4855-6089  1S-MH-MU-ZSC-CL09-2366  1S-MH-MU-ZSC-CL09-2348   \n",
       "\n",
       "    RANK FROM_SWITCH TO_SWITCH LATEST_DT_DATE     DT_LOAD  \\\n",
       "119    1       10205     18556     2025-04-04  616.320000   \n",
       "120    2       18558     18559     2025-04-04  674.080488   \n",
       "121    2       10205     10634     2025-03-06  473.442574   \n",
       "130  2.1       16951      4853     2025-03-06  148.855367   \n",
       "131  2.2        4855      6089     2025-03-06  400.698507   \n",
       "\n",
       "                   LOCATION  \n",
       "119  1S-MH-MU-ZSC-CL09-2382  \n",
       "120  1S-MH-MU-ZSC-CL09-2383  \n",
       "121  1S-MH-MU-ZSC-CL09-2723  \n",
       "130  1S-MH-MU-ZSC-CL09-2366  \n",
       "131  1S-MH-MU-ZSC-CL09-2348  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ONE-CELL PIPELINE  ─────────────────────────────────────────────────────────\n",
    "from __future__ import annotations\n",
    "import sys, re\n",
    "from collections import defaultdict, deque\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Set\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sys.setrecursionlimit(200_000)\n",
    "\n",
    "# ───────────────────────── CONFIG ──────────────────────────────────────────\n",
    "BASE = \"/media/sagark24/New Volume/MERGE CDIS\"\n",
    "\n",
    "INPUT_HT      = f\"{BASE}/2-Year-data/CLEANED_DATA/ht_cleaned.csv\"\n",
    "INPUT_EN      = f\"{BASE}/2-Year-data/CLEANED_DATA/energyaudit_cleaned.csv\"\n",
    "OUTPUT        = f\"{BASE}/IPYNB_FILE/DATA_GENERATION/11_KV_FEEDER_ALL DATA/11_WITHOUTDT_CONNECTED_RANKED.csv\"\n",
    "LOST_IDS_OUT  = f\"{BASE}/IPYNB_FILE/DATA_GENERATION/lost_feeder_ids.csv\"\n",
    "LOST_DATA_OUT = f\"{BASE}/IPYNB_FILE/DATA_GENERATION/lost_feeders_full_data.csv\"\n",
    "\n",
    "FEEDER_ID_COL  = \"FEEDERID\"\n",
    "SRC_SWITCH_COL = \"SOURCE_SWITCH_ID\"\n",
    "DST_SWITCH_COL = \"DESTINATION_SWITCH_ID\"\n",
    "SRC_LOC_COL    = \"SOURCE_SSFL\"\n",
    "DST_LOC_COL    = \"DESTINATION_SSFL\"\n",
    "FUNC_LOC_COL   = \"FUNC_LOC\"\n",
    "DATE_COL       = \"SYSTEM_DATE\"\n",
    "LOAD_COL       = \"MD_KVA\"\n",
    "edge_cols      = [SRC_SWITCH_COL, DST_SWITCH_COL, SRC_LOC_COL, DST_LOC_COL]\n",
    "\n",
    "# ─────────────────────── helper: canonicalise ALL string cols ──────────────\n",
    "def canon_all(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    obj_cols = df.select_dtypes(\"object\").columns\n",
    "    for c in obj_cols:\n",
    "        df[c] = (df[c].astype(str)          # make sure dtype=object\n",
    "                       .str.strip()          # remove leading/trailing spaces\n",
    "                       .str.upper()        # <-- uncomment for case-insensitive\n",
    "                )\n",
    "    return df\n",
    "\n",
    "# ─────────────────────── helper: 11 kV feeder token  ───────────────────────\n",
    "def feeder_token(val) -> str | None:\n",
    "    s = str(val).upper().split(\"_\")\n",
    "    return s[2].lstrip(\"0\") if len(s) >= 3 and s[1] == \"11KV\" else None\n",
    "\n",
    "# ─────────────────────── load HT cleaned  ──────────────────────────────────\n",
    "print(\"1. load + clean HT\")\n",
    "ht = canon_all(pd.read_csv(INPUT_HT, low_memory=False)).drop_duplicates()\n",
    "ht[\"FEEDER_ID\"] = ht[FEEDER_ID_COL].apply(feeder_token)\n",
    "\n",
    "\n",
    "# ── 2) TRACE A SINGLE FEEDER → rows list ──────────────────────────────────\n",
    "def trace_feeder(fid: str) -> List[dict]:\n",
    "    sub = ht[ht[\"FEEDER_ID\"] == fid][edge_cols].drop_duplicates()\n",
    "    if sub.empty:\n",
    "        return []\n",
    "\n",
    "    # build adjacency\n",
    "    edges: List[Tuple[str, str, str, str]] = [tuple(r) for r in sub.itertuples(index=False, name=None)]\n",
    "    adj: Dict[str, List[Tuple]] = defaultdict(list)\n",
    "    rev_adj: Dict[str, List[str]] = defaultdict(list)\n",
    "    for s_sw, d_sw, s_loc, d_loc in edges:\n",
    "        adj[s_loc].append((s_sw, d_sw, s_loc, d_loc))\n",
    "        rev_adj[d_loc].append(s_loc)\n",
    "\n",
    "    # pick roots whose SOURCE_SWITCH == feeder-id\n",
    "    roots_raw = [e for e in edges if e[0] == fid]\n",
    "    roots     = list({e[1]: e for e in roots_raw}.values()) or [edges[0]]\n",
    "\n",
    "    # prune to reachable DAG (skip cycles)\n",
    "    pruned: Dict[str, List[Tuple]] = defaultdict(list)\n",
    "    visited: Set[str] = set()\n",
    "    for _, _, start_loc, _ in roots:\n",
    "        stack = [start_loc]\n",
    "        while stack:\n",
    "            loc = stack.pop()\n",
    "            if loc in visited:\n",
    "                continue\n",
    "            visited.add(loc)\n",
    "            for edge in adj.get(loc, []):\n",
    "                _, _, _, child = edge\n",
    "                if child in visited:\n",
    "                    continue\n",
    "                pruned[loc].append(edge)\n",
    "                stack.append(child)\n",
    "\n",
    "    # compute depth for trunk-first ordering\n",
    "    all_nodes = set(pruned) | {e[3] for edges in pruned.values() for e in edges}\n",
    "    out_deg = {n: len(pruned.get(n, [])) for n in all_nodes}\n",
    "    rev2: Dict[str, List[str]] = defaultdict(list)\n",
    "    for parent, child_edges in pruned.items():\n",
    "        for *_, child in child_edges:\n",
    "            rev2[child].append(parent)\n",
    "\n",
    "    depth: Dict[str, int] = {}\n",
    "    q = deque([n for n, d in out_deg.items() if d == 0])\n",
    "    for leaf in q:\n",
    "        depth[leaf] = 1\n",
    "    while q:\n",
    "        node = q.popleft()\n",
    "        for parent in rev2.get(node, []):\n",
    "            nd = depth[node] + 1\n",
    "            if depth.get(parent, 0) < nd:\n",
    "                depth[parent] = nd\n",
    "            out_deg[parent] -= 1\n",
    "            if out_deg[parent] == 0:\n",
    "                q.append(parent)\n",
    "\n",
    "    rows: List[dict] = []\n",
    "    side_cnt: Dict[str, int] = defaultdict(int)\n",
    "    global_cnt = 0\n",
    "    stack: List[Tuple[Tuple, str, bool]] = [(r, \"\", True) for r in reversed(roots)]\n",
    "\n",
    "    while stack:\n",
    "        edge, prefix, spine = stack.pop()\n",
    "        src_sw, dst_sw, src_loc, dst_loc = edge\n",
    "\n",
    "        if prefix == \"\":\n",
    "            global_cnt += 1\n",
    "            rank = str(global_cnt)\n",
    "        elif spine:\n",
    "            parts = prefix.split(\".\")\n",
    "            rank = \".\".join([*parts[:-1], str(int(parts[-1]) + 1)]) if parts[:-1] else str(int(parts[-1]) + 1)\n",
    "        else:\n",
    "            side_cnt[prefix] += 1\n",
    "            rank = f\"{prefix}.{side_cnt[prefix]}\"\n",
    "\n",
    "        rows.append(\n",
    "            dict(\n",
    "                FEEDER_ID=fid,\n",
    "                FROM_TO=f\"{src_sw}-{dst_sw}\",\n",
    "                SOURCE_LOCATION=src_loc,\n",
    "                DESTINATION_LOCATION=dst_loc,\n",
    "                RANK=rank,\n",
    "                FROM_SWITCH=src_sw,\n",
    "                TO_SWITCH=dst_sw,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        kids = sorted(pruned.get(dst_loc, []), key=lambda e: depth.get(e[3], 1), reverse=True)\n",
    "        for s in reversed(kids[1:]):  # side branches first\n",
    "            stack.append((s, rank, False))\n",
    "        if kids:\n",
    "            stack.append((kids[0], rank, True))\n",
    "\n",
    "    return rows\n",
    "# ─────────────────────── trace every feeder  ───────────────────────────────\n",
    "print(\"2. trace feeders\")\n",
    "traces=[]\n",
    "for i,fid in enumerate(ht[\"FEEDER_ID\"].dropna().unique(),1):\n",
    "    if i==1 or i%100==0: print(f\"   {i}. FEEDER {fid}\")\n",
    "    traces.extend(trace_feeder(fid))\n",
    "trace_df=pd.DataFrame(traces)\n",
    "trace_df= trace_df.drop_duplicates()\n",
    "# ─────────────────────── load + prep audit  ────────────────────────────────\n",
    "print(\"3. merge energy audit\")\n",
    "audit = canon_all(pd.read_csv(INPUT_EN, low_memory=False, parse_dates=[DATE_COL]))\n",
    "audit = audit.drop_duplicates([FUNC_LOC_COL, DATE_COL, LOAD_COL])\n",
    "audit.columns=[c.upper() for c in audit.columns]\n",
    "\n",
    "agg=(audit[[FUNC_LOC_COL,DATE_COL,LOAD_COL]]\n",
    "        .dropna(subset=[FUNC_LOC_COL])\n",
    "        .groupby(FUNC_LOC_COL)\n",
    "        .agg(LATEST_DT_DATE=(DATE_COL,\"max\"),DT_LOAD=(LOAD_COL,\"mean\"))\n",
    "        .reset_index())\n",
    "\n",
    "merged=(trace_df.merge(agg,how=\"left\",\n",
    "                       left_on=\"DESTINATION_LOCATION\",\n",
    "                       right_on=FUNC_LOC_COL)\n",
    "                 .drop(columns=[FUNC_LOC_COL]))\n",
    "merged[\"LATEST_DT_DATE\"]=pd.to_datetime(merged[\"LATEST_DT_DATE\"]).dt.date\n",
    "merged[\"LOCATION\"]=merged[\"DESTINATION_LOCATION\"]\n",
    "\n",
    "# ─────────────────────── filter malformed FROM_TO  ────────────────────────\n",
    "is_pair=lambda x: bool(re.fullmatch(r\"\\d+-\\d+\",str(x)))\n",
    "kept   = merged[merged[\"FROM_TO\"].apply(is_pair)]\n",
    "dropped= merged[~merged[\"FROM_TO\"].apply(is_pair)]\n",
    "\n",
    "lost=sorted(set(merged[\"FEEDER_ID\"]) - set(kept[\"FEEDER_ID\"]))\n",
    "pd.Series(lost,name=\"LOST_FEEDER_ID\").to_csv(LOST_IDS_OUT,index=False)\n",
    "dropped.to_csv(LOST_DATA_OUT,index=False)\n",
    "\n",
    "# ─────────────────────── gap-free renumber per feeder  ─────────────────────\n",
    "# def renumber(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     mapping,counters,new={},defaultdict(int),[]\n",
    "#     for old in df[\"RANK\"]:\n",
    "#         pre=\".\".join(old.split(\".\")[:-1]); pre_new=mapping.get(pre,\"\")\n",
    "#         counters[pre_new]+=1; idx=counters[pre_new]\n",
    "#         new_rank=f\"{pre_new}.{idx}\" if pre_new else str(idx)\n",
    "#         mapping[old]=new_rank; new.append(new_rank)\n",
    "#     out=df.copy(); out[\"RANK\"]=new; return out\n",
    "\n",
    "# kept[\"_RK\"]=kept[\"RANK\"].map(lambda r: tuple(int(x) for x in r.split(\".\")))\n",
    "# kept=(kept.sort_values([\"FEEDER_ID\",\"_RK\"]).drop(columns=\"_RK\")\n",
    "#           .groupby(\"FEEDER_ID\",group_keys=False).apply(renumber))\n",
    "\n",
    "# ─────────────────────── export  ───────────────────────────────────────────\n",
    "OUT_COLS=[\"FEEDER_ID\",\"FROM_TO\",\"SOURCE_LOCATION\",\"DESTINATION_LOCATION\",\n",
    "          \"LOCATION\",\"RANK\",\"LATEST_DT_DATE\",\"DT_LOAD\"]\n",
    "kept = kept.sort_values(by=[\"FEEDER_ID\", \"RANK\"], na_position=\"last\")\n",
    "kept.to_csv(OUTPUT,index=False,columns=OUT_COLS)\n",
    "print(f\" saved {len(kept):,} rows → {OUTPUT}\")\n",
    "\n",
    "# preview\n",
    "from IPython.display import display\n",
    "display(kept.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd7b5588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique FEEDER_ID values: 1099\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv(OUTPUT)\n",
    "col = df['FEEDER_ID'].unique()\n",
    "print(\"Unique FEEDER_ID values:\", len(col))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b49247ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feeders with only main chain (no sub-chains): 467 of 1099 (42.49%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_49718/2588241840.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(is_main_chain_only)\n"
     ]
    }
   ],
   "source": [
    "# After your 'kept' DataFrame is ready...\n",
    "\n",
    "# 1. Identify feeders with only main chain (no sub-branches)\n",
    "def is_main_chain_only(subdf):\n",
    "    return all('.' not in r for r in subdf['RANK'])\n",
    "\n",
    "# 2. Group by FEEDER_ID, check for main chain only\n",
    "main_chain_flags = (\n",
    "    kept.groupby('FEEDER_ID')\n",
    "        .apply(is_main_chain_only)\n",
    "        .rename('IS_MAIN_CHAIN_ONLY')\n",
    "        .reset_index()\n",
    ")\n",
    "\n",
    "# 3. Calculate statistics\n",
    "n_total = main_chain_flags.shape[0]\n",
    "n_main  = main_chain_flags['IS_MAIN_CHAIN_ONLY'].sum()\n",
    "percent_main = 100 * n_main / n_total if n_total else 0\n",
    "\n",
    "print(f\"Feeders with only main chain (no sub-chains): {n_main} of {n_total} ({percent_main:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af79a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acd75c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
