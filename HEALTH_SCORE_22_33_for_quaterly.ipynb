{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cc6becf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:56:21 | STEP 1: Processing fault history...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:56:22 | Processed fault history for 208 switches.\n",
      "22:56:22 | \n",
      "STEP 2: Building dataset and training Poisson-LSTM model...\n",
      "22:56:24 | Training finished in 5 epochs | best val-loss=0.2613\n",
      "22:56:24 | \n",
      "STEP 3: Generating forecasts for target year...\n",
      "22:56:24 | Generated quarterly and yearly forecasts for 2024.\n",
      "22:56:24 | \n",
      "STEP 4: Pre-processing cable master data for dynamic calculations...\n",
      "22:56:25 | Found 0 monthly cycle columns and 0 monthly variation columns.\n",
      "22:56:25 | \n",
      "STEP 5: Calculating Health Scores per Quarter with dynamic loading...\n",
      "22:56:25 | \n",
      "--- Processing Q1_Jan-Mar (2024) ---\n",
      "22:56:25 | Saved results for 256 cables to Q1_Jan-Mar/\n",
      "22:56:25 | Q1_Jan-Mar confusion matrix:\n",
      "[[191  43]\n",
      " [ 10  12]]\n",
      "AUROC = 0.752\n",
      "22:56:25 | \n",
      "--- Processing Q2_Apr-Jun (2024) ---\n",
      "22:56:25 | Saved results for 256 cables to Q2_Apr-Jun/\n",
      "22:56:25 | Q2_Apr-Jun confusion matrix:\n",
      "[[178  34]\n",
      " [ 24  20]]\n",
      "AUROC = 0.724\n",
      "22:56:25 | \n",
      "--- Processing Q3_Jul-Sep (2024) ---\n",
      "22:56:25 | Saved results for 256 cables to Q3_Jul-Sep/\n",
      "22:56:25 | Q3_Jul-Sep confusion matrix:\n",
      "[[203  40]\n",
      " [  3  10]]\n",
      "AUROC = 0.863\n",
      "22:56:25 | \n",
      "--- Processing Q4_Oct-Dec (2024) ---\n",
      "22:56:25 | Saved results for 256 cables to Q4_Oct-Dec/\n",
      "22:56:25 | Q4_Oct-Dec confusion matrix:\n",
      "[[186  49]\n",
      " [ 10  11]]\n",
      "AUROC = 0.718\n",
      "22:56:25 | \n",
      "=============================================\n",
      "22:56:25 | FLEET-WIDE FAULT SUMMARY FOR FULL YEAR 2024\n",
      "22:56:25 | =============================================\n",
      "22:56:25 | Total Predicted Faults (Yearly): 154.0\n",
      "22:56:25 | Total Actual Faults (Yearly):    128\n",
      "22:56:25 | =============================================\n",
      "22:56:25 | \n",
      "Pipeline complete → Quarterly results saved in: /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/A_fault_model_with_health_QUARTERLY\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "HT-cable Poisson-LSTM forecast + 9-factor health score (2018-2024)\n",
    "2025-08-19 — QUARTERLY HEALTH SCORE + SEASONAL FAULT FACTOR\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import logging, math, sys, json, numpy as np, pandas as pd\n",
    "import torch, torch.nn as nn\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ─── paths ──────────────────────────────────────────────────────────────\n",
    "FAULT_CSV = Path(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/FAULT DATA/HT_fault_cable_info_processed_with_affected.csv\")\n",
    "CABLE_CSV = Path(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/SWNO_MASTER_COMBINED_FULL_FINAL4.csv\")\n",
    "OUT_DIR = Path(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/A_fault_model_with_health_QUARTERLY\")\n",
    "\n",
    "# ─── constants / hyper-params ───────────────────────────────────────────\n",
    "MIN_YEAR, TARGET_YEAR   = 2017, 2024\n",
    "TRAIN_YEARS             = list(range(2019, TARGET_YEAR))\n",
    "KEEP_VOLTAGES           = {22, 33}\n",
    "EXPECTED_LIFE_YEARS     = 35\n",
    "\n",
    "# 9-factor weights\n",
    "W = dict(  a= 0.01941, c= 0.03371, f= 0.12157, i= 0.10727, l= 0.25278,\n",
    "           p= 0.15000, r= 0.04904, s= 0.06028, u= 0.20594 )\n",
    "\n",
    "BATCH, EPOCHS, PATIENCE = 64, 40, 12\n",
    "LR, WD, CLIP            = 1e-3, 1e-5, 1.0\n",
    "HID, LAY, DROP, EMB     = 512, 2, 0.1, 16\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42); np.random.seed(42)\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s | %(message)s\",\n",
    "                    datefmt=\"%H:%M:%S\", level=logging.INFO, stream=sys.stdout)\n",
    "log = logging.getLogger(\"pipeline\").info\n",
    "\n",
    "# ─── helper functions (unchanged) ───────────────────────────────────────\n",
    "def v_to_num(v):\n",
    "    try: return float(str(v).lower().replace(\"kv\",\"\"))\n",
    "    except: return math.nan\n",
    "\n",
    "def norm_sw(s):\n",
    "    return (s.astype(str).str.upper().str.strip()\n",
    "             .str.replace(r\"^(SWNO_|SWNO|SW|S)\\s*\", \"\", regex=True)\n",
    "             .str.replace(r\"\\D+\",\"\",regex=True)\n",
    "             .replace(\"\", np.nan)).astype(\"Int64\")\n",
    "\n",
    "def month_range(a,b): return pd.period_range(f\"{a}-01\", f\"{b}-12\", freq=\"M\")\n",
    "\n",
    "def sincos(idx):\n",
    "    ang = 2*np.pi*idx.month.values/12\n",
    "    return np.stack([np.sin(ang), np.cos(ang)], 1)\n",
    "\n",
    "def robust(s, lo=5, hi=95):\n",
    "    c=s.replace([np.inf,-np.inf],np.nan).dropna()\n",
    "    if c.empty: return pd.Series(0., index=s.index)\n",
    "    a,b=np.percentile(c,[lo,hi]); return ((s-a).clip(0,b-a)/(b-a+1e-9)).fillna(0.)\n",
    "\n",
    "def months_to_quarters(mat):\n",
    "    q=np.zeros((mat.shape[0],4))\n",
    "    q[:,0]=mat[:,0:3].sum(1); q[:,1]=mat[:,3:6].sum(1)\n",
    "    q[:,2]=mat[:,6:9].sum(1); q[:,3]=mat[:,9:12].sum(1)\n",
    "    return q\n",
    "\n",
    "# ─── 1 · fault history ──────────────────────────────────────────────────\n",
    "log(\"STEP 1: Processing fault history...\")\n",
    "fault=pd.read_csv(FAULT_CSV,parse_dates=[\"TIME_OUTAGE\"],low_memory=False)\n",
    "sw_col=\"TO_SWITCH\" if \"TO_SWITCH\" in fault.columns else fault.columns[0]\n",
    "fault[\"SWITCH_ID\"]=norm_sw(fault[sw_col])\n",
    "fault=fault.dropna(subset=[\"SWITCH_ID\",\"TIME_OUTAGE\"])\n",
    "fault[\"TIME_OUTAGE\"] = fault[\"TIME_OUTAGE\"].dt.tz_localize(None)\n",
    "fault=fault[fault[\"TIME_OUTAGE\"].dt.year.between(MIN_YEAR,TARGET_YEAR)]\n",
    "if \"VOLTAGE\" in fault.columns:\n",
    "    fault[\"VNUM\"]=fault[\"VOLTAGE\"].apply(v_to_num)\n",
    "    fault=fault[fault[\"VNUM\"].isin(KEEP_VOLTAGES)]\n",
    "fault=fault.drop_duplicates([\"SWITCH_ID\",\"TIME_OUTAGE\"])\n",
    "fault[\"YM\"]=fault[\"TIME_OUTAGE\"].dt.to_period(\"M\")\n",
    "idx_full=month_range(MIN_YEAR,TARGET_YEAR)\n",
    "counts=(fault[fault[\"TIME_OUTAGE\"].dt.year<=TARGET_YEAR-1]\n",
    "        .groupby([\"SWITCH_ID\",\"YM\"]).size()\n",
    "        .unstack(fill_value=0)\n",
    "        .reindex(columns=idx_full,fill_value=0).astype(float))\n",
    "switches=counts.index.tolist(); sw2idx={sw:i for i,sw in enumerate(switches)}\n",
    "log(f\"Processed fault history for {len(switches)} switches.\")\n",
    "\n",
    "# ─── 2 · Poisson-LSTM dataset & training ────────────────────────────────\n",
    "log(\"\\nSTEP 2: Building dataset and training Poisson-LSTM model...\")\n",
    "class WinDS(Dataset):\n",
    "    def __init__(self, fr):\n",
    "        self.x  = torch.tensor(fr[\"X_seq\"],dtype=torch.float32)\n",
    "        self.xs = torch.tensor(fr[\"X_season\"],dtype=torch.float32)\n",
    "        self.y  = torch.tensor(fr[\"y_seq\"],dtype=torch.float32)\n",
    "        self.sw = torch.tensor(fr[\"sw_idx\"],dtype=torch.long)\n",
    "    def __len__(self): return len(self.sw)\n",
    "    def __getitem__(self,i): return self.x[i],self.xs[i],self.y[i],self.sw[i]\n",
    "class PoissonLSTM(nn.Module):\n",
    "    def __init__(self,n_sw):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(n_sw, EMB)\n",
    "        self.rnn = nn.LSTM(3,HID,LAY,batch_first=True,\n",
    "                           dropout=DROP if LAY>1 else 0.)\n",
    "        self.head=nn.Sequential(nn.Linear(HID+EMB,HID),nn.ReLU(),\n",
    "                                nn.Linear(HID,HID//2),nn.ReLU(),\n",
    "                                nn.Linear(HID//2,12))\n",
    "        self.sp = nn.Softplus()\n",
    "    def forward(self,x,xs,sw):\n",
    "        h,_=self.rnn(torch.cat([x,xs],-1))\n",
    "        h=torch.cat([h[:,-1],self.emb(sw)],1)\n",
    "        return self.sp(self.head(h))\n",
    "def build_frames(years, nonzero=True):\n",
    "    X_seq,X_sea,y_seq,sw_idx=[],[],[],[]\n",
    "    for sw,row in counts.iterrows():\n",
    "        for Y in years:\n",
    "            tr,tg=month_range(Y-2,Y-1),month_range(Y,Y)\n",
    "            if nonzero and row[tr].sum()==0: continue\n",
    "            X_seq.append(np.log1p(row[tr]).values[:,None])\n",
    "            X_sea.append(sincos(tr)); y_seq.append(row[tg].values)\n",
    "            sw_idx.append(sw2idx[sw])\n",
    "    return dict(X_seq=np.stack(X_seq), X_season=np.stack(X_sea),\n",
    "                y_seq=np.stack(y_seq), sw_idx=np.array(sw_idx,dtype=np.int64))\n",
    "train_frames=build_frames(TRAIN_YEARS,True)\n",
    "ds=WinDS(train_frames); perm=np.random.permutation(len(ds))\n",
    "n_val=max(1,int(.1*len(ds)))\n",
    "dl_tr=DataLoader(torch.utils.data.Subset(ds,perm[:-n_val]),BATCH,shuffle=True)\n",
    "dl_va=DataLoader(torch.utils.data.Subset(ds,perm[-n_val:]),BATCH,shuffle=False)\n",
    "model=PoissonLSTM(len(switches)).to(DEVICE)\n",
    "loss_fn,opt=nn.PoissonNLLLoss(log_input=False),torch.optim.Adam(model.parameters(),lr=LR,weight_decay=WD)\n",
    "best,bad,best_ep=1e9,0,0\n",
    "for ep in range(1,EPOCHS+1):\n",
    "    model.train()\n",
    "    for xb,xs,yb,swb in dl_tr:\n",
    "        xb,xs,yb,swb=[t.to(DEVICE) for t in (xb,xs,yb,swb)]\n",
    "        opt.zero_grad(); loss_fn(model(xb,xs,swb),yb).backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),CLIP); opt.step()\n",
    "    with torch.no_grad():\n",
    "        v=np.mean([loss_fn(model(x.to(DEVICE),xs.to(DEVICE),sw.to(DEVICE)),\n",
    "                           y.to(DEVICE)).item() for x,xs,y,sw in dl_va])\n",
    "    if v<best-1e-4: best,bad,best_ep,ckpt=v,0,ep,model.state_dict()\n",
    "    else: bad+=1\n",
    "    if bad>=PATIENCE: break\n",
    "model.load_state_dict(ckpt)\n",
    "log(f\"Training finished in {best_ep} epochs | best val-loss={best:.4f}\")\n",
    "\n",
    "# ─── 3 · TARGET YEAR forecasts ──────────────────────────────────────────\n",
    "log(\"\\nSTEP 3: Generating forecasts for target year...\")\n",
    "eval_frames=build_frames([TARGET_YEAR],False)\n",
    "mu,swids=[],[]\n",
    "with torch.no_grad():\n",
    "    for xb,xs,_,swb in DataLoader(WinDS(eval_frames),256,False):\n",
    "        xb,xs,swb=[t.to(DEVICE) for t in (xb,xs,swb)]\n",
    "        mu.append(model(xb,xs,swb).cpu().numpy())\n",
    "        swids.append(swb.cpu().numpy())\n",
    "MU,SWIDX=np.concatenate(mu),np.concatenate(swids)\n",
    "QTRS=months_to_quarters(MU)\n",
    "ym=month_range(TARGET_YEAR,TARGET_YEAR)\n",
    "rows_q,rows_y=[],[]\n",
    "for i,ix in enumerate(SWIDX):\n",
    "    sw=switches[ix]\n",
    "    rows_y.append(dict(SWITCH_ID=sw,YEAR=TARGET_YEAR,PRED_FULL_YEAR=float(MU[i].sum())))\n",
    "    for qi,qsum in enumerate(QTRS[i],1):\n",
    "        rows_q.append(dict(SWITCH_ID=sw,YEAR=TARGET_YEAR,QUARTER=qi,PRED_FAULTS_Q=float(qsum)))\n",
    "q_preds = pd.DataFrame(rows_q)\n",
    "y_preds = pd.DataFrame(rows_y)\n",
    "log(f\"Generated quarterly and yearly forecasts for {TARGET_YEAR}.\")\n",
    "\n",
    "# ─── 4 · PRE-PROCESSING FOR DYNAMIC FACTORS ─────────────────────────────\n",
    "log(\"\\nSTEP 4: Pre-processing cable master data for dynamic calculations...\")\n",
    "cables_master = pd.read_csv(CABLE_CSV, low_memory=False)\n",
    "def parse_date_from_col(col_name, prefix):\n",
    "    try:\n",
    "        date_str = col_name.replace(f\"{prefix}_Month_\", \"\")\n",
    "        return pd.to_datetime(date_str, format='%Y%m')\n",
    "    except: return None\n",
    "cycle_cols = {c: parse_date_from_col(c, \"CYCLE\") for c in cables_master.columns if c.startswith(\"CYCLE_Month_\")}\n",
    "var_cols = {c: parse_date_from_col(c, \"VAR\") for c in cables_master.columns if c.startswith(\"VAR_Month_\")}\n",
    "cycle_cols = {k: v for k, v in cycle_cols.items() if v is not None}\n",
    "var_cols = {k: v for k, v in var_cols.items() if v is not None}\n",
    "log(f\"Found {len(cycle_cols)} monthly cycle columns and {len(var_cols)} monthly variation columns.\")\n",
    "\n",
    "# ─── 5 · QUARTERLY HEALTH SCORE WITH DYNAMIC LOADING ────────────────────\n",
    "log(\"\\nSTEP 5: Calculating Health Scores per Quarter with dynamic loading...\")\n",
    "QUARTERS = {\n",
    "    \"Q1_Jan-Mar\": (1, 3), \"Q2_Apr-Jun\": (4, 6),\n",
    "    \"Q3_Jul-Sep\": (7, 9), \"Q4_Oct-Dec\": (10, 12)\n",
    "}\n",
    "# MODIFIED: Updated the label for the 'u' factor\n",
    "LABELS = {\n",
    "    \"c\":\"High cyclic loading (c)\", \"r\":\"Wide load-range utilisation (r)\",\n",
    "    \"a\":\"Advanced cable age (a)\", \"f\":\"Many historic faults (f)\",\n",
    "    \"s\":\"Numerous joints / segments (s)\", \"p\":\"High predicted faults per km (p)\",\n",
    "    \"i\":\"Frequent interruptions (i)\", \"l\":\"Long circuit length (ℓ)\",\n",
    "    \"u\":\"Faults in same quarter last year (u)\"\n",
    "}\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for i, (q_name, (start_month, end_month)) in enumerate(QUARTERS.items()):\n",
    "    q_num = i + 1\n",
    "    log(f\"\\n--- Processing {q_name} ({TARGET_YEAR}) ---\")\n",
    "    q_out_dir = OUT_DIR / q_name\n",
    "    q_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    q_start_date = pd.Timestamp(f\"{TARGET_YEAR}-{start_month}-01\")\n",
    "    loading_start_date = q_start_date - pd.DateOffset(years=1)\n",
    "    cab = cables_master.drop_duplicates(\"DESTINATION_SWITCH_ID\").rename(columns={\n",
    "        \"DESTINATION_SWITCH_ID\":\"SWITCH_ID\", \"MEASUREDLENGTH\":\"LENGTH_M\",\n",
    "        \"COMMISSIONEDDATE\":\"DATE_INSTALLED\", \"NO_OF_SEGMENT\":\"SEGMENTS\"})\n",
    "    cab[\"DATE_INSTALLED\"] = pd.to_datetime(cab[\"DATE_INSTALLED\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "    cab[\"LENGTH_KM\"] = pd.to_numeric(cab[\"LENGTH_M\"], errors=\"coerce\") / 1000\n",
    "    relevant_cycle_cols = [c for c, dt in cycle_cols.items() if loading_start_date <= dt < q_start_date]\n",
    "    relevant_var_cols = [c for c, dt in var_cols.items() if loading_start_date <= dt < q_start_date]\n",
    "    if relevant_cycle_cols: cab[\"cycle_pm\"] = cab[relevant_cycle_cols].mean(axis=1)\n",
    "    else: cab[\"cycle_pm\"] = 0\n",
    "    if relevant_var_cols:\n",
    "        median_of_period = cab[relevant_var_cols].median(axis=1)\n",
    "        cab[\"load_range_idx\"] = cab[relevant_var_cols].mean(axis=1) / median_of_period.replace(0, np.nan)\n",
    "    else: cab[\"load_range_idx\"] = 0\n",
    "    cab[\"c_raw\"] = cab[\"cycle_pm\"]; cab[\"r_raw\"] = cab[\"load_range_idx\"]\n",
    "    hist = fault[fault[\"TIME_OUTAGE\"] < q_start_date].groupby(\"SWITCH_ID\").size()\n",
    "    cab = cab.merge(hist.rename(\"hist_faults\"), on=\"SWITCH_ID\", how=\"left\").fillna({\"hist_faults\": 0})\n",
    "    cab[\"f_raw\"] = cab[\"hist_faults\"] / cab[\"LENGTH_KM\"].replace(0, np.nan)\n",
    "    q_pred_current = q_preds[q_preds['QUARTER'] == q_num]\n",
    "    cab = cab.merge(q_pred_current[['SWITCH_ID', 'PRED_FAULTS_Q']], on=\"SWITCH_ID\", how=\"left\").fillna({\"PRED_FAULTS_Q\": 0})\n",
    "    cab[\"p_raw\"] = cab[\"PRED_FAULTS_Q\"].replace(0, np.nan)\n",
    "    fault_sorted = fault[fault[\"TIME_OUTAGE\"] < q_start_date].sort_values([\"SWITCH_ID\", \"TIME_OUTAGE\"])\n",
    "    fault_sorted[\"Δt_h\"] = (fault_sorted.groupby(\"SWITCH_ID\")[\"TIME_OUTAGE\"].diff().dt.total_seconds().div(3600))\n",
    "    mean_dt = fault_sorted.groupby(\"SWITCH_ID\")[\"Δt_h\"].mean().rename(\"mean_h\")\n",
    "    cab = cab.merge(mean_dt, on=\"SWITCH_ID\", how=\"left\")\n",
    "    cab[\"i_raw\"] = 1 / cab[\"mean_h\"].clip(lower=1)\n",
    "    \n",
    "    # MODIFIED: 'u' factor is now faults in the same quarter of the previous year\n",
    "    last_year = TARGET_YEAR - 1\n",
    "    same_q_faults_last_year = fault[\n",
    "        (fault[\"TIME_OUTAGE\"].dt.year == last_year) &\n",
    "        (fault[\"TIME_OUTAGE\"].dt.month.between(start_month, end_month))\n",
    "    ].groupby(\"SWITCH_ID\").size()\n",
    "    cab = cab.merge(same_q_faults_last_year.rename(\"faults_same_q_last_yr\"), on=\"SWITCH_ID\", how=\"left\").fillna({\"faults_same_q_last_yr\": 0})\n",
    "    cab[\"u_raw\"] = cab[\"faults_same_q_last_yr\"]\n",
    "    \n",
    "    cab[\"a_raw\"] = (q_start_date - cab[\"DATE_INSTALLED\"]).dt.days / (EXPECTED_LIFE_YEARS * 365)\n",
    "    cab[\"s_raw\"] = (cab[\"SEGMENTS\"].fillna(1) - 1).clip(lower=0)\n",
    "    cab[\"l_raw\"] = np.log1p(cab[\"LENGTH_KM\"])\n",
    "    for col in [\"a_raw\", \"c_raw\", \"f_raw\", \"i_raw\", \"l_raw\", \"p_raw\", \"r_raw\", \"s_raw\", \"u_raw\"]:\n",
    "        cab[col[0]] = robust(cab[col])\n",
    "    risk = sum(W[k] * cab[k] for k in W.keys())\n",
    "    cab[\"health_score\"] = np.rint(100 * (1 - risk)).clip(0, 100).astype(int)\n",
    "    cab[\"health_score_10\"] = np.clip(np.ceil(cab[\"health_score\"] / 10), 1, 10).astype(int)\n",
    "    cab[\"health_band\"] = pd.cut(cab[\"health_score\"], [-np.inf, 40, 60, 100], labels=[\"Poor\", \"Moderate\", \"Good\"])\n",
    "    for k, wv in W.items(): cab[f\"weight_{k}\"] = wv\n",
    "    risk_contrib = pd.DataFrame({k: W[k] * cab[k] for k in W.keys()})\n",
    "    top3 = risk_contrib.apply(lambda r: r.nlargest(3).index.tolist(), axis=1)\n",
    "    cab[\"primary_health_driver\"] = top3.apply(lambda lst: LABELS[lst[0]])\n",
    "    cab[\"top3_health_drivers\"] = top3.apply(lambda lst: \"; \".join(LABELS[k] for k in lst))\n",
    "\n",
    "    # MODIFIED: Logic for \"no fault\" assets now only considers total historical faults\n",
    "    mask_nofault = cab[\"hist_faults\"].eq(0)\n",
    "    cab.loc[mask_nofault, [\"health_score\", \"health_score_10\"]] = [100, 10]\n",
    "    cab.loc[mask_nofault, \"health_band\"] = \"Good\"\n",
    "    cab.loc[mask_nofault, \"primary_health_driver\"] = \"No recorded faults\"\n",
    "    cab.loc[mask_nofault, \"top3_health_drivers\"] = \"No recorded faults\"\n",
    "\n",
    "    output_cols = [c for c in cab.columns if c not in cycle_cols and c not in var_cols]\n",
    "    cab[output_cols].to_csv(q_out_dir / f\"cable_health_{TARGET_YEAR}_{q_name}_scored.csv\", index=False)\n",
    "    cab[output_cols][~mask_nofault].to_csv(q_out_dir / f\"cable_health_{TARGET_YEAR}_{q_name}_predictions.csv\", index=False)\n",
    "    log(f\"Saved results for {len(cab)} cables to {q_out_dir.name}/\")\n",
    "    \n",
    "    actual_faults_q = fault[(fault[\"TIME_OUTAGE\"].dt.year == TARGET_YEAR) & (fault[\"TIME_OUTAGE\"].dt.month.between(start_month, end_month))]\n",
    "    actual_switches_q = actual_faults_q[\"SWITCH_ID\"].unique()\n",
    "    cab[\"ACTUAL_FAIL_Q\"] = cab[\"SWITCH_ID\"].isin(actual_switches_q).astype(int)\n",
    "    pred = cab[\"health_band\"].map({\"Poor\": 1, \"Moderate\": 1, \"Good\": 0})\n",
    "    if len(cab[\"ACTUAL_FAIL_Q\"].unique()) > 1:\n",
    "        cm = confusion_matrix(cab[\"ACTUAL_FAIL_Q\"], pred)\n",
    "        au = roc_auc_score(cab[\"ACTUAL_FAIL_Q\"], 100 - cab[\"health_score\"])\n",
    "        log(f\"{q_name} confusion matrix:\\n{cm}\\nAUROC = {au:.3f}\")\n",
    "    else:\n",
    "        log(f\"Skipping validation for {q_name}: Only one class present in actuals.\")\n",
    "\n",
    "# --- Final Summary ---\n",
    "total_predicted_faults = y_preds['PRED_FULL_YEAR'].sum()\n",
    "total_actual_faults_2024 = fault[fault['TIME_OUTAGE'].dt.year == TARGET_YEAR].shape[0]\n",
    "\n",
    "log(\"\\n\" + \"=\"*45)\n",
    "log(f\"FLEET-WIDE FAULT SUMMARY FOR FULL YEAR {TARGET_YEAR}\")\n",
    "log(\"=\"*45)\n",
    "log(f\"Total Predicted Faults (Yearly): {total_predicted_faults:.1f}\")\n",
    "log(f\"Total Actual Faults (Yearly):    {total_actual_faults_2024}\")\n",
    "log(\"=\"*45)\n",
    "log(f\"\\nPipeline complete → Quarterly results saved in: {OUT_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4026f9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:46:49 | STEP 1: Processing fault history...\n",
      "16:46:49 | Processed fault history for 210 switches.\n",
      "16:46:49 | \n",
      "STEP 2: Building dataset and training Poisson-LSTM model...\n",
      "16:46:52 | Training finished in 4 epochs | best val-loss=0.1891\n",
      "16:46:52 | \n",
      "STEP 3: Generating forecasts for target year...\n",
      "16:46:53 | Generated quarterly and yearly forecasts for 2024.\n",
      "16:46:53 | \n",
      "STEP 4: Pre-processing cable master data for dynamic calculations...\n",
      "16:46:53 | Found 0 monthly cycle columns and 0 monthly variation columns.\n",
      "16:46:53 | \n",
      "STEP 5: Calculating Health Scores per Quarter with dynamic loading...\n",
      "16:46:53 | \n",
      "--- Processing Q1_Jan-Mar (2024) ---\n",
      "16:46:53 | Saved results for 256 cables to Q1_Jan-Mar/\n",
      "16:46:53 | Q1_Jan-Mar confusion matrix:\n",
      "[[165  69]\n",
      " [  5  17]]\n",
      "AUROC = 0.781\n",
      "16:46:53 | \n",
      "--- Processing Q2_Apr-Jun (2024) ---\n",
      "16:46:53 | Saved results for 256 cables to Q2_Apr-Jun/\n",
      "16:46:53 | Q2_Apr-Jun confusion matrix:\n",
      "[[151  61]\n",
      " [ 17  27]]\n",
      "AUROC = 0.727\n",
      "16:46:53 | \n",
      "--- Processing Q3_Jul-Sep (2024) ---\n",
      "16:46:53 | Saved results for 256 cables to Q3_Jul-Sep/\n",
      "16:46:53 | Q3_Jul-Sep confusion matrix:\n",
      "[[176  67]\n",
      " [  1  12]]\n",
      "AUROC = 0.877\n",
      "16:46:53 | \n",
      "--- Processing Q4_Oct-Dec (2024) ---\n",
      "16:46:53 | Saved results for 256 cables to Q4_Oct-Dec/\n",
      "16:46:53 | Q4_Oct-Dec confusion matrix:\n",
      "[[154  81]\n",
      " [  8  13]]\n",
      "AUROC = 0.712\n",
      "16:46:53 | \n",
      "=============================================\n",
      "16:46:53 | FLEET-WIDE FAULT SUMMARY FOR FULL YEAR 2024\n",
      "16:46:53 | =============================================\n",
      "16:46:53 | Total Predicted Faults (Yearly): 124.6\n",
      "16:46:53 | Total Actual Faults (Yearly):    128\n",
      "16:46:53 | =============================================\n",
      "16:46:53 | \n",
      "Pipeline complete → Quarterly results saved in: /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/A_fault_model_with_health_QUARTERLY\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "HT-cable Poisson-LSTM forecast + 9-factor health score (2018-2024)\n",
    "2025-08-19 — QUARTERLY HEALTH SCORE + AGGRESSIVE WEIGHTS\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import logging, math, sys, json, numpy as np, pandas as pd\n",
    "import torch, torch.nn as nn\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ─── paths ──────────────────────────────────────────────────────────────\n",
    "FAULT_CSV = Path(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/FAULT DATA/HT_fault_cable_info_processed_with_affected.csv\")\n",
    "CABLE_CSV = Path(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/SWNO_MASTER_COMBINED_FULL_FINAL3.csv\")\n",
    "OUT_DIR = Path(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/A_fault_model_with_health_QUARTERLY\")\n",
    "\n",
    "# ─── constants / hyper-params ───────────────────────────────────────────\n",
    "MIN_YEAR, TARGET_YEAR   = 2016, 2024\n",
    "TRAIN_YEARS             = list(range(2018, TARGET_YEAR))\n",
    "KEEP_VOLTAGES           = {22, 33}\n",
    "EXPECTED_LIFE_YEARS     = 35\n",
    "\n",
    "# MODIFIED: Weights have been made more aggressive as per your request\n",
    "W = dict(  a= 0.01941,\n",
    "  c= 0.53371,\n",
    "  f= 0.12157,\n",
    "  i= 0.10727,\n",
    "  l= 0.20278,\n",
    "  p= 0.15000,\n",
    "  r= 0.54904,\n",
    "  s= 0.06028,\n",
    "  u= 0.15594 )\n",
    "BATCH, EPOCHS, PATIENCE = 64, 40, 12\n",
    "LR, WD, CLIP            = 1e-3, 1e-5, 1.0\n",
    "HID, LAY, DROP, EMB     = 512, 2, 0.1, 16\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42); np.random.seed(42)\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s | %(message)s\",\n",
    "                    datefmt=\"%H:%M:%S\", level=logging.INFO, stream=sys.stdout)\n",
    "log = logging.getLogger(\"pipeline\").info\n",
    "\n",
    "# ─── helper functions (unchanged) ───────────────────────────────────────\n",
    "def v_to_num(v):\n",
    "    try: return float(str(v).lower().replace(\"kv\",\"\"))\n",
    "    except: return math.nan\n",
    "\n",
    "def norm_sw(s):\n",
    "    return (s.astype(str).str.upper().str.strip()\n",
    "             .str.replace(r\"^(SWNO_|SWNO|SW|S)\\s*\", \"\", regex=True)\n",
    "             .str.replace(r\"\\D+\",\"\",regex=True)\n",
    "             .replace(\"\", np.nan)).astype(\"Int64\")\n",
    "\n",
    "def month_range(a,b): return pd.period_range(f\"{a}-01\", f\"{b}-12\", freq=\"M\")\n",
    "\n",
    "def sincos(idx):\n",
    "    ang = 2*np.pi*idx.month.values/12\n",
    "    return np.stack([np.sin(ang), np.cos(ang)], 1)\n",
    "\n",
    "def robust(s, lo=5, hi=95):\n",
    "    c=s.replace([np.inf,-np.inf],np.nan).dropna()\n",
    "    if c.empty: return pd.Series(0., index=s.index)\n",
    "    a,b=np.percentile(c,[lo,hi]); return ((s-a).clip(0,b-a)/(b-a+1e-9)).fillna(0.)\n",
    "\n",
    "def months_to_quarters(mat):\n",
    "    q=np.zeros((mat.shape[0],4))\n",
    "    q[:,0]=mat[:,0:3].sum(1); q[:,1]=mat[:,3:6].sum(1)\n",
    "    q[:,2]=mat[:,6:9].sum(1); q[:,3]=mat[:,9:12].sum(1)\n",
    "    return q\n",
    "\n",
    "# ─── 1 · fault history ──────────────────────────────────────────────────\n",
    "log(\"STEP 1: Processing fault history...\")\n",
    "fault=pd.read_csv(FAULT_CSV,parse_dates=[\"TIME_OUTAGE\"],low_memory=False)\n",
    "sw_col=\"TO_SWITCH\" if \"TO_SWITCH\" in fault.columns else fault.columns[0]\n",
    "fault[\"SWITCH_ID\"]=norm_sw(fault[sw_col])\n",
    "fault=fault.dropna(subset=[\"SWITCH_ID\",\"TIME_OUTAGE\"])\n",
    "fault[\"TIME_OUTAGE\"] = fault[\"TIME_OUTAGE\"].dt.tz_localize(None)\n",
    "fault=fault[fault[\"TIME_OUTAGE\"].dt.year.between(MIN_YEAR,TARGET_YEAR)]\n",
    "if \"VOLTAGE\" in fault.columns:\n",
    "    fault[\"VNUM\"]=fault[\"VOLTAGE\"].apply(v_to_num)\n",
    "    fault=fault[fault[\"VNUM\"].isin(KEEP_VOLTAGES)]\n",
    "fault=fault.drop_duplicates([\"SWITCH_ID\",\"TIME_OUTAGE\"])\n",
    "fault[\"YM\"]=fault[\"TIME_OUTAGE\"].dt.to_period(\"M\")\n",
    "idx_full=month_range(MIN_YEAR,TARGET_YEAR)\n",
    "counts=(fault[fault[\"TIME_OUTAGE\"].dt.year<=TARGET_YEAR-1]\n",
    "        .groupby([\"SWITCH_ID\",\"YM\"]).size()\n",
    "        .unstack(fill_value=0)\n",
    "        .reindex(columns=idx_full,fill_value=0).astype(float))\n",
    "switches=counts.index.tolist(); sw2idx={sw:i for i,sw in enumerate(switches)}\n",
    "log(f\"Processed fault history for {len(switches)} switches.\")\n",
    "\n",
    "# ─── 2 · Poisson-LSTM dataset & training ────────────────────────────────\n",
    "log(\"\\nSTEP 2: Building dataset and training Poisson-LSTM model...\")\n",
    "class WinDS(Dataset):\n",
    "    def __init__(self, fr):\n",
    "        self.x  = torch.tensor(fr[\"X_seq\"],dtype=torch.float32)\n",
    "        self.xs = torch.tensor(fr[\"X_season\"],dtype=torch.float32)\n",
    "        self.y  = torch.tensor(fr[\"y_seq\"],dtype=torch.float32)\n",
    "        self.sw = torch.tensor(fr[\"sw_idx\"],dtype=torch.long)\n",
    "    def __len__(self): return len(self.sw)\n",
    "    def __getitem__(self,i): return self.x[i],self.xs[i],self.y[i],self.sw[i]\n",
    "class PoissonLSTM(nn.Module):\n",
    "    def __init__(self,n_sw):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(n_sw, EMB)\n",
    "        self.rnn = nn.LSTM(3,HID,LAY,batch_first=True,\n",
    "                           dropout=DROP if LAY>1 else 0.)\n",
    "        self.head=nn.Sequential(nn.Linear(HID+EMB,HID),nn.ReLU(),\n",
    "                                nn.Linear(HID,HID//2),nn.ReLU(),\n",
    "                                nn.Linear(HID//2,12))\n",
    "        self.sp = nn.Softplus()\n",
    "    def forward(self,x,xs,sw):\n",
    "        h,_=self.rnn(torch.cat([x,xs],-1))\n",
    "        h=torch.cat([h[:,-1],self.emb(sw)],1)\n",
    "        return self.sp(self.head(h))\n",
    "def build_frames(years, nonzero=True):\n",
    "    X_seq,X_sea,y_seq,sw_idx=[],[],[],[]\n",
    "    for sw,row in counts.iterrows():\n",
    "        for Y in years:\n",
    "            tr,tg=month_range(Y-2,Y-1),month_range(Y,Y)\n",
    "            if nonzero and row[tr].sum()==0: continue\n",
    "            X_seq.append(np.log1p(row[tr]).values[:,None])\n",
    "            X_sea.append(sincos(tr)); y_seq.append(row[tg].values)\n",
    "            sw_idx.append(sw2idx[sw])\n",
    "    return dict(X_seq=np.stack(X_seq), X_season=np.stack(X_sea),\n",
    "                y_seq=np.stack(y_seq), sw_idx=np.array(sw_idx,dtype=np.int64))\n",
    "train_frames=build_frames(TRAIN_YEARS,True)\n",
    "ds=WinDS(train_frames); perm=np.random.permutation(len(ds))\n",
    "n_val=max(1,int(.1*len(ds)))\n",
    "dl_tr=DataLoader(torch.utils.data.Subset(ds,perm[:-n_val]),BATCH,shuffle=True)\n",
    "dl_va=DataLoader(torch.utils.data.Subset(ds,perm[-n_val:]),BATCH,shuffle=False)\n",
    "model=PoissonLSTM(len(switches)).to(DEVICE)\n",
    "loss_fn,opt=nn.PoissonNLLLoss(log_input=False),torch.optim.Adam(model.parameters(),lr=LR,weight_decay=WD)\n",
    "best,bad,best_ep=1e9,0,0\n",
    "for ep in range(1,EPOCHS+1):\n",
    "    model.train()\n",
    "    for xb,xs,yb,swb in dl_tr:\n",
    "        xb,xs,yb,swb=[t.to(DEVICE) for t in (xb,xs,yb,swb)]\n",
    "        opt.zero_grad(); loss_fn(model(xb,xs,swb),yb).backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),CLIP); opt.step()\n",
    "    with torch.no_grad():\n",
    "        v=np.mean([loss_fn(model(x.to(DEVICE),xs.to(DEVICE),sw.to(DEVICE)),\n",
    "                           y.to(DEVICE)).item() for x,xs,y,sw in dl_va])\n",
    "    if v<best-1e-4: best,bad,best_ep,ckpt=v,0,ep,model.state_dict()\n",
    "    else: bad+=1\n",
    "    if bad>=PATIENCE: break\n",
    "model.load_state_dict(ckpt)\n",
    "log(f\"Training finished in {best_ep} epochs | best val-loss={best:.4f}\")\n",
    "\n",
    "# ─── 3 · TARGET YEAR forecasts ──────────────────────────────────────────\n",
    "log(\"\\nSTEP 3: Generating forecasts for target year...\")\n",
    "eval_frames=build_frames([TARGET_YEAR],False)\n",
    "mu,swids=[],[]\n",
    "with torch.no_grad():\n",
    "    for xb,xs,_,swb in DataLoader(WinDS(eval_frames),256,False):\n",
    "        xb,xs,swb=[t.to(DEVICE) for t in (xb,xs,swb)]\n",
    "        mu.append(model(xb,xs,swb).cpu().numpy())\n",
    "        swids.append(swb.cpu().numpy())\n",
    "MU,SWIDX=np.concatenate(mu),np.concatenate(swids)\n",
    "QTRS=months_to_quarters(MU)\n",
    "ym=month_range(TARGET_YEAR,TARGET_YEAR)\n",
    "rows_q,rows_y=[],[]\n",
    "for i,ix in enumerate(SWIDX):\n",
    "    sw=switches[ix]\n",
    "    rows_y.append(dict(SWITCH_ID=sw,YEAR=TARGET_YEAR,PRED_FULL_YEAR=float(MU[i].sum())))\n",
    "    for qi,qsum in enumerate(QTRS[i],1):\n",
    "        rows_q.append(dict(SWITCH_ID=sw,YEAR=TARGET_YEAR,QUARTER=qi,PRED_FAULTS_Q=float(qsum)))\n",
    "q_preds = pd.DataFrame(rows_q)\n",
    "y_preds = pd.DataFrame(rows_y)\n",
    "log(f\"Generated quarterly and yearly forecasts for {TARGET_YEAR}.\")\n",
    "\n",
    "# ─── 4 · PRE-PROCESSING FOR DYNAMIC FACTORS ─────────────────────────────\n",
    "log(\"\\nSTEP 4: Pre-processing cable master data for dynamic calculations...\")\n",
    "cables_master = pd.read_csv(CABLE_CSV, low_memory=False)\n",
    "def parse_date_from_col(col_name, prefix):\n",
    "    try:\n",
    "        date_str = col_name.replace(f\"{prefix}_Month_\", \"\")\n",
    "        return pd.to_datetime(date_str, format='%Y%m')\n",
    "    except: return None\n",
    "cycle_cols = {c: parse_date_from_col(c, \"CYCLE\") for c in cables_master.columns if c.startswith(\"CYCLE_Month_\")}\n",
    "var_cols = {c: parse_date_from_col(c, \"VAR\") for c in cables_master.columns if c.startswith(\"VAR_Month_\")}\n",
    "cycle_cols = {k: v for k, v in cycle_cols.items() if v is not None}\n",
    "var_cols = {k: v for k, v in var_cols.items() if v is not None}\n",
    "log(f\"Found {len(cycle_cols)} monthly cycle columns and {len(var_cols)} monthly variation columns.\")\n",
    "\n",
    "# ─── 5 · QUARTERLY HEALTH SCORE WITH DYNAMIC LOADING ────────────────────\n",
    "log(\"\\nSTEP 5: Calculating Health Scores per Quarter with dynamic loading...\")\n",
    "QUARTERS = {\n",
    "    \"Q1_Jan-Mar\": (1, 3), \"Q2_Apr-Jun\": (4, 6),\n",
    "    \"Q3_Jul-Sep\": (7, 9), \"Q4_Oct-Dec\": (10, 12)\n",
    "}\n",
    "LABELS = {\n",
    "    \"c\":\"High cyclic loading (c)\", \"r\":\"Wide load-range utilisation (r)\",\n",
    "    \"a\":\"Advanced cable age (a)\", \"f\":\"Many historic faults (f)\",\n",
    "    \"s\":\"Numerous joints / segments (s)\", \"p\":\"High predicted faults per km (p)\",\n",
    "    \"i\":\"Frequent interruptions (i)\", \"l\":\"Long circuit length (ℓ)\",\n",
    "    \"u\":\"Faults in same quarter last year (u)\"\n",
    "}\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for i, (q_name, (start_month, end_month)) in enumerate(QUARTERS.items()):\n",
    "    q_num = i + 1\n",
    "    log(f\"\\n--- Processing {q_name} ({TARGET_YEAR}) ---\")\n",
    "    q_out_dir = OUT_DIR / q_name\n",
    "    q_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    q_start_date = pd.Timestamp(f\"{TARGET_YEAR}-{start_month}-01\")\n",
    "    loading_start_date = q_start_date - pd.DateOffset(years=1)\n",
    "    cab = cables_master.drop_duplicates(\"DESTINATION_SWITCH_ID\").rename(columns={\n",
    "        \"DESTINATION_SWITCH_ID\":\"SWITCH_ID\", \"MEASUREDLENGTH\":\"LENGTH_M\",\n",
    "        \"COMMISSIONEDDATE\":\"DATE_INSTALLED\", \"NO_OF_SEGMENT\":\"SEGMENTS\"})\n",
    "    cab[\"DATE_INSTALLED\"] = pd.to_datetime(cab[\"DATE_INSTALLED\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "    cab[\"LENGTH_KM\"] = pd.to_numeric(cab[\"LENGTH_M\"], errors=\"coerce\") / 1000\n",
    "    relevant_cycle_cols = [c for c, dt in cycle_cols.items() if loading_start_date <= dt < q_start_date]\n",
    "    relevant_var_cols = [c for c, dt in var_cols.items() if loading_start_date <= dt < q_start_date]\n",
    "    if relevant_cycle_cols: cab[\"cycle_pm\"] = cab[relevant_cycle_cols].mean(axis=1)\n",
    "    else: cab[\"cycle_pm\"] = 0\n",
    "    if relevant_var_cols:\n",
    "        median_of_period = cab[relevant_var_cols].median(axis=1)\n",
    "        cab[\"load_range_idx\"] = cab[relevant_var_cols].mean(axis=1) / median_of_period.replace(0, np.nan)\n",
    "    else: cab[\"load_range_idx\"] = 0\n",
    "    cab[\"c_raw\"] = cab[\"cycle_pm\"]; cab[\"r_raw\"] = cab[\"load_range_idx\"]\n",
    "    hist = fault[fault[\"TIME_OUTAGE\"] < q_start_date].groupby(\"SWITCH_ID\").size()\n",
    "    cab = cab.merge(hist.rename(\"hist_faults\"), on=\"SWITCH_ID\", how=\"left\").fillna({\"hist_faults\": 0})\n",
    "    cab[\"f_raw\"] = cab[\"hist_faults\"] / cab[\"LENGTH_KM\"].replace(0, np.nan)\n",
    "    q_pred_current = q_preds[q_preds['QUARTER'] == q_num]\n",
    "    cab = cab.merge(q_pred_current[['SWITCH_ID', 'PRED_FAULTS_Q']], on=\"SWITCH_ID\", how=\"left\").fillna({\"PRED_FAULTS_Q\": 0})\n",
    "    cab[\"p_raw\"] = cab[\"PRED_FAULTS_Q\"].replace(0, np.nan)\n",
    "    fault_sorted = fault[fault[\"TIME_OUTAGE\"] < q_start_date].sort_values([\"SWITCH_ID\", \"TIME_OUTAGE\"])\n",
    "    fault_sorted[\"Δt_h\"] = (fault_sorted.groupby(\"SWITCH_ID\")[\"TIME_OUTAGE\"].diff().dt.total_seconds().div(3600))\n",
    "    mean_dt = fault_sorted.groupby(\"SWITCH_ID\")[\"Δt_h\"].mean().rename(\"mean_h\")\n",
    "    cab = cab.merge(mean_dt, on=\"SWITCH_ID\", how=\"left\")\n",
    "    cab[\"i_raw\"] = 1 / cab[\"mean_h\"].clip(lower=1)\n",
    "    last_year = TARGET_YEAR - 1\n",
    "    same_q_faults_last_year = fault[\n",
    "        (fault[\"TIME_OUTAGE\"].dt.year == last_year) &\n",
    "        (fault[\"TIME_OUTAGE\"].dt.month.between(start_month, end_month))\n",
    "    ].groupby(\"SWITCH_ID\").size()\n",
    "    cab = cab.merge(same_q_faults_last_year.rename(\"faults_same_q_last_yr\"), on=\"SWITCH_ID\", how=\"left\").fillna({\"faults_same_q_last_yr\": 0})\n",
    "    cab[\"u_raw\"] = cab[\"faults_same_q_last_yr\"]\n",
    "    cab[\"a_raw\"] = (q_start_date - cab[\"DATE_INSTALLED\"]).dt.days / (EXPECTED_LIFE_YEARS * 365)\n",
    "    cab[\"s_raw\"] = (cab[\"SEGMENTS\"].fillna(1) - 1).clip(lower=0)\n",
    "    cab[\"l_raw\"] = np.log1p(cab[\"LENGTH_KM\"])\n",
    "    for col in [\"a_raw\", \"c_raw\", \"f_raw\", \"i_raw\", \"l_raw\", \"p_raw\", \"r_raw\", \"s_raw\", \"u_raw\"]:\n",
    "        cab[col[0]] = robust(cab[col])\n",
    "    risk = sum(W[k] * cab[k] for k in W.keys())\n",
    "    cab[\"health_score\"] = np.rint(100 * (1 - risk)).clip(0, 100).astype(int)\n",
    "    cab[\"health_score_10\"] = np.clip(np.ceil(cab[\"health_score\"] / 10), 1, 10).astype(int)\n",
    "    cab[\"health_band\"] = pd.cut(cab[\"health_score\"], [-np.inf, 40, 70, 100], labels=[\"Poor\", \"Moderate\", \"Good\"])\n",
    "    for k, wv in W.items(): cab[f\"weight_{k}\"] = wv\n",
    "    risk_contrib = pd.DataFrame({k: W[k] * cab[k] for k in W.keys()})\n",
    "    top3 = risk_contrib.apply(lambda r: r.nlargest(3).index.tolist(), axis=1)\n",
    "    cab[\"primary_health_driver\"] = top3.apply(lambda lst: LABELS[lst[0]])\n",
    "    cab[\"top3_health_drivers\"] = top3.apply(lambda lst: \"; \".join(LABELS[k] for k in lst))\n",
    "    mask_nofault = cab[\"hist_faults\"].eq(0)\n",
    "    cab.loc[mask_nofault, [\"health_score\", \"health_score_10\"]] = [100, 10]\n",
    "    cab.loc[mask_nofault, \"health_band\"] = \"Good\"\n",
    "    cab.loc[mask_nofault, \"primary_health_driver\"] = \"No recorded faults\"\n",
    "    cab.loc[mask_nofault, \"top3_health_drivers\"] = \"No recorded faults\"\n",
    "    output_cols = [c for c in cab.columns if c not in cycle_cols and c not in var_cols]\n",
    "    cab[output_cols].to_csv(q_out_dir / f\"cable_health_{TARGET_YEAR}_{q_name}_scored.csv\", index=False)\n",
    "    cab[output_cols][~mask_nofault].to_csv(q_out_dir / f\"cable_health_{TARGET_YEAR}_{q_name}_predictions.csv\", index=False)\n",
    "    log(f\"Saved results for {len(cab)} cables to {q_out_dir.name}/\")\n",
    "    actual_faults_q = fault[(fault[\"TIME_OUTAGE\"].dt.year == TARGET_YEAR) & (fault[\"TIME_OUTAGE\"].dt.month.between(start_month, end_month))]\n",
    "    actual_switches_q = actual_faults_q[\"SWITCH_ID\"].unique()\n",
    "    cab[\"ACTUAL_FAIL_Q\"] = cab[\"SWITCH_ID\"].isin(actual_switches_q).astype(int)\n",
    "    pred = cab[\"health_band\"].map({\"Poor\": 1, \"Moderate\": 1, \"Good\": 0})\n",
    "    if len(cab[\"ACTUAL_FAIL_Q\"].unique()) > 1:\n",
    "        cm = confusion_matrix(cab[\"ACTUAL_FAIL_Q\"], pred)\n",
    "        au = roc_auc_score(cab[\"ACTUAL_FAIL_Q\"], 100 - cab[\"health_score\"])\n",
    "        log(f\"{q_name} confusion matrix:\\n{cm}\\nAUROC = {au:.3f}\")\n",
    "    else:\n",
    "        log(f\"Skipping validation for {q_name}: Only one class present in actuals.\")\n",
    "\n",
    "# --- Final Summary ---\n",
    "total_predicted_faults = y_preds['PRED_FULL_YEAR'].sum()\n",
    "total_actual_faults_2024 = fault[fault['TIME_OUTAGE'].dt.year == TARGET_YEAR].shape[0]\n",
    "\n",
    "log(\"\\n\" + \"=\"*45)\n",
    "log(f\"FLEET-WIDE FAULT SUMMARY FOR FULL YEAR {TARGET_YEAR}\")\n",
    "log(\"=\"*45)\n",
    "log(f\"Total Predicted Faults (Yearly): {total_predicted_faults:.1f}\")\n",
    "log(f\"Total Actual Faults (Yearly):    {total_actual_faults_2024}\")\n",
    "log(\"=\"*45)\n",
    "log(f\"\\nPipeline complete → Quarterly results saved in: {OUT_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a183955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:43:38 | STEP 1: Processing fault history...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:43:38 | Processed fault history for 210 switches.\n",
      "16:43:38 | \n",
      "STEP 2: Building dataset and training Poisson-LSTM model...\n",
      "16:43:41 | Training finished in 4 epochs | best val-loss=0.1891\n",
      "16:43:41 | \n",
      "STEP 3: Generating forecasts for target year...\n",
      "16:43:41 | Generated quarterly and yearly forecasts for 2024.\n",
      "16:43:41 | \n",
      "STEP 4: Pre-processing cable master data for dynamic calculations...\n",
      "16:43:41 | Found 0 monthly cycle columns and 0 monthly variation columns.\n",
      "16:43:41 | \n",
      "STEP 5: Calculating Health Scores per Quarter with dynamic loading...\n",
      "16:43:41 | \n",
      "--- Processing Q1_Jan-Mar (2024) ---\n",
      "16:43:42 | Saved results for 256 cables to Q1_Jan-Mar/\n",
      "16:43:42 | Saved confusion-matrix plot → /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/A_fault_model_with_health_QUARTERLY/Q1_Jan-Mar/confusion_matrix_2024_Q1_Jan-Mar.png\n",
      "16:43:42 | Q1_Jan-Mar AUROC = 0.778\n",
      "16:43:42 | \n",
      "--- Processing Q2_Apr-Jun (2024) ---\n",
      "16:43:42 | Saved results for 256 cables to Q2_Apr-Jun/\n",
      "16:43:42 | Saved confusion-matrix plot → /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/A_fault_model_with_health_QUARTERLY/Q2_Apr-Jun/confusion_matrix_2024_Q2_Apr-Jun.png\n",
      "16:43:42 | Q2_Apr-Jun AUROC = 0.711\n",
      "16:43:42 | \n",
      "--- Processing Q3_Jul-Sep (2024) ---\n",
      "16:43:42 | Saved results for 256 cables to Q3_Jul-Sep/\n",
      "16:43:42 | Saved confusion-matrix plot → /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/A_fault_model_with_health_QUARTERLY/Q3_Jul-Sep/confusion_matrix_2024_Q3_Jul-Sep.png\n",
      "16:43:42 | Q3_Jul-Sep AUROC = 0.864\n",
      "16:43:42 | \n",
      "--- Processing Q4_Oct-Dec (2024) ---\n",
      "16:43:42 | Saved results for 256 cables to Q4_Oct-Dec/\n",
      "16:43:43 | Saved confusion-matrix plot → /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/A_fault_model_with_health_QUARTERLY/Q4_Oct-Dec/confusion_matrix_2024_Q4_Oct-Dec.png\n",
      "16:43:43 | Q4_Oct-Dec AUROC = 0.699\n",
      "16:43:43 | \n",
      "=============================================\n",
      "16:43:43 | FLEET-WIDE FAULT SUMMARY FOR FULL YEAR 2024\n",
      "16:43:43 | =============================================\n",
      "16:43:43 | Total Predicted Faults (Yearly): 124.6\n",
      "16:43:43 | Total Actual Faults (Yearly):    128\n",
      "16:43:43 | =============================================\n",
      "16:43:43 | \n",
      "Pipeline complete → Quarterly results saved in: /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/A_fault_model_with_health_QUARTERLY\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "HT-cable Poisson-LSTM forecast + 9-factor health score (2018-2024)\n",
    "2025-08-19 — QUARTERLY HEALTH SCORE + AGGRESSIVE WEIGHTS\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import logging, math, sys, json, numpy as np, pandas as pd\n",
    "import torch, torch.nn as nn\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ── NEW: plotting (headless-safe) ────────────────────────────────────────\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ─── paths ──────────────────────────────────────────────────────────────\n",
    "FAULT_CSV = Path(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/FAULT DATA/HT_fault_cable_info_processed_with_affected.csv\")\n",
    "CABLE_CSV = Path(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/SWNO_MASTER_COMBINED_FULL_FINAL3.csv\")\n",
    "OUT_DIR   = Path(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/A_fault_model_with_health_QUARTERLY\")\n",
    "\n",
    "# ─── constants / hyper-params ───────────────────────────────────────────\n",
    "MIN_YEAR, TARGET_YEAR   = 2016, 2024\n",
    "TRAIN_YEARS             = list(range(2018, TARGET_YEAR))\n",
    "KEEP_VOLTAGES           = {22, 33}\n",
    "EXPECTED_LIFE_YEARS     = 35\n",
    "\n",
    "# Aggressive weights\n",
    "W = dict(\n",
    "    a=0.01500,  # Age (decreased)\n",
    "    c=0.10000,  # Cycle Count (increased)\n",
    "    f=0.155000,  # Historic Faults (increased)\n",
    "    i=0.14000,  # MTBF (increased)\n",
    "    l=0.15000,  # Length (decreased)\n",
    "    p=0.25000,  # Predicted Faults (increased)\n",
    "    r=0.06500,  # Daily Variation (increased)\n",
    "    s=0.05000,  # Segments (decreased)\n",
    "    u=0.10000,  # Same Qtr Last Year (decreased)\n",
    ")\n",
    "\n",
    "BATCH, EPOCHS, PATIENCE = 64, 40, 12\n",
    "LR, WD, CLIP            = 1e-3, 1e-5, 1.0\n",
    "HID, LAY, DROP, EMB     = 512, 2, 0.1, 16\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42); np.random.seed(42)\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s | %(message)s\",\n",
    "                    datefmt=\"%H:%M:%S\", level=logging.INFO, stream=sys.stdout)\n",
    "log = logging.getLogger(\"pipeline\").info\n",
    "\n",
    "# ─── helpers ────────────────────────────────────────────────────────────\n",
    "def v_to_num(v):\n",
    "    try: return float(str(v).lower().replace(\"kv\",\"\"))\n",
    "    except: return math.nan\n",
    "\n",
    "def norm_sw(s):\n",
    "    return (s.astype(str).str.upper().str.strip()\n",
    "             .str.replace(r\"^(SWNO_|SWNO|SW|S)\\s*\", \"\", regex=True)\n",
    "             .str.replace(r\"\\D+\",\"\",regex=True)\n",
    "             .replace(\"\", np.nan)).astype(\"Int64\")\n",
    "\n",
    "def month_range(a,b): return pd.period_range(f\"{a}-01\", f\"{b}-12\", freq=\"M\")\n",
    "\n",
    "def sincos(idx):\n",
    "    ang = 2*np.pi*idx.month.values/12\n",
    "    return np.stack([np.sin(ang), np.cos(ang)], 1)\n",
    "\n",
    "def robust(s, lo=5, hi=95):\n",
    "    c=s.replace([np.inf,-np.inf],np.nan).dropna()\n",
    "    if c.empty: return pd.Series(0., index=s.index)\n",
    "    a,b=np.percentile(c,[lo,hi]); return ((s-a).clip(0,b-a)/(b-a+1e-9)).fillna(0.)\n",
    "\n",
    "def months_to_quarters(mat):\n",
    "    q=np.zeros((mat.shape[0],4))\n",
    "    q[:,0]=mat[:,0:3].sum(1); q[:,1]=mat[:,3:6].sum(1)\n",
    "    q[:,2]=mat[:,6:9].sum(1); q[:,3]=mat[:,9:12].sum(1)\n",
    "    return q\n",
    "\n",
    "# ── NEW: confusion-matrix plot helper ───────────────────────────────────\n",
    "def save_confusion_matrix(cm: np.ndarray, save_path: Path, title: str):\n",
    "    \"\"\"Save a 2×2 confusion matrix as a blue heatmap with counts.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    im = ax.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "    fig.colorbar(im, ax=ax)\n",
    "\n",
    "    ax.set(\n",
    "        xticks=[0, 1], yticks=[0, 1],\n",
    "        xticklabels=[\"Pred OK\", \"Pred Fail\"],\n",
    "        yticklabels=[\"Actual OK\", \"Actual Fail\"],\n",
    "        xlabel=\"Prediction\", ylabel=\"Reality\", title=title,\n",
    "    )\n",
    "\n",
    "    vmax = cm.max() if cm.size else 1\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, f\"{int(cm[i, j])}\",\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=(\"white\" if cm[i, j] > vmax/2 else \"black\"),\n",
    "                    fontsize=12)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "# ─── 1 · fault history ──────────────────────────────────────────────────\n",
    "log(\"STEP 1: Processing fault history...\")\n",
    "fault=pd.read_csv(FAULT_CSV,parse_dates=[\"TIME_OUTAGE\"],low_memory=False)\n",
    "sw_col=\"TO_SWITCH\" if \"TO_SWITCH\" in fault.columns else fault.columns[0]\n",
    "fault[\"SWITCH_ID\"]=norm_sw(fault[sw_col])\n",
    "fault=fault.dropna(subset=[\"SWITCH_ID\",\"TIME_OUTAGE\"])\n",
    "fault[\"TIME_OUTAGE\"] = fault[\"TIME_OUTAGE\"].dt.tz_localize(None)\n",
    "fault=fault[fault[\"TIME_OUTAGE\"].dt.year.between(MIN_YEAR,TARGET_YEAR)]\n",
    "if \"VOLTAGE\" in fault.columns:\n",
    "    fault[\"VNUM\"]=fault[\"VOLTAGE\"].apply(v_to_num)\n",
    "    fault=fault[fault[\"VNUM\"].isin(KEEP_VOLTAGES)]\n",
    "fault=fault.drop_duplicates([\"SWITCH_ID\",\"TIME_OUTAGE\"])\n",
    "fault[\"YM\"]=fault[\"TIME_OUTAGE\"].dt.to_period(\"M\")\n",
    "idx_full=month_range(MIN_YEAR,TARGET_YEAR)\n",
    "counts=(fault[fault[\"TIME_OUTAGE\"].dt.year<=TARGET_YEAR-1]\n",
    "        .groupby([\"SWITCH_ID\",\"YM\"]).size()\n",
    "        .unstack(fill_value=0)\n",
    "        .reindex(columns=idx_full,fill_value=0).astype(float))\n",
    "switches=counts.index.tolist(); sw2idx={sw:i for i,sw in enumerate(switches)}\n",
    "log(f\"Processed fault history for {len(switches)} switches.\")\n",
    "\n",
    "# ─── 2 · Poisson-LSTM dataset & training ────────────────────────────────\n",
    "log(\"\\nSTEP 2: Building dataset and training Poisson-LSTM model...\")\n",
    "class WinDS(Dataset):\n",
    "    def __init__(self, fr):\n",
    "        self.x  = torch.tensor(fr[\"X_seq\"],dtype=torch.float32)\n",
    "        self.xs = torch.tensor(fr[\"X_season\"],dtype=torch.float32)\n",
    "        self.y  = torch.tensor(fr[\"y_seq\"],dtype=torch.float32)\n",
    "        self.sw = torch.tensor(fr[\"sw_idx\"],dtype=torch.long)\n",
    "    def __len__(self): return len(self.sw)\n",
    "    def __getitem__(self,i): return self.x[i],self.xs[i],self.y[i],self.sw[i]\n",
    "\n",
    "class PoissonLSTM(nn.Module):\n",
    "    def __init__(self,n_sw):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(n_sw, EMB)\n",
    "        self.rnn = nn.LSTM(3,HID,LAY,batch_first=True,\n",
    "                           dropout=DROP if LAY>1 else 0.)\n",
    "        self.head=nn.Sequential(nn.Linear(HID+EMB,HID),nn.ReLU(),\n",
    "                                nn.Linear(HID,HID//2),nn.ReLU(),\n",
    "                                nn.Linear(HID//2,12))\n",
    "        self.sp = nn.Softplus()\n",
    "    def forward(self,x,xs,sw):\n",
    "        h,_=self.rnn(torch.cat([x,xs],-1))\n",
    "        h=torch.cat([h[:,-1],self.emb(sw)],1)\n",
    "        return self.sp(self.head(h))\n",
    "\n",
    "def build_frames(years, nonzero=True):\n",
    "    X_seq,X_sea,y_seq,sw_idx=[],[],[],[]\n",
    "    for sw,row in counts.iterrows():\n",
    "        for Y in years:\n",
    "            tr,tg=month_range(Y-2,Y-1),month_range(Y,Y)\n",
    "            if nonzero and row[tr].sum()==0: continue\n",
    "            X_seq.append(np.log1p(row[tr]).values[:,None])\n",
    "            X_sea.append(sincos(tr)); y_seq.append(row[tg].values)\n",
    "            sw_idx.append(sw2idx[sw])\n",
    "    return dict(X_seq=np.stack(X_seq), X_season=np.stack(X_sea),\n",
    "                y_seq=np.stack(y_seq), sw_idx=np.array(sw_idx,dtype=np.int64))\n",
    "\n",
    "train_frames=build_frames(TRAIN_YEARS,True)\n",
    "ds=WinDS(train_frames); perm=np.random.permutation(len(ds))\n",
    "n_val=max(1,int(.1*len(ds)))\n",
    "dl_tr=DataLoader(torch.utils.data.Subset(ds,perm[:-n_val]),BATCH,shuffle=True)\n",
    "dl_va=DataLoader(torch.utils.data.Subset(ds,perm[-n_val:]),BATCH,shuffle=False)\n",
    "\n",
    "model=PoissonLSTM(len(switches)).to(DEVICE)\n",
    "loss_fn,opt=nn.PoissonNLLLoss(log_input=False),torch.optim.Adam(model.parameters(),lr=LR,weight_decay=WD)\n",
    "best,bad,best_ep=1e9,0,0\n",
    "for ep in range(1,EPOCHS+1):\n",
    "    model.train()\n",
    "    for xb,xs,yb,swb in dl_tr:\n",
    "        xb,xs,yb,swb=[t.to(DEVICE) for t in (xb,xs,yb,swb)]\n",
    "        opt.zero_grad(); loss_fn(model(xb,xs,swb),yb).backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),CLIP); opt.step()\n",
    "    with torch.no_grad():\n",
    "        v=np.mean([loss_fn(model(x.to(DEVICE),xs.to(DEVICE),sw.to(DEVICE)),\n",
    "                           y.to(DEVICE)).item() for x,xs,y,sw in dl_va])\n",
    "    if v<best-1e-4: best,bad,best_ep,ckpt=v,0,ep,model.state_dict()\n",
    "    else: bad+=1\n",
    "    if bad>=PATIENCE: break\n",
    "model.load_state_dict(ckpt)\n",
    "log(f\"Training finished in {best_ep} epochs | best val-loss={best:.4f}\")\n",
    "\n",
    "# ─── 3 · TARGET YEAR forecasts ──────────────────────────────────────────\n",
    "log(\"\\nSTEP 3: Generating forecasts for target year...\")\n",
    "eval_frames=build_frames([TARGET_YEAR],False)\n",
    "mu,swids=[],[]\n",
    "with torch.no_grad():\n",
    "    for xb,xs,_,swb in DataLoader(WinDS(eval_frames),256,False):\n",
    "        xb,xs,swb=[t.to(DEVICE) for t in (xb,xs,swb)]\n",
    "        mu.append(model(xb,xs,swb).cpu().numpy())\n",
    "        swids.append(swb.cpu().numpy())\n",
    "MU,SWIDX=np.concatenate(mu),np.concatenate(swids)\n",
    "QTRS=months_to_quarters(MU)\n",
    "ym=month_range(TARGET_YEAR,TARGET_YEAR)\n",
    "\n",
    "rows_q,rows_y=[],[]\n",
    "for i,ix in enumerate(SWIDX):\n",
    "    sw=switches[ix]\n",
    "    rows_y.append(dict(SWITCH_ID=sw,YEAR=TARGET_YEAR,PRED_FULL_YEAR=float(MU[i].sum())))\n",
    "    for qi,qsum in enumerate(QTRS[i],1):\n",
    "        rows_q.append(dict(SWITCH_ID=sw,YEAR=TARGET_YEAR,QUARTER=qi,PRED_FAULTS_Q=float(qsum)))\n",
    "q_preds = pd.DataFrame(rows_q)\n",
    "y_preds = pd.DataFrame(rows_y)\n",
    "log(f\"Generated quarterly and yearly forecasts for {TARGET_YEAR}.\")\n",
    "\n",
    "# ─── 4 · PRE-PROCESSING FOR DYNAMIC FACTORS ─────────────────────────────\n",
    "log(\"\\nSTEP 4: Pre-processing cable master data for dynamic calculations...\")\n",
    "cables_master = pd.read_csv(CABLE_CSV, low_memory=False)\n",
    "\n",
    "def parse_date_from_col(col_name, prefix):\n",
    "    try:\n",
    "        date_str = col_name.replace(f\"{prefix}_Month_\", \"\")\n",
    "        return pd.to_datetime(date_str, format='%Y%m')\n",
    "    except: return None\n",
    "\n",
    "cycle_cols = {c: parse_date_from_col(c, \"CYCLE\") for c in cables_master.columns if c.startswith(\"CYCLE_Month_\")}\n",
    "var_cols   = {c: parse_date_from_col(c, \"VAR\")   for c in cables_master.columns if c.startswith(\"VAR_Month_\")}\n",
    "cycle_cols = {k: v for k, v in cycle_cols.items() if v is not None}\n",
    "var_cols   = {k: v for k, v in var_cols.items() if v is not None}\n",
    "log(f\"Found {len(cycle_cols)} monthly cycle columns and {len(var_cols)} monthly variation columns.\")\n",
    "\n",
    "# ─── 5 · QUARTERLY HEALTH SCORE WITH DYNAMIC LOADING ────────────────────\n",
    "log(\"\\nSTEP 5: Calculating Health Scores per Quarter with dynamic loading...\")\n",
    "QUARTERS = {\n",
    "    \"Q1_Jan-Mar\": (1, 3), \"Q2_Apr-Jun\": (4, 6),\n",
    "    \"Q3_Jul-Sep\": (7, 9), \"Q4_Oct-Dec\": (10, 12)\n",
    "}\n",
    "LABELS = {\n",
    "    \"c\":\"High cyclic loading (c)\", \"r\":\"Wide load-range utilisation (r)\",\n",
    "    \"a\":\"Advanced cable age (a)\", \"f\":\"Many historic faults (f)\",\n",
    "    \"s\":\"Numerous joints / segments (s)\", \"p\":\"High predicted faults per km (p)\",\n",
    "    \"i\":\"Frequent interruptions (i)\", \"l\":\"Long circuit length (ℓ)\",\n",
    "    \"u\":\"Faults in same quarter last year (u)\"\n",
    "}\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for i, (q_name, (start_month, end_month)) in enumerate(QUARTERS.items()):\n",
    "    q_num = i + 1\n",
    "    log(f\"\\n--- Processing {q_name} ({TARGET_YEAR}) ---\")\n",
    "    q_out_dir = OUT_DIR / q_name\n",
    "    q_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    q_start_date = pd.Timestamp(f\"{TARGET_YEAR}-{start_month}-01\")\n",
    "    loading_start_date = q_start_date - pd.DateOffset(years=1)\n",
    "\n",
    "    cab = (cables_master.drop_duplicates(\"DESTINATION_SWITCH_ID\")\n",
    "           .rename(columns={\"DESTINATION_SWITCH_ID\":\"SWITCH_ID\",\n",
    "                            \"MEASUREDLENGTH\":\"LENGTH_M\",\n",
    "                            \"COMMISSIONEDDATE\":\"DATE_INSTALLED\",\n",
    "                            \"NO_OF_SEGMENT\":\"SEGMENTS\"}))\n",
    "\n",
    "    cab[\"DATE_INSTALLED\"] = pd.to_datetime(cab[\"DATE_INSTALLED\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "    cab[\"LENGTH_KM\"] = pd.to_numeric(cab[\"LENGTH_M\"], errors=\"coerce\") / 1000\n",
    "\n",
    "    relevant_cycle_cols = [c for c, dt in cycle_cols.items() if loading_start_date <= dt < q_start_date]\n",
    "    relevant_var_cols   = [c for c, dt in var_cols.items()   if loading_start_date <= dt < q_start_date]\n",
    "\n",
    "    if relevant_cycle_cols: cab[\"cycle_pm\"] = cab[relevant_cycle_cols].mean(axis=1)\n",
    "    else: cab[\"cycle_pm\"] = 0\n",
    "\n",
    "    if relevant_var_cols:\n",
    "        median_of_period = cab[relevant_var_cols].median(axis=1)\n",
    "        cab[\"load_range_idx\"] = cab[relevant_var_cols].mean(axis=1) / median_of_period.replace(0, np.nan)\n",
    "    else:\n",
    "        cab[\"load_range_idx\"] = 0\n",
    "\n",
    "    cab[\"c_raw\"] = cab[\"cycle_pm\"]\n",
    "    cab[\"r_raw\"] = cab[\"load_range_idx\"]\n",
    "\n",
    "    # Historic faults per switch (before quarter)\n",
    "    hist = fault[fault[\"TIME_OUTAGE\"] < q_start_date].groupby(\"SWITCH_ID\").size()\n",
    "    cab = cab.merge(hist.rename(\"hist_faults\"), on=\"SWITCH_ID\", how=\"left\").fillna({\"hist_faults\": 0})\n",
    "    cab[\"f_raw\"] = cab[\"hist_faults\"] / cab[\"LENGTH_KM\"].replace(0, np.nan)\n",
    "\n",
    "    # Predicted quarterly faults\n",
    "    q_pred_current = q_preds[q_preds['QUARTER'] == q_num]\n",
    "    cab = cab.merge(q_pred_current[['SWITCH_ID', 'PRED_FAULTS_Q']], on=\"SWITCH_ID\", how=\"left\").fillna({\"PRED_FAULTS_Q\": 0})\n",
    "    cab[\"p_raw\"] = cab[\"PRED_FAULTS_Q\"].replace(0, np.nan)\n",
    "\n",
    "    # MTBF proxy (inverse mean inter-fault hours)\n",
    "    fault_sorted = fault[fault[\"TIME_OUTAGE\"] < q_start_date].sort_values([\"SWITCH_ID\", \"TIME_OUTAGE\"])\n",
    "    fault_sorted[\"Δt_h\"] = (fault_sorted.groupby(\"SWITCH_ID\")[\"TIME_OUTAGE\"].diff().dt.total_seconds().div(3600))\n",
    "    mean_dt = fault_sorted.groupby(\"SWITCH_ID\")[\"Δt_h\"].mean().rename(\"mean_h\")\n",
    "    cab = cab.merge(mean_dt, on=\"SWITCH_ID\", how=\"left\")\n",
    "    cab[\"i_raw\"] = 1 / cab[\"mean_h\"].clip(lower=1)\n",
    "\n",
    "    # Same quarter last year\n",
    "    last_year = TARGET_YEAR - 1\n",
    "    same_q_faults_last_year = fault[\n",
    "        (fault[\"TIME_OUTAGE\"].dt.year == last_year) &\n",
    "        (fault[\"TIME_OUTAGE\"].dt.month.between(start_month, end_month))\n",
    "    ].groupby(\"SWITCH_ID\").size()\n",
    "    cab = cab.merge(same_q_faults_last_year.rename(\"faults_same_q_last_yr\"), on=\"SWITCH_ID\", how=\"left\").fillna({\"faults_same_q_last_yr\": 0})\n",
    "    cab[\"u_raw\"] = cab[\"faults_same_q_last_yr\"]\n",
    "\n",
    "    # Other factors\n",
    "    cab[\"a_raw\"] = (q_start_date - cab[\"DATE_INSTALLED\"]).dt.days / (EXPECTED_LIFE_YEARS * 365)\n",
    "    cab[\"s_raw\"] = (cab[\"SEGMENTS\"].fillna(1) - 1).clip(lower=0)\n",
    "    cab[\"l_raw\"] = np.log1p(cab[\"LENGTH_KM\"])\n",
    "\n",
    "    # Robust scaling to [0,1]\n",
    "    for col in [\"a_raw\",\"c_raw\",\"f_raw\",\"i_raw\",\"l_raw\",\"p_raw\",\"r_raw\",\"s_raw\",\"u_raw\"]:\n",
    "        cab[col[0]] = robust(cab[col])\n",
    "\n",
    "    # Weighted risk and health\n",
    "    risk = sum(W[k] * cab[k] for k in W.keys())\n",
    "    cab[\"health_score\"]    = np.rint(100 * (1 - risk)).clip(0, 100).astype(int)\n",
    "    cab[\"health_score_10\"] = np.clip(np.ceil(cab[\"health_score\"] / 10), 1, 10).astype(int)\n",
    "    cab[\"health_band\"]     = pd.cut(cab[\"health_score\"], [-np.inf, 40, 60, 100], labels=[\"Poor\", \"Moderate\", \"Good\"])\n",
    "\n",
    "    for k, wv in W.items(): cab[f\"weight_{k}\"] = wv\n",
    "    risk_contrib = pd.DataFrame({k: W[k] * cab[k] for k in W.keys()})\n",
    "    top3 = risk_contrib.apply(lambda r: r.nlargest(3).index.tolist(), axis=1)\n",
    "    cab[\"primary_health_driver\"] = top3.apply(lambda lst: LABELS[lst[0]])\n",
    "    cab[\"top3_health_drivers\"]   = top3.apply(lambda lst: \"; \".join(LABELS[k] for k in lst))\n",
    "\n",
    "    # No-fault switches → force Good\n",
    "    mask_nofault = cab[\"hist_faults\"].eq(0)\n",
    "    cab.loc[mask_nofault, [\"health_score\", \"health_score_10\"]] = [100, 10]\n",
    "    cab.loc[mask_nofault, \"health_band\"] = \"Good\"\n",
    "    cab.loc[mask_nofault, \"primary_health_driver\"] = \"No recorded faults\"\n",
    "    cab.loc[mask_nofault, \"top3_health_drivers\"]   = \"No recorded faults\"\n",
    "\n",
    "    # Save scored tables\n",
    "    output_cols = [c for c in cab.columns if c not in cycle_cols and c not in var_cols]\n",
    "    cab[output_cols].to_csv(q_out_dir / f\"cable_health_{TARGET_YEAR}_{q_name}_scored.csv\", index=False)\n",
    "    cab[output_cols][~mask_nofault].to_csv(q_out_dir / f\"cable_health_{TARGET_YEAR}_{q_name}_predictions.csv\", index=False)\n",
    "    log(f\"Saved results for {len(cab)} cables to {q_out_dir.name}/\")\n",
    "\n",
    "    # ── Validation labels and predictions ────────────────────────────────\n",
    "    actual_faults_q = fault[\n",
    "        (fault[\"TIME_OUTAGE\"].dt.year == TARGET_YEAR) &\n",
    "        (fault[\"TIME_OUTAGE\"].dt.month.between(start_month, end_month))\n",
    "    ]\n",
    "    actual_switches_q = actual_faults_q[\"SWITCH_ID\"].unique()\n",
    "    cab[\"ACTUAL_FAIL_Q\"] = cab[\"SWITCH_ID\"].isin(actual_switches_q).astype(int)\n",
    "\n",
    "    pred = cab[\"health_band\"].map({\"Poor\": 1, \"Moderate\": 1, \"Good\": 0}).fillna(0)\n",
    "    y_true = cab[\"ACTUAL_FAIL_Q\"].astype(int).values\n",
    "    y_pred = pred.astype(int).values\n",
    "\n",
    "    # Always build 2×2 CM so we can save a plot even if a class is missing\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])  # [[TN, FP],[FN, TP]]\n",
    "    cm_path = q_out_dir / f\"confusion_matrix_{TARGET_YEAR}_{q_name}.png\"\n",
    "    save_confusion_matrix(cm, cm_path, title=f\"{q_name} — Confusion Matrix\")\n",
    "    log(f\"Saved confusion-matrix plot → {cm_path}\")\n",
    "\n",
    "    # AUROC only when both classes present\n",
    "    if np.unique(y_true).size > 1:\n",
    "        au = roc_auc_score(y_true, 100 - cab[\"health_score\"])\n",
    "        log(f\"{q_name} AUROC = {au:.3f}\")\n",
    "    else:\n",
    "        log(f\"{q_name} AUROC skipped (only one class present)\")\n",
    "\n",
    "# --- Final Summary ---\n",
    "total_predicted_faults = y_preds['PRED_FULL_YEAR'].sum()\n",
    "total_actual_faults_2024 = fault[fault['TIME_OUTAGE'].dt.year == TARGET_YEAR].shape[0]\n",
    "\n",
    "log(\"\\n\" + \"=\"*45)\n",
    "log(f\"FLEET-WIDE FAULT SUMMARY FOR FULL YEAR {TARGET_YEAR}\")\n",
    "log(\"=\"*45)\n",
    "log(f\"Total Predicted Faults (Yearly): {total_predicted_faults:.1f}\")\n",
    "log(f\"Total Actual Faults (Yearly):    {total_actual_faults_2024}\")\n",
    "log(\"=\"*45)\n",
    "log(f\"\\nPipeline complete → Quarterly results saved in: {OUT_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f16d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd50e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eedbcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698700a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f52f85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afda6ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:41:03 | STEP 1: Processing fault history...\n",
      "16:41:03 | Processed fault history for 210 switches.\n",
      "16:41:03 | \n",
      "STEP 2: Building dataset and training Poisson-LSTM model...\n",
      "16:41:07 | Training finished in 4 epochs | best val-loss=0.1891\n",
      "16:41:07 | \n",
      "STEP 3: Generating forecasts for target year...\n",
      "16:41:08 | Generated semi-annual and yearly forecasts for 2024.\n",
      "16:41:08 | \n",
      "STEP 4: Pre-processing cable master data...\n",
      "16:41:08 | Found 0 monthly cycle columns and 0 monthly variation columns.\n",
      "16:41:08 | \n",
      "STEP 5A: Building training set for health score model...\n",
      "16:41:08 | STEP 5B: Training Logistic Regression model...\n",
      "16:41:08 | Data-Driven Feature Importance:\n",
      "  feature  importance\n",
      "5   p_raw       4.519\n",
      "7   s_raw       0.127\n",
      "8   u_raw       0.094\n",
      "4   l_raw       0.066\n",
      "2   f_raw       0.044\n",
      "3   i_raw       0.011\n",
      "0   a_raw       0.007\n",
      "1   c_raw       0.000\n",
      "6   r_raw       0.000\n",
      "16:41:08 | \n",
      "STEP 6: Calculating Health Scores for 2024 using the trained ML model...\n",
      "16:41:08 | \n",
      "--- Processing H1_Jan-Jun (2024) ---\n",
      "16:41:08 | Saved results for 256 cables to H1_Jan-Jun/\n",
      "16:41:08 | H1_Jan-Jun confusion matrix:\n",
      "[[153  46]\n",
      " [ 26  31]]\n",
      "AUROC = 0.703\n",
      "16:41:08 | \n",
      "--- Processing H2_Jul-Dec (2024) ---\n",
      "16:41:08 | Saved results for 256 cables to H2_Jul-Dec/\n",
      "16:41:08 | H2_Jul-Dec confusion matrix:\n",
      "[[207  16]\n",
      " [ 22  11]]\n",
      "AUROC = 0.756\n",
      "16:41:08 | \n",
      "=============================================\n",
      "16:41:08 | FLEET-WIDE FAULT SUMMARY FOR FULL YEAR 2024\n",
      "16:41:08 | =============================================\n",
      "16:41:08 | Total Predicted Faults (Yearly): 124.6\n",
      "16:41:08 | Total Actual Faults (Yearly):    128\n",
      "16:41:08 | =============================================\n",
      "16:41:08 | \n",
      "Pipeline complete → ML-driven results saved in: /media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/A_fault_model_with_health_ML_WEIGHTS\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "HT-cable Poisson-LSTM forecast + Health Score with Machine Learning Weights\n",
    "2025-08-20 — 6-MONTH PERIODS + DATA-DRIVEN WEIGHTS (LOGISTIC REGRESSION) - FULL SCRIPT\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import logging, math, sys, json, numpy as np, pandas as pd\n",
    "import torch, torch.nn as nn\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ─── paths ──────────────────────────────────────────────────────────────\n",
    "FAULT_CSV = Path(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/FAULT DATA/HT_fault_cable_info_processed_with_affected.csv\")\n",
    "CABLE_CSV = Path(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/DATA_GENERATION/SWNO_MASTER_COMBINED_FULL_FINAL3.csv\")\n",
    "OUT_DIR = Path(\"/media/sagark24/New Volume/MERGE CDIS/IPYNB_FILE/A_fault_model_with_health_ML_WEIGHTS\")\n",
    "\n",
    "# ─── constants / hyper-params ───────────────────────────────────────────\n",
    "MIN_YEAR, TARGET_YEAR   = 2016, 2024\n",
    "TRAIN_YEARS             = list(range(2018, TARGET_YEAR))\n",
    "HISTORY_YEARS           = list(range(2018, TARGET_YEAR)) # Years to build ML training set\n",
    "KEEP_VOLTAGES           = {22, 33}\n",
    "EXPECTED_LIFE_YEARS     = 35\n",
    "\n",
    "# NOTE: The static 'W' dictionary is now replaced by a machine learning model\n",
    "BATCH, EPOCHS, PATIENCE = 64, 40, 12\n",
    "LR, WD, CLIP            = 1e-3, 1e-5, 1.0\n",
    "HID, LAY, DROP, EMB     = 512, 2, 0.1, 16\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42); np.random.seed(42)\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s | %(message)s\",\n",
    "                    datefmt=\"%H:%M:%S\", level=logging.INFO, stream=sys.stdout)\n",
    "log = logging.getLogger(\"pipeline\").info\n",
    "\n",
    "# ─── helper functions ───────────────────────────────────────────────────\n",
    "def v_to_num(v):\n",
    "    try: return float(str(v).lower().replace(\"kv\",\"\"))\n",
    "    except: return math.nan\n",
    "def norm_sw(s):\n",
    "    return (s.astype(str).str.upper().str.strip()\n",
    "             .str.replace(r\"^(SWNO_|SWNO|SW|S)\\s*\", \"\", regex=True)\n",
    "             .str.replace(r\"\\D+\",\"\",regex=True)\n",
    "             .replace(\"\", np.nan)).astype(\"Int64\")\n",
    "def month_range(a,b): return pd.period_range(f\"{a}-01\", f\"{b}-12\", freq=\"M\")\n",
    "def sincos(idx):\n",
    "    ang = 2*np.pi*idx.month.values/12\n",
    "    return np.stack([np.sin(ang), np.cos(ang)], 1)\n",
    "def months_to_halves(mat):\n",
    "    h=np.zeros((mat.shape[0],2))\n",
    "    h[:,0]=mat[:,0:6].sum(1); h[:,1]=mat[:,6:12].sum(1)\n",
    "    return h\n",
    "\n",
    "# ─── 1 · fault history ──────────────────────────────────────────────────\n",
    "log(\"STEP 1: Processing fault history...\")\n",
    "fault=pd.read_csv(FAULT_CSV,parse_dates=[\"TIME_OUTAGE\"],low_memory=False)\n",
    "sw_col=\"TO_SWITCH\" if \"TO_SWITCH\" in fault.columns else fault.columns[0]\n",
    "fault[\"SWITCH_ID\"]=norm_sw(fault[sw_col])\n",
    "fault=fault.dropna(subset=[\"SWITCH_ID\",\"TIME_OUTAGE\"])\n",
    "fault[\"TIME_OUTAGE\"] = fault[\"TIME_OUTAGE\"].dt.tz_localize(None)\n",
    "fault=fault[fault[\"TIME_OUTAGE\"].dt.year.between(MIN_YEAR,TARGET_YEAR)]\n",
    "if \"VOLTAGE\" in fault.columns:\n",
    "    fault[\"VNUM\"]=fault[\"VOLTAGE\"].apply(v_to_num)\n",
    "    fault=fault[fault[\"VNUM\"].isin(KEEP_VOLTAGES)]\n",
    "fault=fault.drop_duplicates([\"SWITCH_ID\",\"TIME_OUTAGE\"])\n",
    "fault[\"YM\"]=fault[\"TIME_OUTAGE\"].dt.to_period(\"M\")\n",
    "idx_full=month_range(MIN_YEAR,TARGET_YEAR)\n",
    "counts=(fault[fault[\"TIME_OUTAGE\"].dt.year<=TARGET_YEAR-1]\n",
    "        .groupby([\"SWITCH_ID\",\"YM\"]).size()\n",
    "        .unstack(fill_value=0)\n",
    "        .reindex(columns=idx_full,fill_value=0).astype(float))\n",
    "switches=counts.index.tolist(); sw2idx={sw:i for i,sw in enumerate(switches)}\n",
    "log(f\"Processed fault history for {len(switches)} switches.\")\n",
    "\n",
    "# ─── 2 · Poisson-LSTM dataset & training ────────────────────────────────\n",
    "log(\"\\nSTEP 2: Building dataset and training Poisson-LSTM model...\")\n",
    "class WinDS(Dataset):\n",
    "    def __init__(self, fr):\n",
    "        self.x  = torch.tensor(fr[\"X_seq\"],dtype=torch.float32)\n",
    "        self.xs = torch.tensor(fr[\"X_season\"],dtype=torch.float32)\n",
    "        self.y  = torch.tensor(fr[\"y_seq\"],dtype=torch.float32)\n",
    "        self.sw = torch.tensor(fr[\"sw_idx\"],dtype=torch.long)\n",
    "    def __len__(self): return len(self.sw)\n",
    "    def __getitem__(self,i): return self.x[i],self.xs[i],self.y[i],self.sw[i]\n",
    "class PoissonLSTM(nn.Module):\n",
    "    def __init__(self,n_sw):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(n_sw, EMB)\n",
    "        self.rnn = nn.LSTM(3,HID,LAY,batch_first=True, dropout=DROP if LAY>1 else 0.)\n",
    "        self.head=nn.Sequential(nn.Linear(HID+EMB,HID),nn.ReLU(),\n",
    "                                nn.Linear(HID,HID//2),nn.ReLU(),\n",
    "                                nn.Linear(HID//2,12))\n",
    "        self.sp = nn.Softplus()\n",
    "    def forward(self,x,xs,sw):\n",
    "        h,_=self.rnn(torch.cat([x,xs],-1)); h=torch.cat([h[:,-1],self.emb(sw)],1)\n",
    "        return self.sp(self.head(h))\n",
    "def build_frames(years, nonzero=True):\n",
    "    X_seq,X_sea,y_seq,sw_idx=[],[],[],[]\n",
    "    for sw,row in counts.iterrows():\n",
    "        for Y in years:\n",
    "            tr,tg=month_range(Y-2,Y-1),month_range(Y,Y)\n",
    "            if nonzero and row[tr].sum()==0: continue\n",
    "            X_seq.append(np.log1p(row[tr]).values[:,None])\n",
    "            X_sea.append(sincos(tr)); y_seq.append(row[tg].values)\n",
    "            sw_idx.append(sw2idx[sw])\n",
    "    return dict(X_seq=np.stack(X_seq), X_season=np.stack(X_sea),\n",
    "                y_seq=np.stack(y_seq), sw_idx=np.array(sw_idx,dtype=np.int64))\n",
    "train_frames=build_frames(TRAIN_YEARS,True)\n",
    "ds=WinDS(train_frames); perm=np.random.permutation(len(ds))\n",
    "n_val=max(1,int(.1*len(ds)))\n",
    "dl_tr=DataLoader(torch.utils.data.Subset(ds,perm[:-n_val]),BATCH,shuffle=True)\n",
    "dl_va=DataLoader(torch.utils.data.Subset(ds,perm[-n_val:]),BATCH,shuffle=False)\n",
    "model=PoissonLSTM(len(switches)).to(DEVICE)\n",
    "loss_fn,opt=nn.PoissonNLLLoss(log_input=False),torch.optim.Adam(model.parameters(),lr=LR,weight_decay=WD)\n",
    "best,bad,best_ep=1e9,0,0\n",
    "for ep in range(1,EPOCHS+1):\n",
    "    model.train()\n",
    "    for xb,xs,yb,swb in dl_tr:\n",
    "        xb,xs,yb,swb=[t.to(DEVICE) for t in (xb,xs,yb,swb)]\n",
    "        opt.zero_grad(); loss_fn(model(xb,xs,swb),yb).backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),CLIP); opt.step()\n",
    "    with torch.no_grad():\n",
    "        v=np.mean([loss_fn(model(x.to(DEVICE),xs.to(DEVICE),sw.to(DEVICE)),\n",
    "                           y.to(DEVICE)).item() for x,xs,y,sw in dl_va])\n",
    "    if v<best-1e-4: best,bad,best_ep,ckpt=v,0,ep,model.state_dict()\n",
    "    else: bad+=1\n",
    "    if bad>=PATIENCE: break\n",
    "model.load_state_dict(ckpt)\n",
    "log(f\"Training finished in {best_ep} epochs | best val-loss={best:.4f}\")\n",
    "\n",
    "# ─── 3 · TARGET YEAR forecasts ──────────────────────────────────────────\n",
    "log(\"\\nSTEP 3: Generating forecasts for target year...\")\n",
    "eval_frames=build_frames([TARGET_YEAR],False)\n",
    "mu,swids=[],[]\n",
    "with torch.no_grad():\n",
    "    for xb,xs,_,swb in DataLoader(WinDS(eval_frames),256,False):\n",
    "        xb,xs,swb=[t.to(DEVICE) for t in (xb,xs,swb)]\n",
    "        mu.append(model(xb,xs,swb).cpu().numpy()); swids.append(swb.cpu().numpy())\n",
    "MU,SWIDX=np.concatenate(mu),np.concatenate(swids)\n",
    "HALVES=months_to_halves(MU)\n",
    "rows_h,rows_y=[],[]\n",
    "for i,ix in enumerate(SWIDX):\n",
    "    sw=switches[ix]\n",
    "    rows_y.append(dict(SWITCH_ID=sw,YEAR=TARGET_YEAR,PRED_FULL_YEAR=float(MU[i].sum())))\n",
    "    for hi,hsum in enumerate(HALVES[i],1):\n",
    "        rows_h.append(dict(SWITCH_ID=sw,YEAR=TARGET_YEAR,HALF=hi, PRED_FAULTS_H=float(hsum)))\n",
    "h_preds = pd.DataFrame(rows_h)\n",
    "y_preds = pd.DataFrame(rows_y)\n",
    "log(f\"Generated semi-annual and yearly forecasts for {TARGET_YEAR}.\")\n",
    "\n",
    "# ─── 4 · PRE-PROCESSING ────────────────────────────────────────────────\n",
    "log(\"\\nSTEP 4: Pre-processing cable master data...\")\n",
    "cables_master = pd.read_csv(CABLE_CSV, low_memory=False)\n",
    "def parse_date_from_col(col_name, prefix):\n",
    "    try:\n",
    "        date_str = col_name.replace(f\"{prefix}_Month_\", \"\"); return pd.to_datetime(date_str, format='%Y%m')\n",
    "    except: return None\n",
    "cycle_cols = {c: parse_date_from_col(c, \"CYCLE\") for c in cables_master.columns if c.startswith(\"CYCLE_Month_\")}\n",
    "var_cols = {c: parse_date_from_col(c, \"VAR\") for c in cables_master.columns if c.startswith(\"VAR_Month_\")}\n",
    "cycle_cols = {k: v for k, v in cycle_cols.items() if v is not None}\n",
    "var_cols = {k: v for k, v in var_cols.items() if v is not None}\n",
    "log(f\"Found {len(cycle_cols)} monthly cycle columns and {len(var_cols)} monthly variation columns.\")\n",
    "\n",
    "# ─── 5. NEW: Train ML Model to Find Optimal Weights ───────────────────\n",
    "def train_health_model(history_years, fault_df, cable_df, cycle_cols, var_cols):\n",
    "    log(\"\\nSTEP 5A: Building training set for health score model...\")\n",
    "    training_data = []\n",
    "    PERIODS = {\"H1\": (1, 6), \"H2\": (7, 12)}\n",
    "    feature_names = ['a_raw', 'c_raw', 'f_raw', 'i_raw', 'l_raw', 'p_raw', 'r_raw', 's_raw', 'u_raw']\n",
    "    \n",
    "    base_cab = cable_df.drop_duplicates(\"DESTINATION_SWITCH_ID\").rename(columns={\n",
    "        \"DESTINATION_SWITCH_ID\":\"SWITCH_ID\", \"MEASUREDLENGTH\":\"LENGTH_M\",\n",
    "        \"COMMISSIONEDDATE\":\"DATE_INSTALLED\", \"NO_OF_SEGMENT\":\"SEGMENTS\"})\n",
    "    base_cab[\"DATE_INSTALLED\"] = pd.to_datetime(base_cab[\"DATE_INSTALLED\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "    base_cab[\"LENGTH_KM\"] = pd.to_numeric(base_cab[\"LENGTH_M\"], errors=\"coerce\") / 1000\n",
    "\n",
    "    for year in history_years:\n",
    "        for p_name, (start_month, end_month) in PERIODS.items():\n",
    "            cab = base_cab.copy()\n",
    "            p_start_date = pd.Timestamp(f\"{year}-{start_month}-01\")\n",
    "            \n",
    "            # --- Calculate all 9 factors for this historical period ---\n",
    "            cab[\"a_raw\"] = (p_start_date - cab[\"DATE_INSTALLED\"]).dt.days / (EXPECTED_LIFE_YEARS * 365)\n",
    "            hist = fault_df[fault_df[\"TIME_OUTAGE\"] < p_start_date].groupby(\"SWITCH_ID\").size()\n",
    "            cab = cab.merge(hist.rename(\"hist_faults\"), on=\"SWITCH_ID\", how=\"left\").fillna(0)\n",
    "            cab[\"f_raw\"] = cab[\"hist_faults\"] / cab[\"LENGTH_KM\"].replace(0, np.nan)\n",
    "            fault_sorted = fault_df[fault_df[\"TIME_OUTAGE\"] < p_start_date].sort_values([\"SWITCH_ID\", \"TIME_OUTAGE\"])\n",
    "            fault_sorted[\"Δt_h\"] = (fault_sorted.groupby(\"SWITCH_ID\")[\"TIME_OUTAGE\"].diff().dt.total_seconds().div(3600))\n",
    "            mean_dt = fault_sorted.groupby(\"SWITCH_ID\")[\"Δt_h\"].mean().rename(\"mean_h\")\n",
    "            cab = cab.merge(mean_dt, on=\"SWITCH_ID\", how=\"left\"); cab[\"i_raw\"] = 1 / cab[\"mean_h\"].clip(lower=1)\n",
    "            cab[\"s_raw\"] = (cab[\"SEGMENTS\"].fillna(1) - 1).clip(lower=0)\n",
    "            cab[\"l_raw\"] = np.log1p(cab[\"LENGTH_KM\"])\n",
    "            \n",
    "            last_year = year - 1\n",
    "            same_period_faults = fault_df[(fault_df[\"TIME_OUTAGE\"].dt.year == last_year) & (fault_df[\"TIME_OUTAGE\"].dt.month.between(start_month, end_month))].groupby(\"SWITCH_ID\").size()\n",
    "            cab = cab.merge(same_period_faults.rename(\"u_raw\"), on=\"SWITCH_ID\", how=\"left\").fillna(0)\n",
    "            \n",
    "            actual_faults_in_period = fault_df[(fault_df[\"TIME_OUTAGE\"].dt.year == year) & (fault_df[\"TIME_OUTAGE\"].dt.month.between(start_month, end_month))].groupby(\"SWITCH_ID\").size()\n",
    "            cab = cab.merge(actual_faults_in_period.rename(\"p_raw\"), on=\"SWITCH_ID\", how=\"left\").fillna(0)\n",
    "            \n",
    "            loading_start_date = p_start_date - pd.DateOffset(years=1)\n",
    "            rel_cyc_cols = [c for c, dt in cycle_cols.items() if loading_start_date <= dt < p_start_date]\n",
    "            rel_var_cols = [c for c, dt in var_cols.items() if loading_start_date <= dt < p_start_date]\n",
    "            cab[\"c_raw\"] = cab[rel_cyc_cols].mean(axis=1) if rel_cyc_cols else 0\n",
    "            if rel_var_cols:\n",
    "                median = cab[rel_var_cols].median(axis=1)\n",
    "                cab[\"r_raw\"] = cab[rel_var_cols].mean(axis=1) / median.replace(0, np.nan)\n",
    "            else: cab[\"r_raw\"] = 0\n",
    "            \n",
    "            faulty_switches = actual_faults_in_period.index.unique()\n",
    "            cab[\"ACTUAL_FAIL\"] = cab[\"SWITCH_ID\"].isin(faulty_switches).astype(int)\n",
    "            training_data.append(cab)\n",
    "\n",
    "    training_df = pd.concat(training_data, ignore_index=True).fillna(0)\n",
    "    \n",
    "    log(\"STEP 5B: Training Logistic Regression model...\")\n",
    "    X_train = training_df[feature_names].replace([np.inf, -np.inf], 0)\n",
    "    y_train = training_df[\"ACTUAL_FAIL\"]\n",
    "    \n",
    "    scaler = StandardScaler(); X_train_scaled = scaler.fit_transform(X_train)\n",
    "    model = LogisticRegression(class_weight='balanced', random_state=42, solver='liblinear', C=0.1)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    feature_importance = pd.DataFrame({'feature': feature_names, 'importance': np.abs(model.coef_[0])})\n",
    "    feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "    log(\"Data-Driven Feature Importance:\\n\" + str(feature_importance.round(3)))\n",
    "    \n",
    "    return model, scaler, feature_importance\n",
    "\n",
    "# ─── 6. MAIN EXECUTION with ML Model ────────────────────────────────────\n",
    "model_ml, scaler, feature_importance = train_health_model(HISTORY_YEARS, fault, cables_master, cycle_cols, var_cols)\n",
    "\n",
    "log(\"\\nSTEP 6: Calculating Health Scores for {} using the trained ML model...\".format(TARGET_YEAR))\n",
    "PERIODS = {\"H1_Jan-Jun\": (1, 6), \"H2_Jul-Dec\": (7, 12)}\n",
    "LABELS = {\n",
    "    \"a_raw\":\"Advanced cable age (a)\", \"c_raw\":\"High cyclic loading (c)\", \"f_raw\":\"Many historic faults (f)\",\n",
    "    \"i_raw\":\"Frequent interruptions (i)\", \"l_raw\":\"Long circuit length (l)\", \"p_raw\":\"High predicted faults (p)\",\n",
    "    \"r_raw\":\"Wide load-range utilisation (r)\", \"s_raw\":\"Numerous joints / segments (s)\", \"u_raw\":\"Faults in same period last year (u)\"\n",
    "}\n",
    "base_cab = cables_master.drop_duplicates(\"DESTINATION_SWITCH_ID\").rename(columns={\n",
    "    \"DESTINATION_SWITCH_ID\":\"SWITCH_ID\", \"MEASUREDLENGTH\":\"LENGTH_M\",\n",
    "    \"COMMISSIONEDDATE\":\"DATE_INSTALLED\", \"NO_OF_SEGMENT\":\"SEGMENTS\"})\n",
    "base_cab[\"DATE_INSTALLED\"] = pd.to_datetime(base_cab[\"DATE_INSTALLED\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "base_cab[\"LENGTH_KM\"] = pd.to_numeric(base_cab[\"LENGTH_M\"], errors=\"coerce\") / 1000\n",
    "\n",
    "for i, (p_name, (start_month, end_month)) in enumerate(PERIODS.items()):\n",
    "    p_num = i + 1; log(f\"\\n--- Processing {p_name} ({TARGET_YEAR}) ---\")\n",
    "    p_out_dir = OUT_DIR / p_name; p_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    cab = base_cab.copy()\n",
    "    p_start_date = pd.Timestamp(f\"{TARGET_YEAR}-{start_month}-01\")\n",
    "    \n",
    "    # --- Calculate all 9 factors for the target period ---\n",
    "    cab[\"a_raw\"] = (p_start_date - cab[\"DATE_INSTALLED\"]).dt.days/(EXPECTED_LIFE_YEARS*365)\n",
    "    hist = fault[fault[\"TIME_OUTAGE\"]<p_start_date].groupby(\"SWITCH_ID\").size()\n",
    "    cab = cab.merge(hist.rename(\"hist_faults\"),on=\"SWITCH_ID\",how=\"left\").fillna(0)\n",
    "    cab[\"f_raw\"]=cab[\"hist_faults\"]/cab[\"LENGTH_KM\"].replace(0,np.nan)\n",
    "    fault_sorted=fault[fault[\"TIME_OUTAGE\"]<p_start_date].sort_values([\"SWITCH_ID\",\"TIME_OUTAGE\"])\n",
    "    fault_sorted[\"Δt_h\"]=(fault_sorted.groupby(\"SWITCH_ID\")[\"TIME_OUTAGE\"].diff().dt.total_seconds().div(3600))\n",
    "    mean_dt=fault_sorted.groupby(\"SWITCH_ID\")[\"Δt_h\"].mean().rename(\"mean_h\")\n",
    "    cab=cab.merge(mean_dt,on=\"SWITCH_ID\",how=\"left\");cab[\"i_raw\"]=1/cab[\"mean_h\"].clip(lower=1)\n",
    "    cab[\"s_raw\"]=(cab[\"SEGMENTS\"].fillna(1)-1).clip(lower=0)\n",
    "    cab[\"l_raw\"]=np.log1p(cab[\"LENGTH_KM\"])\n",
    "    last_year=TARGET_YEAR-1\n",
    "    same_period_faults=fault[(fault[\"TIME_OUTAGE\"].dt.year==last_year)&(fault[\"TIME_OUTAGE\"].dt.month.between(start_month,end_month))].groupby(\"SWITCH_ID\").size()\n",
    "    cab=cab.merge(same_period_faults.rename(\"u_raw\"),on=\"SWITCH_ID\",how=\"left\").fillna(0)\n",
    "    h_pred_current=h_preds[h_preds['HALF']==p_num]\n",
    "    cab=cab.merge(h_pred_current[['SWITCH_ID','PRED_FAULTS_H']],on=\"SWITCH_ID\",how=\"left\",).fillna({\"PRED_FAULTS_H\":0})\n",
    "    cab[\"p_raw\"]=cab[\"PRED_FAULTS_H\"]\n",
    "    loading_start_date=p_start_date-pd.DateOffset(years=1)\n",
    "    rel_cyc_cols=[c for c,dt in cycle_cols.items() if loading_start_date<=dt<p_start_date]\n",
    "    rel_var_cols=[c for c,dt in var_cols.items() if loading_start_date<=dt<p_start_date]\n",
    "    cab[\"c_raw\"]=cab[rel_cyc_cols].mean(axis=1) if rel_cyc_cols else 0\n",
    "    if rel_var_cols:\n",
    "        median=cab[rel_var_cols].median(axis=1); cab[\"r_raw\"]=cab[rel_var_cols].mean(axis=1)/median.replace(0,np.nan)\n",
    "    else: cab[\"r_raw\"]=0\n",
    "    \n",
    "    # --- Predict with ML model ---\n",
    "    features = ['a_raw', 'c_raw', 'f_raw', 'i_raw', 'l_raw', 'p_raw', 'r_raw', 's_raw', 'u_raw']\n",
    "    X_target = cab[features].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    X_target_scaled = scaler.transform(X_target)\n",
    "    \n",
    "    fail_probability = model_ml.predict_proba(X_target_scaled)[:, 1]\n",
    "    cab[\"health_score\"] = np.rint(100 * (1 - fail_probability)).clip(0, 100).astype(int)\n",
    "    \n",
    "    # --- Analysis & Output ---\n",
    "    cab[\"health_band\"] = pd.cut(cab[\"health_score\"], [-np.inf, 40, 70, 100], labels=[\"Poor\", \"Moderate\", \"Good\"])\n",
    "    risk_contrib = pd.DataFrame(X_target_scaled * model_ml.coef_[0], columns=features)\n",
    "    cab[\"primary_health_driver\"]=risk_contrib.idxmax(axis=1).map(LABELS)\n",
    "    \n",
    "    mask_nofault = cab[\"hist_faults\"].eq(0)\n",
    "    cab.loc[mask_nofault, \"health_score\"] = 100\n",
    "    cab.loc[mask_nofault, \"health_band\"] = \"Good\"\n",
    "    cab.loc[mask_nofault, \"primary_health_driver\"] = \"No recorded faults\"\n",
    "\n",
    "    cab.to_csv(p_out_dir / f\"cable_health_{TARGET_YEAR}_{p_name}_scored.csv\", index=False)\n",
    "    log(f\"Saved results for {len(cab)} cables to {p_out_dir.name}/\")\n",
    "    \n",
    "    actual_faults_p = fault[(fault[\"TIME_OUTAGE\"].dt.year == TARGET_YEAR) & (fault[\"TIME_OUTAGE\"].dt.month.between(start_month, end_month))]\n",
    "    actual_switches_p = actual_faults_p[\"SWITCH_ID\"].unique()\n",
    "    cab[\"ACTUAL_FAIL_H\"] = cab[\"SWITCH_ID\"].isin(actual_switches_p).astype(int)\n",
    "    pred = cab[\"health_band\"].map({\"Poor\": 1, \"Moderate\": 1, \"Good\": 0})\n",
    "    if len(cab[\"ACTUAL_FAIL_H\"].unique()) > 1:\n",
    "        cm = confusion_matrix(cab[\"ACTUAL_FAIL_H\"], pred)\n",
    "        au = roc_auc_score(cab[\"ACTUAL_FAIL_H\"], 100 - cab[\"health_score\"])\n",
    "        log(f\"{p_name} confusion matrix:\\n{cm}\\nAUROC = {au:.3f}\")\n",
    "    else:\n",
    "        log(f\"Skipping validation for {p_name}: Only one class present in actuals.\")\n",
    "\n",
    "# --- Final Summary ---\n",
    "total_predicted_faults = y_preds['PRED_FULL_YEAR'].sum()\n",
    "total_actual_faults_2024 = fault[fault['TIME_OUTAGE'].dt.year == TARGET_YEAR].shape[0]\n",
    "\n",
    "log(\"\\n\" + \"=\"*45)\n",
    "log(f\"FLEET-WIDE FAULT SUMMARY FOR FULL YEAR {TARGET_YEAR}\")\n",
    "log(\"=\"*45)\n",
    "log(f\"Total Predicted Faults (Yearly): {total_predicted_faults:.1f}\")\n",
    "log(f\"Total Actual Faults (Yearly):    {total_actual_faults_2024}\")\n",
    "log(\"=\"*45)\n",
    "log(f\"\\nPipeline complete → ML-driven results saved in: {OUT_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c84764e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
